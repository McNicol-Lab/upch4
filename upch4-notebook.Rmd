---
title: "FLUXNET-CH4 Upscaling"
author: "Gavin McNicol"
date: "2/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages and ggplot theme:

```{r message = F}
rm(list=ls())
library(scales)
library(rlist)
library(tidyverse) 
library(lubridate)
library(raster)
library(oce) # for time series transformations
library(geosphere)
library(sf)
library(caret)
library(parallel)
library(RColorBrewer)
library(ncdf4)
library(ggrepel)
library(here)
library(proxy)
library(vegan)
library(rworldmap)
library(rgdal)
library(VGAM)
library(rnaturalearth)
library(rnaturalearthdata)
# source(paste(loc, "dissimilarity/rplots/import_libraries.r", sep=  "")) update this with all libraries
source("code/ggplot_theme.R")
source("code/fit_all_pairs.R")
source("code/fit_stepwise.R")
source() #placeholder for rescaling grid function (static)
source() #placeholder for rescaling grid function (temporal)
```

**NOTE: This work flow uses many large files so most data is stored locally and requires hard filepaths**
The local file path used for all large files is: `/Volumes/Samsung_T5/Stanford/upch4_local/`.

Set the local head directory:

```{r}
loc <- "/Volumes/Samsung_T5/Stanford/upch4_local/"
```

#### Workflow

  1. Objective of Study
  2. Input Data
      + FLUXNET-CH4 Data
      + Gridded Data
  3. FLUXNET-CH4 Pre-Processing
      + FLUXNET-CH4 Data Preparation
          - Averaging
          - Variable Selection
  4. Gridded Data Pre-Processing
      + Grid Preparation
          - Potential Radiation (computed)
          - MODIS
      + Grid Extraction
          - Canopy Height
          - Computed (Rpot)
          - Compound Topographic Index
          - Earth Environment
          - N and S Deposition
          - SoilGrids
          - TerraClimate
          - Fractional Vegetation Cover (VCF)
          - Wetland Extent (WAD2M)
          - WorldClim 2.0
      + Merge Gridded Data
      + Gridded Data Quality Control
  5. Finalize Training Data
      + Merge Flux Data with Geospaital and MODIS Data
      + Subset Wetlands, Dates
      + Cluster Sites for Cross Validation
      + Finalize Data
          - Remove Extraneous
          - Impute Missing Data
          - Add Lagged Data
      + FLUXNET-CH4 Data Quality Control
      + Table of Final FLUXNET-CH4 Inputs
  6. Forward Feature Selection (FFS)
      + Filter Weekly Data
      + Feature Subset Experiments
      + FFS
          - First Pair
          - Additional Stepwise Features
          - FFS Evolution Plots
      + Summarize FFS Performance
  7. Cross Validation
      + ML Model Training
          - RF and Final Predictors or Subsets
          - XGB and Final Predictors or Subsets
          - ANN and Final Predictors or Subsets
          - RNN and Final Predictors or Subsets
      + Output Ensembles and Predictions
      + Validation 
          - Global Performance
          - Site-means
          - Monthly Seasonal Cycles
          - Monthly Anomalies
  8. Variable Importances
      + Variable Importance Rankings
      + Variable Responses
      + ShapR
  9. Upscaled Model with Monte Carlo (MC)
      + Forcing Data
          - Mapping
          - Member Product Choices
      + Data Preparation
          - Extract Gridded Data for MC
          - Pre-Process FLUXNET-CH4 for MC
      + MC Simulations
      + MC ML Model Training
      + MC ML Model Validation
  10. Data Representativeness
      + Prepare Gridded Data
      + Global Dissimilarity
      + Tower Constituency
      + Extrapolation Errors
          - MC ML Model Training - Dissimilarity Only
  11. Upscaling
      + Prepare Grid Forcing Data
      + Run on Computing Cluster
          - Output Grids and Sums
      + 
      + Product Evaluation
          - Unweighted Wetland Fluxes (nmol)
          - Weighted Sums and Uncertainties (Tg)
          - Independent Validation
          
[**Link to Workflow Figure**](https://drive.google.com/drive/folders/1jiFyqzoxxMpdtRLCwxCtzKpfILRNIW5K)
          
$~$            
      
#### 1. Objective of Study 

The goal of `FLUXNET-CH4 Upscaling` is to implement FLUXCOM-like ML approaches (e.g. [Jung et al. 2020](10.5194/bg-17-1343-2020)) to train a machine learning model using eddy covariance data that can predict wetland methane (CH4) fluxes globally. The predictions should be readily comparable to the Global Carbon Project (GCP) bottom-up process model ensembles that inform the Global Methane Budget ([Saunois et al. 2020](10.5194/essd-12-1561-2020)). Wetland fluxes specifically, rather than methane fluxes from all terrestrial ecosystems, are the predictive goal of this study because 1) most eddy covariance data available are in wetlands, with limited coverage across the multitude of upland ecosystems, 2) methane fluxes are highest and most variable in wetlands, and 3) comparable bottom-up process models predict wetland fluxes, then scale predictions to a global grid-cell using a prescribed (diagnostic runs) or model-derived (prognostic runs) wetland extent. In the last GCP Global Methane Budget ([Saunois et al. 2020](10.5194/essd-12-1561-2020)), diagnostic runs used the WAD2M product ([Zhang et al. in review](10.5194/essd-2020-262)). WAD2M includes coastal wetlands, however, we exclude coastal wetlands from the upscaling because they are salt-influenced and we do not have consistent salinity coverage, thus their inclusion is likely to bias flux estimates low even in non-coastal wetlands. The final model will be forced with available globally gridded data. Final product specifications are:

  - Monthly time-step
  - Historic reconstruction: ca. 2001 - 2018
  - 0.25-degree grid cell resolution (as is WAD2M)
  - Propagates training data uncertainties using Monte Carlo simulations
  - Considers sensitivity to global forcing data product choices

$~$    
  
#### 2. Input Data

**FLUXNET-CH4 Data**

The eddy covariance data used in this study are publicly available as part of the FLUXNET-CH4 community product V1.0 at [fluxnet.org](https://fluxnet.org/data/fluxnet-ch4-community-product/). The FLUXNET-CH4 synthesis activity is introduced in [Knox et al. 2019](10.1175/BAMS-D-18-0268.1) along with a detailed description of the eddy covariance post-processing steps including methane flux (FCH4) uncertainties. The first full (V1.0) dataset release (FLUXNET-CH4, hereafter) is described for 81 sites and is used in a wetland seasonality analysis in [Delwiche et al. in review](10.5194/essd-2020-307). **Data for this CH4 Upscaling Project were downloaded from [fluxnet.org](www.fluxnet.org) on Feb 22, 2021. [Download Manifest](https://docs.google.com/spreadsheets/d/1--_-XyBqsyiMIdc6JXqhilOOYFLeaY-UTMFhhI4Sd5M/edit#gid=0)**

Links:

  - [FLUXNET-CH4 Site (81) Metadata](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1384338468)
  - Permission was received via email on Feb 22, 2021, to use *Tier 2 Data Policy* sites in this study (SE-St1 and RU-Vrk; PI Thomas Friborg)
  - FLUXNET-CH4 data were used both for methane fluxes (FCH4; target variable) and tower-measured bio-meteorological variables (e.g., LE, GPP, TA; predictors)
      + Link to [FLUXNET.org variable descriptions](https://fluxnet.org/data/fluxnet-ch4-community-product/data-variables/)
  
$~$  
  
**Gridded Data (Predictors)**

A summary of predictors (`Predictor Summary`) is available [here](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=0), as well as an appendix table with all individual predictors (`Appendix: All Predictors`). Candidate predictors were drawn from a mix of source classes (EC tower measurements, global models, computations from observed data, or remote sensing (e.g. NASA MODIS)), different information content groups (Spatial-only or Spatio-temporal), and at different temporal frequencies (Static, Yearly, Monthly, Weekly, or Half-hourly). Using this information, we assigned each candidate predictor to a class (Generic, Climate, Biometeorological, Land Cover, Soil and Relief, or Greenness) to evaluate predictive power of particular classes, given the likelihood for redundancy in useful predictors across the full predictor set. For MODIS predictors, we computed the mean seasonal cycle (`_msc`), and yearly mean, min, max, and amplitude parameters, as in [Tramontana et al. (2016)](10.5194/bg-13-4291-2016) and [Jung et al. (2020)](10.5194/bg-17-1343-2020). For derived TerraClimate soil moisture and actual evapotranspiration (aet) we computed the interannual range and annual seasonality. More information on preprocessing of predictors is provided in the manuscript.

$~$

#### 3. FLUXNET-CH4 Pre-Processing

##### Local machine steps:

  + Create a copy of FLUXNET-CH4 data and name it `/fluxnet-ch4-data-original`
  + Unzip all site flux files in `/fluxnet-ch4-data` 
  + Reorganize into half-hourly (`/hh`) and `/daily` folders for easy access
  
##### Look at one site of **daily means** data.

```{r}
# setwd(loc)
# files <- list.files("fluxnet-ch4-data/daily/")
# one_site <- read.csv(paste(loc, "fluxnet-ch4-data/daily/", files[1], sep = ""))
# head(one_site)
```

No, but there is a quality flag `_QC`. `1`= data gap shorter than 2 months, `3` = gap exceeds 2 months. 

Issue: **There is also no uncertainty (`_UNC`) estimate on the downloaded daily mean data.**

##### Clean up workspace:

```{r}
# rm(one_site)
```

##### Look at one site of **half-hourly** data.

```{r}
setwd(loc)
files <- list.files("fluxnet-ch4-data/hh/")
one_site <- read.csv(paste(loc, "fluxnet-ch4-data/hh/", files[1], sep = ""))
head(one_site)
```

  - Half-hourly data has `FCH4` uncertainty columns `FCH4_F_RANDUNC` and `FCH4_F_ANNOPTLM_UNC`. Can be averaged over day or week.
  - Missing values are filled with `-9999`

##### Get FLUXNET-CH4 site names:

```{r}
files <- list.files(paste0(loc, "fluxnet-ch4-data/hh/"))
site.names <- paste( substr(files, 5, 6), substr(files, 8, 10), sep = "")
```

##### Get all **half-hourly** flux data: (this will take a few minutes)

```{r echo = F, message = F, warning = F}
hh <- lapply(paste(loc, "fluxnet-ch4-data/hh/", files, sep = ""), read.csv)
names(hh) <- site.names
# str(hh)
```

##### Create a pristine replicate:

```{r}
hh2 <- hh
```

##### Look at all column names (including TIMESTAMP columns, to see date and time format):

```{r}
names(hh[[1]])
head(hh[[1]]$TIMESTAMP_END)
```

##### Write and apply function to expand `TIMESTAMP_END` into`Year`, `Month`, `Week`, `Day`, and `DOY` variables to facilitate merging with other data:

```{r}
expand_date <- function(hh_data) {
  hh_data <- hh_data %>% 
  mutate(Year = as.numeric(substr(TIMESTAMP_END, 1, 4)),
         Month = as.numeric(substr(TIMESTAMP_END, 5, 6)),
         Day = as.numeric(substr(TIMESTAMP_END, 7, 8)),
         Date = make_date(Year, Month, Day),
         DOY = yday(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week)) %>% 
    group_by(Year, DOY) %>% 
    mutate(HH = 1:n()) %>% 
  dplyr::select(Year, Month, Week, Day, HH, DOY, everything(), - Date, -TIMESTAMP_START, -TIMESTAMP_END)
}

hh <- lapply(hh, expand_date)
hh[[2]] # check it worked
```

##### Check if gap-filled FCH4 `CH4_F_ANNOPTLM` has already been pre-filled with observations `FCH4`, where available.

```{r}
hh[[1]] 
```

FCH4 in row 7 in `hh[[1]]` (ID BRNpw) = 10.71, which **matches** `CH4_F_ANNOPTLM` = 1.071e+01. 
**`CH4_F_ANNOPTLM` has been pre-filled with observations.**

##### Write and apply function to create `imputed`, a flag variable where:

  - `1` == `CH4_F_ANNOPTLM` was imputed
  - `2` == `CH4_F_ANNOPTLM` was observed

```{r}
create_imputed <- function(hh_data) {
  hh_data <- hh_data %>% 
    mutate(imputed = ifelse(FCH4 == -9999, 1, 0))
}

hh <- lapply(hh, create_imputed)
# hh[[1]] check it worked
```

##### Create ID column:

```{r}
for (i in 1:length(hh)){
  hh[[i]] <- hh[[i]] %>%
    mutate(ID = site.names[i]) %>%
    dplyr::select(ID, everything())
}
head(hh[[1]])
```

##### Convert missing values (`-9999`) to `NA`:

```{r}
for (i in 1:length(hh)){
  hh[[i]][hh[[i]] == -9999] <- NA
}
head(hh[[1]])
```

##### Get `u` and `v` wind components:

Identify which sites lack WD:

```{r}
find_wd <- function(hh_data) {
  sum(names(hh_data) == "WD") == 0
}

unlist(lapply(hh, find_wd))
sum(unname(unlist(lapply(hh, find_wd))))
```

OK, `CASCB` and `RUVrk` and missing `WD`.

Create dummy `WD` variables:

```{r}
hh$CASCB$WD <- NA
hh$RUVrk$WD <- NA

unlist(lapply(hh, find_wd))
sum(unname(unlist(lapply(hh, find_wd))))
```

Now compute wind direction components:

```{r}
compute_uv <- function(hh_data) {
  hh_data <- hh_data %>% 
    mutate(U = -WS_F * sin(2 * pi * WD/360),
           V = -WS_F * cos(2 * pi * WD/360))
}

hh <- lapply(hh, compute_uv)

head(hh[[1]]) # check it worked
```
##### Merge Random and Gap-filling uncertainty columns before taking mean 

```{r}
for (i in 1:length(hh)){
    hh[[i]] <- hh[[i]] %>% 
      mutate(FCH4_UNC = ifelse(is.na(FCH4_F_RANDUNC), FCH4_F_ANNOPTLM_UNC, FCH4_F_RANDUNC)) %>% 
      dplyr::select(-FCH4_F_RANDUNC, -FCH4_F_ANNOPTLM_UNC)
}
head(hh[[1]])
```


##### Compute weekly means of everything, including vector average WD and speed, excluding precip. (sums)

Create function to compute weekly means:

```{r echo = F}
compute_weekly1 <- function(hh_data) {
  hh_data <- hh_data %>% 
    group_by(ID, Year, Month, Week) %>% 
    summarize_all(list(~mean(., na.rm = T))) %>% 
    mutate(WD_mean = (atan2(U, V) * 360/2/pi) + 180,
           WS_mean = ((U^2 + V^2)^0.5)) %>% 
    dplyr::select(ID, Year, Month, Week, everything(), -HH, -Day, -DOY)
}
```

Check run time for one site:

```{r warning = F}
system.time( 
 compute_weekly1(hh[[1]])
)
```

Apply to all sites using lapply:

```{r warning = F}
weekly <- lapply(hh, compute_weekly1)
```

Get sum of precip.

```{r warning = F, message = F}
sum_precip <- function(hh_data) {
  hh_data <- hh_data %>% 
    group_by(ID, Year, Month, Week) %>% 
    summarize(P_F_sum = sum(P_F))
}

weekly_precip <- lapply(hh, sum_precip)
```

Rejoin precip. to main data frame:

```{r}
for (i in 1:length(weekly)) {
weekly[[i]] <- weekly[[i]] %>% 
  left_join(weekly_precip[[i]], by = c("ID", "Year", "Month", "Week")) 
}
names(weekly[[1]])
```

##### Output each "complete" weekly site as a separate .csv file:

```{r}
for (i in 1:length(weekly)) {
  weekly[[i]] %>% write.csv(paste(loc, "fluxnet-ch4-data/weekly_upch4/", site.names[i], "_weekly_upch4.csv", sep = ""), 
                           row.names = F)
}
```

#### 3. FLUXNET-CH4 Pre-Processing (cont.) 
##### (re)Load Weekly Data

```{r message = F, warning = F, echo = F}
files <- list.files(paste(loc, "fluxnet-ch4-data/weekly_upch4/", sep = ""))
weekly <- lapply(paste(loc,"fluxnet-ch4-data/weekly_upch4/",files, sep = ""), read.csv)
```

##### Bind rows

```{r}
weekly_flat <- weekly %>% bind_rows() %>% as_tibble()
head(weekly_flat)
```

##### Compute FCH4 uncertainty, and subset gap-filled (and best) predictors

Notes on variable selection:

  + Total FCH4 uncertainty `FCH4_F_UNC` is the average of the weekly random `FCH4_F_RANDUNC` and gap-filling `FCH4_F_ANNOPTLM_UNC` uncertainty. I also drop the `FCH4_F_ANNOPTLM_QC` Quality control flag because I implement a stricter filtering criteria later of at least one observed flux per day.
  + The output of the daytime method `_DT` is used for gross primary production (GPP) and ecosystem respiration (RECO). Although both methods are subject to bias due to light-inhibition of leaf respiration, [Keenan et al. 2019](https://doi.org/10.1038/s41559-019-0809-2) show that the biases for the DT method only impact night-time respiration, but did not impact apparent photosynthesis or daytime respiration.
  + LE and NEE were gap-filled according to [Knox et al. 2019](10.1175/BAMS-D-18-0268.1) - same overall method to FCH4.
  + Other micro-meteorology (e.g., PPFD_IN) was gap-filled using ERA interim data (see [Delwiche et al.](10.5194/essd-2020-307)).
  + Only PPFD_IN (not PPFD_OUT) was selected as it can be approximated from SW_IN.
  + The shallowest available soil temperature is taken (`TS_1`)

```{r}
weekly_subset <- weekly_flat %>% 
  mutate(FCH4_UNC = FCH4_UNC) %>% 
  dplyr::select(ID, Year, Month, Week,    # descriptive data
         FCH4 = FCH4_F_ANNOPTLM, FCH4_UNC, imputed,   # methane fluxes
         NEE = NEE_F_ANNOPTLM,  GPP = GPP_DT, RECO = RECO_DT,  # ecosystem C fluxes
         PPFD_IN = PPFD_IN_F, SW_IN = SW_IN_F, LW_IN = LW_IN_F, NETRAD = NETRAD_F,   # radiation
         LE = LE_F_ANNOPTLM, H = H_F,  # ecosystem energy fluxes
         TA = TA_F, PA = PA_F, RH = RH_F, VPD = VPD_F, P = P_F_sum, USTAR = USTAR, WS = WS_mean, # meteorology 
         TS = TS_1, SWC = SWC_F, WTD = WTD_F) # soil properties 
head(weekly_subset)
```

##### Append site metadata

Pulled from [FLUXNET-CH4 Site Metadata](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1384338468)

```{r}
metadata <- read.csv(paste(loc, "fluxnet-ch4-data/metadata/fluxnet-ch4-site-metadata.csv", sep = "")) 
weekly_subset_meta <- metadata %>% 
  mutate(ID = paste(substr(ID, 1, 2),substr(ID, 4, 6), sep="")) %>% 
  right_join(weekly_subset, by = ("ID")) 
```

##### Save subset, flattened, and metadata-appended weekly FLUXNET-CH4 data

```{r}
write.csv(weekly_subset_meta, 
          paste(loc, "fluxnet-ch4-data/weekly_flat/weekly_subset_meta.csv", sep = ""),
          row.names = F)
```

#### 4. Gridded Data Pre-Processing

#### *Create Potential Radiation (RPot)*

**Description**
Get the mean seasonal cycle from Zutao Ouyang's 2001-2018 MATLAB output. Original citation for `Rpot` is [Peltola et al. 2019](10.5194/essd-11-1263-2019) where it was found to be a useful predictor of high latitude wetland FCH4.

Define a function to take a number of years and return an index of months:

```{r}
create_msc_index <- function(years) {
  msc_index <- list()
  for (i in 1:12){
      msc_index[[i]] <- seq(i, ((years-1)*12+i), by = 12)
  }
  msc_index
}
# create_msc_index(15) # test it out
```

Load in Rpot and create msc index for 19 years.

```{r}
Rpot <- brick(paste(loc, "grids/computed/Rpot.nc", sep = ""))
msc_index <- create_msc_index(19)
```

Get `_msc`:

```{r}
Rpot_msc <- list()
for (i in 1:12) {
  Rpot_msc[[i]] <- calc(Rpot[[msc_index[[i]]]], fun = mean)
}
```

Create raster stack:

```{r}
Rpot_msc <- stack(Rpot_msc)
```

Output `Rpot_msc.nc`:

```{r}
# writeRaster(Rpot_msc, paste(loc, "grids/computed/Rpot_msc.nc", sep = ""), format="CDF", overwrite = T)
```

#### *MODIS*

**Description**
Zutao Ouyang (Stanford University) extracted MODIS pixels at the FLUXNET-CH4 sites in April 2020, for 9 products:
 
  + Daytime Land Surface Temperature (`LSTD` from **MOD11A2**) 
      - *(not used as it is a correlate of nighttime temp., and nighttime is more applicable to soil conditions)*
  + Nighttime Land Surface Temperature (`LSTN` from **MOD11A2**)
  + Normalized Difference Vegetation Index (`NDVI` from **MOD09A1**)
  + Enhanced Vegetation Index (`EVI` from **MOD09A1**)
  + Leaf Area Index (`LAI` from **MCD15A2H**)
      - This is modeled, not directly measured.
  + Long Short Water Index (`LSWI`  from **MOD09A1**)
  + Simple Ratio Water Index (`SRWI` from **MOD09A1**)
  + Normalized Difference Water Index (`NDWI` from **MOD09A1**)
  + Normalized Difference Snow Index (`NDSI`  from **MOD11A2**)
  
**NOTE** Tables docmenting MODIS processing step effects on data are here  under [MODIS Processing](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1971246167) and QC figures are output to `upch4_local/modis/modis_qc`.


##### Get file local file names/paths for 8-day extracted MODIS data:

```{r}
setwd(paste(loc, "/modis/modis-extracted", sep = ""))
files <- list.files()
```

##### Set MODIS product/file names (for gather values):

```{r}
modis.names <- c("LSTD", "EVI", "LAI", "LSWI", "NDSI", "NDVI", "NDWI", "LSTN", "SRWI")
```

##### Read files:

```{r echo = F, warning = F, message = F}
modis <- lapply(paste(loc, "/modis/modis-extracted/", files, sep = ""), read.csv)
names(modis) <- modis.names
# str(modis)
```

##### Look at head for first file:

```{r}
head(modis[[1]])
```
Files are not tidy (short and wide). There are **86** extracted sites (extra sites than in FLUXNET-CH4 V1.0) but there should be **81**. 

##### Correct issues in Site IDs (columan names) to match `fluxnet-ch4-site-metadata.csv`:

```{r}
for(i in 1:length(modis)){
  modis[[i]] <- modis[[i]] %>% 
    dplyr::select(-`CA-DBB`, -`RU-SAM`, -`SE-Sto`, -`US-Bgl`, -`US-Brw`) %>% 
    rename(`JP-SwL` = `JP-Swl`, `PH-RiF` = `PH-RIf`, `US-ICs` = `US-Ics`, `US-MAC` = `MAERC`)
}
dim(modis[[1]])
```

##### Use gather to make long and narrow, remove hyphen from ID:

```{r}
for (i in 1:length(files)) {
  modis[[i]] <- modis[[i]] %>% 
    gather(key = ID, value = "modis.name", 2:82) %>% 
    mutate(ID = paste(substr(ID, 1, 2), substr(ID, 4, 6), sep = "")) %>% 
    as_tibble()
}
names(modis) <- modis.names
```

##### Check data are same length:

```{r}
str(modis)
```

1) `NDSI` is longer, 2) `LAI` is slightly shorter. Based on the dates, `NDSI` is daily.

Will not be able to bind columns with different lengths

##### Remove `LAI` and `NDSI` then rejoin:

```{r}
modis.1.names <- c("LSTD", "EVI", "LSWI", "NDVI", "NDWI", "LSTN", "SRWI")
modis.1 <- modis[modis.1.names]
```

##### Select only first two columns (Date and ID) then data columns:

```{r}
modis.1 <- modis.1 %>% 
  bind_cols() %>% 
  dplyr::select(1, 2, 3,6,9,12,15,18,21) %>% # subset only data columns
  as_tibble() 
names(modis.1) <- c("Date", "ID", modis.1.names)
```

##### Rejoin `LAI` using `Date`:

```{r}
modis.2 <- modis.1 %>% left_join(modis$LAI)
names(modis.2)[10] <- "LAI"
```

##### Rejoin `NDSI`:

```{r}
modis <- modis.2 %>% right_join(modis$NDSI)
names(modis)[11] <- "NDSI"
```

##### Convert `Date` into `Year` and `DOY`, and remove `LSTD`:

```{r}
modis <- modis %>% 
  mutate(Date = as.Date(Date, format = "%m/%d/%Y")) %>% 
  mutate(DOY = yday(Date),
         Date = as_date(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week),
         Week = as.factor(Week),
         Year = as.integer(substr(Date, 1,4 )),
         Month = as.numeric(substr(Date, 6, 7))) %>% 
  dplyr::select(ID, Date, Year, Month, Week, DOY, NDSI, NDVI, EVI, LAI, NDWI, SRWI, LSWI, LSTN) 
```

##### Count `NAs` before despiking:

```{r}
modis_nas <- modis %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)))) 
modis_nas
```
##### Despike outlier values for each product:

```{r}
modis_dsp <- modis %>% 
  mutate(NDSI_dsp = despike(NDSI, reference = 'trim', min = 0, max = 100, replace = "NA"),
         NDVI_dsp = despike(NDVI, reference = 'trim', min = -1, max = 1, replace = "NA"),
         EVI_dsp = despike(EVI, reference = 'trim', min = -1, max = 1, replace = "NA"),
         LAI_dsp = despike(LAI, reference = 'trim', min = 0, max = 5, replace = "NA"),
         NDWI_dsp = despike(NDWI, reference = 'trim', min = 0, max = 1, replace = "NA"),
         SRWI_dsp = despike(SRWI, reference = 'trim', min = -1, max = 3, replace = "NA"),
         LSWI_dsp = despike(LSWI, reference = 'trim', min = 0, max = 1, replace = "NA"),
         LSTN_dsp = despike(LSTN, reference = 'trim', min = -60, max = 50, replace = "NA")) %>% 
dplyr::select(ID, Date, Year, Month, Week, DOY, NDSI, NDVI, EVI, LAI, NDWI, SRWI, LSWI, LSTN,
              NDSI_dsp, NDVI_dsp, EVI_dsp, LAI_dsp, NDWI_dsp, SRWI_dsp, LSWI_dsp, LSTN_dsp) 
``` 

##### Count `NAs` after despiking:

```{r}
modis_dsp_nas <- modis_dsp %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)))) 
modis_dsp_nas
```
##### Calculate removed values during despiking and output to `upch4_local/modis/modis_qc/`

```{r}
modis_dsp_nas %>% 
  mutate(NDSI = NDSI - NDSI_dsp,
         NDVI = NDVI - NDVI_dsp,
         EVI = EVI - EVI_dsp,
         LAI = LAI - LAI_dsp,
         NDWI = NDWI - NDWI_dsp,
         SRWI = SRWI - SRWI_dsp,
         LSWI = LSWI - LSWI_dsp,
         LSTN = LSTN - LSTN_dsp) %>% 
  dplyr::select(-ID) %>% 
  summarize_all(list(~sum(.))) %>% 
  write.csv(paste(loc, "/modis/modis-qc/despiking_na_effects.csv", sep = ""))
```


##### Rename without `_dsp` suffix:

```{r}
modis_dsp <- modis_dsp %>% 
  dplyr::select(ID, Date, Year, Month, Week, DOY, 
                NDSI = NDSI_dsp, 
                NDVI = NDVI_dsp, 
                EVI = EVI_dsp,
                LAI = LAI_dsp,
                NDWI = NDWI_dsp,
                SRWI = SRWI_dsp,
                LSWI = LSWI_dsp,
                LSTN = LSTN_dsp)
```


##### Calculate mean  seasonal cycle (msc), then weekly means, then fill weekly gaps with msc:

```{r}
modis_gapfilled <- modis_dsp %>% 
  group_by(ID, Month) %>%  # this section computes  monthly values averaged over multiple years
  mutate(NDSI_msc = mean(NDSI, na.rm = TRUE),
         NDVI_msc = mean(NDVI, na.rm = TRUE),
         EVI_msc = mean(EVI, na.rm = TRUE), 
         LAI_msc = mean(LAI, na.rm = TRUE),
         NDWI_msc = mean(NDWI, na.rm = TRUE),
         SRWI_msc = mean(SRWI, na.rm = TRUE),
         LSWI_msc = mean(LSWI, na.rm = TRUE),
         LSTN_msc = mean(LSTN, na.rm =TRUE)) %>% 
  group_by(ID, Year, Month, Week) %>% # this section compute weekly means
  summarize(NDSI = mean(NDSI, na.rm = TRUE),
            NDVI = mean(NDVI, na.rm = TRUE),
            EVI = mean(EVI, na.rm = TRUE),
            LAI = mean(LAI, na.rm = TRUE),
            NDWI = mean(NDWI, na.rm = TRUE),
            SRWI = mean(SRWI, na.rm = TRUE),
            LSWI = mean(LSWI, na.rm = TRUE),
            LSTN = mean(LSTN, na.rm = TRUE),
            NDSI_msc = NDSI_msc[1],    # this section selects the monthly (msc) value corresponding to the weekly data
            NDVI_msc = NDVI_msc[1],
            EVI_msc = EVI_msc[1],
            LAI_msc = LAI_msc[1],
            NDWI_msc = NDWI_msc[1],
            SRWI_msc = SRWI_msc[1],
            LSWI_msc = LSWI_msc[1],
            LSTN_msc = LSTN_msc[1]) %>% 
  mutate(NDSI_F = ifelse(is.na(NDSI), NDSI_msc, NDSI), # this section fills any missing weekly values with the msc (monthly average)
         NDVI_F = ifelse(is.na(NDVI), NDVI_msc, NDVI),
         EVI_F = ifelse(is.na(EVI), EVI_msc, EVI),
         LAI_F = ifelse(is.na(LAI), LAI_msc, LAI),
         NDWI_F = ifelse(is.na(NDWI), NDWI_msc, NDWI),
         SRWI_F = ifelse(is.na(SRWI), SRWI_msc, SRWI),
         LSWI_F = ifelse(is.na(LSWI), LSWI_msc, LSWI),
         LSTN_F = ifelse(is.na(LSTN), LSTN_msc, LSTN)) 
```


##### If snow is on the ground, set water indices to the 5% quantile (frozen):

```{r}
modis_frozen <- modis_gapfilled %>% 
  group_by(ID,Year) %>% 
  mutate(NDWI_msc = ifelse(NDSI_msc > 0, quantile(NDWI_msc, 0.05, na.rm=TRUE), NDWI_msc),
         SRWI_msc = ifelse(NDSI_msc > 0, quantile(SRWI_msc, 0.05, na.rm=TRUE), SRWI_msc),
         LSWI_msc = ifelse(NDSI_msc > 0, quantile(LSWI_msc, 0.05, na.rm=TRUE), LSWI_msc),
         
         NDWI_F = ifelse(NDSI_F > 0, quantile(NDWI_F, 0.05, na.rm=TRUE), NDWI_F),
         SRWI_F = ifelse(NDSI_F > 0, quantile(SRWI_F, 0.05, na.rm=TRUE), SRWI_F),
         LSWI_F = ifelse(NDSI_F > 0, quantile(LSWI_F, 0.05, na.rm=TRUE), LSWI_F))
```

##### Compute mean, min, max, amplitude:

```{r message = F, warning = F}
modis_frozen <- modis_frozen %>% 
  group_by(ID, Year) %>% 
  mutate(NDSI_mean = mean(NDSI_F, na.rm=TRUE),
         NDVI_mean = mean(NDVI_F, na.rm=TRUE),
         EVI_mean = mean(EVI_F, na.rm=TRUE),
         LAI_mean = mean(LAI_F, na.rm=TRUE),
         NDWI_mean = mean(NDWI_F, na.rm=TRUE),
         SRWI_mean = mean(SRWI_F, na.rm=TRUE),
         LSWI_mean = mean(LSWI_F, na.rm=TRUE),
         LSTN_mean = mean(LSTN_F, na.rm=TRUE),
         
         NDSI_min = min(NDSI_F, na.rm=TRUE),
         NDVI_min = min(NDVI_F, na.rm=TRUE),
         EVI_min = min(EVI_F, na.rm=TRUE),
         LAI_min = min(LAI_F, na.rm=TRUE),
         NDWI_min = min(NDWI_F, na.rm=TRUE),
         SRWI_min = min(SRWI_F, na.rm=TRUE),
         LSWI_min = min(LSWI_F, na.rm=TRUE),
         LSTN_min = min(LSTN_F, na.rm=TRUE),
         
         NDSI_max = max(NDSI_F, na.rm=TRUE),
         NDVI_max = max(NDVI_F, na.rm=TRUE),
         EVI_max = max(EVI_F, na.rm=TRUE),
         LAI_max = max(LAI_F, na.rm=TRUE),
         NDWI_max = max(NDWI_F, na.rm=TRUE),
         SRWI_max = max(SRWI_F, na.rm=TRUE),
         LSWI_max = max(LSWI_F, na.rm=TRUE),
         LSTN_max = max(LSTN_F, na.rm=TRUE),
         
         NDSI_amp = NDSI_max-NDSI_min,
         NDVI_amp = NDVI_max-NDVI_min,
         EVI_amp = EVI_max-EVI_min,
         LAI_amp = LAI_max-LAI_min,
         NDWI_amp = NDWI_max-NDWI_min,
         SRWI_amp = SRWI_max-SRWI_min,
         LSWI_amp = LSWI_max-LSWI_min,
         LSTN_amp = LSTN_max-LSTN_min)
```

##### Get site IDs:

```{r}
site.names <- modis_frozen %>%
  ungroup() %>% 
  mutate(ID = factor(ID)) %>% 
  dplyr::select(ID) %>% 
  pull() %>% unique()
```

##### Get variable names:

```{r}
names <- names(modis)[7:14]
names_F <- paste(names, "_F", sep = "")
names_msc <- paste(names, "_msc", sep = "")
```

##### Create modis qcqa figures:

```{r message = F, warning = F}
for (i in 1:length(names)){
  
  # visualize 1-45
  modis_frozen %>%
    filter(ID %in% site.names[1:45]) %>%
    dplyr::select(Month, value_F = names_F[i], value_msc = names_msc[i]) %>% 
    ggplot(aes(Month, value_F)) +
    geom_point(size = 1, alpha = 0.3) +
    geom_line(aes(Month, value_msc), col = 'purple', size = 2) +
    facet_wrap(~ID, scales = 'free', ncol = 9) +
    my_theme
  ggsave(paste(loc, "/modis/modis-qc/", names[i], "_1.pdf", sep = ""),
         width = 50, height = 30, units = c("cm"), dpi = 300)
  
  # visualize 46-86
  modis_frozen %>%
    filter(ID %in% site.names[46:86]) %>%
    dplyr::select(Month, value_F = names_F[i], value_msc = names_msc[i]) %>% 
    ggplot(aes(Month, value_F)) +
    geom_point(size = 1, alpha = 0.3) +
    geom_line(aes(Month, value_msc), col = 'purple', size = 2) +
    facet_wrap(~ID, scales = 'free', ncol = 9) +
    my_theme
  ggsave(paste(loc, "/modis/modis-qc/", names[i], "_2.pdf", sep = ""),
         width = 50, height = 30, units = c("cm"), dpi = 300)
  
}
```

##### Output processed MODIS data:

```{r}
write.csv(modis_frozen, paste(loc, "/modis/modis-processed/modis-processed.csv", sep = ""),
          row.names = FALSE)
```

#### *Grid Extraction* 

**Extract geospatial data at FLUXNET-CH4 sites and output a single geospatial .csv.**

##### Load site metadata (ID, Latitude, Longitude), applicable to all extractions:

```{r}
sites <- read.csv(paste(loc, "fluxnet-ch4-data/metadata/fluxnet-ch4-site-metadata.csv", sep = "")) %>% 
  mutate(ID = paste(substr(ID, 1, 2), substr(ID, 4, 6), sep = ""))
site.coords <- cbind(sites$Longitude, sites$Latitude) # (x, y)
site.num <- length(site.coords[,1])
```

##### Global Canopy Height

```{r}
raster.names <- list.files(paste(loc, "grids/global-canopy-height/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/global-canopy-height/", raster.names, sep = ""))
canopyht <- as_tibble(raster::extract(stack, site.coords)) 
canopyht <- cbind(sites, canopyht) %>% 
  rename(canopyht = Simard_Pinto_3DGlobalVeg_JGR)
head(canopyht)
```

###### Check for missing data

```{r}
canopyht %>% 
  mutate(missing = is.na(canopyht)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(canopyht, paste(loc, "grids/extracted/", "canopyht.csv", sep = ""), row.names =  F)
```

##### Computed (rpot)

```{r}
stack <- stack(paste(loc, "grids/computed/Rpot_msc.nc", sep = ""))
rpot <- as_tibble(raster::extract(stack, site.coords)) 
rpot <- cbind(sites, rpot) %>% 
  gather(key = "Month", value = "rpot", 15:26) %>% 
  mutate(Month = str_remove(Month, "X"))
head(rpot)
```

###### Check for missing data

```{r}
rpot %>% 
  mutate(missing = is.na(rpot)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```
No missing data

###### Write data

```{r}
write.csv(rpot, paste(loc, "grids/extracted/", "rpot.csv", sep = ""), row.names =  F)
```

##### Compound Topographic Index

```{r}
raster.names <- list.files(paste(loc, "grids/geomorpho90m/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/geomorpho90m/", raster.names, sep = ""))
cti <- as_tibble(raster::extract(stack, site.coords)) 
cti <- cbind(sites, cti) %>% 
  rename(cti = dtm_cti_merit.dem_m_250m_s0cm_2018_v1.0)
head(cti)
```

No missing data. 

After QC below, noticed USHRA has a high CTI value. Check on map:

###### Check locations of USHRA and USHRC by cropping raster:

```{r}
# site.coords[sites$ID %in% c("DEDgw", "USDPW"),]
USHRA_bb <- c( "xmin" =  -91.751735 - 0.1, "xmax" = -91.751735 + 0.1, 
               "ymin" = 34.585208 - 0.1 , "ymax" =  34.585208 + 0.1)
USHRC_bb <- c( "xmin" =  -91.751658 - 0.1, "xmax" = -91.751658 + 0.1, 
               "ymin" = 34.588830 - 0.1 , "ymax" =  34.588830 + 0.1)
USHRA_bb <- crop(stack[[1]], USHRA_bb)
USHRC_bb <- crop(stack[[1]], USHRC_bb)
```

```{r}
plot(USHRA_bb)
points(site.coords)
```

Caused by HRA being apparently closer to a river/flow channel. No difference obvious in Google Earth images. Probably an issue with the CTI product.
Given USHRA will become an outlier, I'll correct it to match the lower USHRC value.

###### Correct USHRA to USHRC value

```{r}
cti[cti$ID == "USHRA", 15] <- cti[cti$ID == "USHRC", 15]
```


###### Check for missing data

```{r}
cti %>% 
  mutate(missing = is.na(cti)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))

```

No missing data

###### Write data

```{r}
write.csv(cti, paste(loc, "grids/extracted/", "cti.csv", sep = ""), row.names =  F)
```

##### Earth Environment Texture, Land Cover and Topography

Texture (14 variables)
Land Cover (2 variables)
Topography (10 variables)
See: [Appendix: All Predictors](https://docs.google.com/spreadsheets/d/1yBCIt5nFHVIb9v_0ExX8-4Svr0wlcmjD9vdp67hGtnQ/edit#gid=800847828). Search for `EarthEnv` under column `Gridded Product`.

###### First do texture:

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/texture/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/texture/", raster.names, sep = ""))
earthenv_texture <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_texture) <- c("cont","corr","cv","diss","entr","even","homo","max","rang","shan","simp","std","unif","var")
```

###### Then land cover (commented code for first-run creates `HD` - human development -  as the sum of LC7 and LC9):

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/cover/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/cover/", raster.names, sep = ""))

# # create `HD` (human development)
# HD <- stack[[1]] + stack[[2]]
# writeRaster(HD, paste(loc, "/grids/earthenv/cover/HD.tiff", overwrite = T))

earthenv_cover <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_cover) <- c("HD7", "HD9", "HD")
```

###### Then Topography:

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/topography/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/topography/", raster.names, sep = ""))
earthenv_topography <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_topography) <- c("east","elev","flat","hollow","north","pcurv","rough","slope","tcurv","tpi")
```

###### Now combine:

```{r}
earthenv <- cbind(sites, earthenv_texture, earthenv_cover, earthenv_topography)
```

###### Look at data histograms:

```{r}
earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  ggplot(aes(value)) + 
  geom_histogram() + 
  facet_wrap(~var, scales = 'free')
```

`cont`, `diss`, and `var` have extreme outliers (xmax = 10e+09)

###### Replace extreme outliers with median

```{r}
earthenv <- earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  group_by(var) %>% 
  mutate(value = ifelse(value > 10^9, median(value, na.rm = TRUE), value)) %>% 
  ungroup() %>% 
  spread(key = "var", value = "value")
```

Visualize again above to check they are corrected.

###### Also check for missing data:

```{r}
earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

###### There are two sites (USDPW and USWPT) with missing values. Plot the sites:

```{r}
plot(stack[[1]])
points(site.coords[sites$ID %in% c("USDPW", "USWPT"), ])
```

###### Fill with nearby site values. 

USDPW from USLA1

USWPT from USOWC

```{r}
earthenv[earthenv$ID == "USDPW", c(is.na(earthenv[earthenv$ID == "USDPW", ])) ] <- earthenv[earthenv$ID == "USLA1", c(is.na(earthenv[earthenv$ID == "USDPW", ])) ] 
earthenv[earthenv$ID == "USWPT", c(is.na(earthenv[earthenv$ID == "USWPT", ])) ] <- earthenv[earthenv$ID == "USOWC", c(is.na(earthenv[earthenv$ID == "USWPT", ])) ] 
```

###### Write data

```{r}
write.csv(earthenv, paste(loc, "grids/extracted/", "earthenv.csv", sep = ""), row.names =  F)
```

##### N and S Deposition

From [Lamarque et al. 2013](10.5194/acp-13-7997-2013):

  - accmip_nhx_acchist_2000.nc
  - accmip_noy_acchist_2000.nc
  - accmip_sox_acchist_2000.nc

```{r message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/ns-deposition/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/ns-deposition/", raster.names, sep = ""))
stack[[1]]
```

###### Update extent (which runs 0, 360, -90, 90)

```{r}
x1 <- crop(stack, extent(180, 360, -90, 90))
x2 <- crop(stack, extent(0, 180, -90, 90))   
extent(x1) <- c(-180, 0, -90, 90)
m <- merge(x1, x2)
names(m) <- names(stack)
```

###### Check overlap of points

```{r}
m[[1]] %>% na_if(-9999) %>% plot() 
```
###### Extract ns data and sum n variables for total n (`nt_depo`)

```{r}
ns_depo <- as_tibble(raster::extract(m, site.coords)) 
ns_depo <- cbind(sites, ns_depo) %>% 
  rename(nx_depo = Dry.deposition.NH3.NH4,
         ny_dep = Dry.deposition.NOy,
         s_dep = Dry.deposition.SO2.SO4) %>% 
  mutate(nt_depo = nx_depo + ny_dep)
head(ns_depo)
```
###### Check for missing data

```{r}
ns_depo %>% 
  gather(key = "var", value = "value", 15:17) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(ns_depo, paste(loc, "grids/extracted/", "ns_depo.csv", sep = ""), row.names =  F)
```

##### SoilGrids

6 products:

  - BLDFIE_M_sl2_250m_II.tif (bulk density)
  - OCDENS_M_sl2_250m_II.tif (organic carbon density)
  - PHIHOX_M_sl2_250m_II.tif (soil pH)
  - CLYPPT_M_sl2_250m_II.tif (proportion of clay particles)
  - AWCh3_M_sl2_250m_II.tif (soil water holding capacity)
  - HISTPR_250m_II.tif (histogram abundance)

```{r}
raster.names <- list.files(paste(loc, "grids/soilgrids/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/soilgrids/", raster.names, sep = ""))
# plot(stack[[3]])
soilgrids <- as_tibble(raster::extract(stack, site.coords)) 
soilgrids <- cbind(sites, soilgrids) %>% 
  rename(awc = AWCh3_M_sl2_250m_ll,
         bd = BLDFIE_M_sl2_250m_ll,
         clay = CLYPPT_M_sl2_250m_ll,
         hist = HISTPR_250m_ll,
         ocd = OCDENS_M_sl2_250m_ll,
         ph = PHIHOX_M_sl2_250m_ll)
head(soilgrids)
```

###### Check for missing soilgrids data:

```{r}
soilgrids %>% 
  gather(key = "var", value = "value", 15:20) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```
###### Check locations of DEDgw and USDPW by cropping raster:

```{r}
# site.coords[sites$ID %in% c("DEDgw", "USDPW"),]
DEDgw_bb <- c( "xmin" =  13.05427 - 0.1, "xmax" = 13.05427 + 0.1, 
               "ymin" = 53.15141 - 0.1 , "ymax" =  53.15141 + 0.1)
USDPW_bb <- c( "xmin" =  -81.43611 - 0.1, "xmax" = -81.43611 + 0.1,
               "ymin" = 28.05206 - 0.1 , "ymax" =  28.05206 + 0.1)
DEDgw_loc <- crop(stack[[1]], DEDgw_bb)
USDPW_loc <- crop(stack[[1]], USDPW_bb)
```

###### First check DEDgw

```{r}
plot(DEDgw_loc)
points(site.coords)
```

###### Then check USDPW

```{r}
plot(USDPW_loc)
points(site.coords)
```
By chance these two sites fall on NA pixels, by being close to water bodies.

###### Shift latitude by 0.005-degrees N

```{r}
sites.temp <- sites %>% 
  filter(ID %in% c("DEDgw","USDPW")) %>% 
  mutate(Latitude = Latitude + 0.005)
site.coords.temp <- sites.temp %>% 
  dplyr::select(Longitude, Latitude) %>% 
  unname() %>% as.matrix()
```

###### Extract only these two lat-adjusted sites

```{r}
soilgrids.temp <- as_tibble(raster::extract(stack, site.coords.temp)) 
soilgrids.temp <- cbind(sites.temp, soilgrids.temp) %>% 
  rename(awc = AWCh3_M_sl2_250m_ll,
         bd = BLDFIE_M_sl2_250m_ll,
         clay = CLYPPT_M_sl2_250m_ll,
         hist = HISTPR_250m_ll,
         ocd = OCDENS_M_sl2_250m_ll,
         ph = PHIHOX_M_sl2_250m_ll)
head(soilgrids.temp)
```

###### Rejoin soilgrids

```{r}
soilgrids <- soilgrids %>% 
  filter(!is.na(bd)) %>% 
  bind_rows(soilgrids.temp) %>%
  arrange(ID)
```

###### Write data

```{r}
write.csv(soilgrids, paste(loc, "grids/extracted/", "soilgrids.csv", sep = ""), row.names =  F)
```

##### TerraClimate

2 products:
  
  - Soil Moisture (soilwater and soilwaterR (range), soilwaterS (annual seasonality))
  - Actual Evapotranspiration (aet and aetR, aetS)
  
###### Read Actual Evapotranspiration grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/terraclimate/aet/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/terraclimate/aet/", raster.names, sep = ""))
plot(stack[[3]])
```
###### Extract Actual Evapotranspiration grids

```{r}
# extract data at sites
terraclimate.aet <- as_tibble(raster::extract(stack, site.coords))

# create new name vector
x <- c(as.character(c(1:216)))

# rename to retain order
terraclimate.aet <- terraclimate.aet %>% as_tibble() %>% 
  rename_all(~x)

# look at combined data
terraclimate.aet <- cbind(sites,terraclimate.aet)
head(terraclimate.aet)
```

###### Finalize Actual Evapotranspiration

```{r}
terraclimate.aet <- terraclimate.aet %>%
  gather(key = Month, value = aet, 15:length(terraclimate.aet)) %>% 
  mutate(Month = as.numeric(Month), 
         aet = as.numeric(aet)) %>% 
  arrange(ID, Month) %>% 
  mutate(Year = floor((Month - 0.1) / 12 + 2001)) %>% 
  group_by(ID, Year) %>% 
  mutate(Month = 1:n()) %>% 
  mutate(aetS = (aet - min(aet, na.rm = TRUE)) / (max(aet, na.rm = TRUE) - min(aet, na.rm = TRUE)),
         aetR = max(aet)-min(aet))
head(terraclimate.aet)
```

###### Read Soil Water grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/terraclimate/soil/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/terraclimate/soil/", raster.names, sep = ""))
plot(stack[[3]])
```

###### Extract Soil Water grids

```{r}
# extract data at sites
terraclimate.soilwater <- as_tibble(raster::extract(stack, site.coords))

# create new name vector
x <- c(as.character(c(1:216)))

# rename to retain order
terraclimate.soilwater <- terraclimate.soilwater %>% as_tibble() %>% 
  rename_all(~x)

# look at combined data
terraclimate.soilwater <- cbind(sites,terraclimate.soilwater)
head(terraclimate.soilwater)
```

###### Finalize soilwater data

```{r}
terraclimate.soilwater <- terraclimate.soilwater %>%
  gather(key = Month, value = soilwater, 15:length(terraclimate.soilwater)) %>% 
  mutate(Month = as.numeric(Month),
         soilwater = as.numeric(soilwater)) %>% 
  arrange(ID, Month) %>% 
  mutate(Year = floor((Month-0.1)/12+2001)) %>% 
  group_by(ID, Year) %>% 
  mutate(Month = 1:n()) %>% 
  mutate(soilwaterS = (soilwater-min(soilwater,na.rm=TRUE))/(max(soilwater,na.rm=TRUE)-min(soilwater,na.rm=TRUE)),
         soilwaterR = max(soilwater)-min(soilwater)) %>% 
  mutate(soilwaterS = ifelse(soilwaterR == 0 & is.na(soilwaterS), 0.5, soilwaterS))
head(terraclimate.soilwater)
```

###### Combined aet and soilwater

```{r}
# combine both aet and soilwater
terraclimate <- terraclimate.aet %>% 
  left_join(terraclimate.soilwater) %>% 
  dplyr::select(1:14, 17, 15, aet, aetS, aetR, soilwater, soilwaterS, soilwaterR)
head(terraclimate)
```

###### Check for missing data

```{r}
terraclimate %>% 
  gather(key = "var", value = "value", 15:20) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(terraclimate, paste(loc, "grids/extracted/", "terraclimate.csv", sep = ""), row.names =  F)
```

##### Fractional Vegetation Cover (VCF)

###### Read VCF grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/VCF5KYR/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/VCF5KYR/", raster.names, sep = ""))
plot(stack[[3]])
```

```{r}
# extract data at sites
vcf <- as_tibble(raster::extract(stack, site.coords))
```

###### Tiff names have year, however some years are missing. Create new name vector from years and data types:

```{r}
years_missing <- c(seq(1982,2016,1)) %>% as_tibble() %>% rename(Year = value) %>% 
  cbind(c(1:35)) %>% as_tibble() %>% rename(Row = `c(1:35)`) %>% 
  filter(Row %in% c(13, 19)) %>% dplyr::select(Year) %>% 
  pull() 

names_vcf <- c(seq(1982,2016,1)) %>% as_tibble() %>% rename(Year = value) %>% 
  cbind(c(1:35)) %>% as_tibble() %>% rename(Row = `c(1:35)`) %>% 
  filter(!Row %in% c(13, 19)) %>% dplyr::select(Year) %>% 
  pull() %>% rep(each = 3) %>% as.character()

suffix <- c("_tree","_shrub","_bare") %>% rep(33)

names_vcf <- paste(names_vcf, suffix, sep="")
```

###### Rename to retain order of data, then add missing years as means of adjacent years, and extent time series by two years to 2018 (assuming no change):

```{r}
# rename to retain order
vcf <- vcf %>% as_tibble() %>% 
  rename_all(~names_vcf)

# add in missing years
vcf <- vcf %>% mutate(row = 1:n()) %>% 
  group_by(row) %>% 
        mutate(`1994_bare` = mean(c(`1993_bare`,`1995_bare`)), 
               `1994_tree` = mean(c(`1993_tree`,`1995_tree`)), 
               `1994_shrub` = mean(c(`1993_shrub`,`1995_shrub`)), 
           
               `2000_bare` = mean(c(`1999_bare`,`2001_bare`)), 
               `2000_tree` = mean(c(`1999_tree`,`2001_tree`)), 
               `2000_shrub` = mean(c(`1999_shrub`,`2001_shrub`)),
               
               `2017_bare` = `2016_bare`,
               `2017_tree` = `2016_tree`,
               `2017_shrub` = `2016_shrub`,

               `2018_bare` = `2016_bare`,
               `2018_tree` = `2016_tree`,
               `2018_shrub` = `2016_shrub`) %>% ungroup() %>% 
  dplyr::select(-row)
```

###### look at combined data

```{r}
vcf <- cbind(sites,vcf)
head(vcf)
```
###### Finalize dataset: gather data split year and predictor

```{r}
vcf <- vcf %>%
  gather(key = Year, value = value, 15:length(vcf)) %>% 
  mutate(Predictor = substr(Year,6,10),
         Year = substr(Year,1,4)) %>% 
  spread(key = Predictor, value = value)
head(vcf)
```
###### Plot data

```{r}
vcf %>% 
  gather(key = "vcf_var", value = value, 16:18) %>% 
  ggplot(aes(value)) + 
  geom_histogram() +
  facet_wrap(~vcf_var) +
  my_theme
```
Data looks reasonable, all between 0 and 100.

###### Check that trends look reasonable:

```{r}
vcf %>% 
  gather(key = "vcf_var", value = value, 16:18) %>% 
  ggplot(aes(Year, value, color = vcf_var)) + 
  geom_point() +
  facet_wrap(~ID) +
  my_theme
```

US-AO3 looks weird (zeros)


###### Replace USA03 with USA10 data, due to strange behavior:

```{r}
vcf[vcf$ID == "USA03", 16:18] <- vcf[vcf$ID == "USA10", 16:18]
```

###### Write data

```{r}
write.csv(vcf, paste(loc, "grids/extracted/", "vcf.csv", sep = ""), row.names =  F)
```

##### Wetland Extent (WAD2M)

###### Get rasters

Use the coarser 0.5-degree map because 0.25-degree edge roughness causes more missing data

```{r}
setwd(loc)
raster.names <-list.files(paste(loc, "grids/WAD2M/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/WAD2M/", sep = "", raster.names[2]))
plot(stack[[7]])
points(site.coords)
```
###### Extract data at sites and look at it

```{r}
wf <- as_tibble(raster::extract(stack, site.coords))
wf <- cbind(sites,wf)
head(wf)
```

###### Finalize WF Seasonality:

  - Create date, month, and year columns from varnames
  - Replace any missing data with zero 
      + some sites are marine/rice paddy, or coastal  (false, need correction where freshwater wetland, see below)
      + frozen high latitude locations in winter are assigned NA (real, no correction)
  - Calculate seasonality

```{r}
wf <- wf %>%
  gather(key = Month, value = WF, 15:length(wf)) %>% 
  mutate(Month2 = Month,
         Date = substr(Month2,2,11),
         Year = substr(Month2,2,5),
         Month = as.factor(as.integer(substr(Month2,7,8)))) %>% 
  dplyr::select(-Month2) %>% 
  mutate(Date = as.Date(Date, format = "%Y.%m.%d"),
         DOY = yday(Date)) %>% 
  group_by(ID, Year) %>% 
  mutate(WF = replace_na(WF,0),
         WFS = (WF-min(WF,na.rm=TRUE))/(max(WF,na.rm=TRUE)-min(WF,na.rm=TRUE))) %>% 
  group_by(ID, Year) %>% 
  mutate(WFR = max(WF)-min(WF)) %>% 
  ungroup() %>% 
  mutate(WFS = ifelse(WFR == 0 & is.na(WFS), 0.5, WFS)) %>% 
  dplyr::select(1:14, Date, Year, Month, DOY, WF, WFS, WFR)
```

###### Look at data patterns

```{r}
wf %>% 
  gather(key = "wf_var", value = "value", 19) %>% 
  ggplot(aes(Date, value, color = wf_var)) +
  geom_point() +
  facet_wrap(~ID, scale = 'free')
```
There are missing data for: "DEHte", "HKMPM", "JPSwL", "KRCRK", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USHRA", "USHRC", "USNC4", "USNGB"
  
Of these, "HKMPM", "JPSwL", "KRCRK", "USHRA", "USHRC", are either lakes, rice, or mangroves (i.e. cannot easily fill missing data)

But these can be corrected: "DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNC4", "USNGB"

###### Get bounding boxes for each correctable site

```{r}
coords_subset <- site.coords[sites$ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNC4", "USNGB"),] 
coords_subset_names <- sites$ID[sites$Latitude %in% coords_subset]

bounding_box <- function(x) {
  c("xmin" =  x[1] - 3, "xmax" = x[1] + 3, 
               "ymin" = x[2] - 3 , "ymax" =  x[2] + 3)
}

bounding_boxes <- list()
for(i in 1:nrow(coords_subset)){
 bounding_boxes[[i]] <- bounding_box(coords_subset[i,])
}

bounding_boxes_maps <- list()
for(i in 1:length(bounding_boxes)){
  bounding_boxes_maps[[i]] <- crop(stack[[8]], bounding_boxes[[i]])
}
```

###### Match up longitudes with site names:

```{r}
sites %>% 
  filter(Longitude %in% coords_subset[,1])
```

###### Look at each site:

```{r}
for(i in 1:length(bounding_boxes)){
  plot(bounding_boxes_maps[[i]])
  points(coords_subset)
}
```

Looks like moving all 1:6 and 8 south by 1 degree, and moving 7 west by 1 degree will help with extractions:

###### Adjust lat/longs and reextract just these 8 sites

```{r}
site.coords.temp <- sites %>% 
  filter(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB", "USNC4")) %>% 
  mutate(Latitude = ifelse(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNGB"), Latitude - 0.35, Latitude),
         Longitude = ifelse(ID == "USNC4", Longitude - 0.25, Longitude)) %>% 
  dplyr::select(Longitude, Latitude) %>% 
  as.matrix() %>% unname()
```

###### Look just at the locations of these adjusted sites:

```{r}
for(i in 1:length(bounding_boxes)){
  plot(bounding_boxes_maps[[i]])
  points(site.coords.temp)
}
```

###### Re-extract at adjusted sites:

```{r}
sites.temp <- sites %>% 
  filter(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB", "USNC4"))
         
wf.temp <- as_tibble(raster::extract(stack, site.coords.temp))
wf.temp <- cbind(sites.temp, wf.temp)
head(wf.temp)
```

###### Finalize WF Seasonality for temporary sites

```{r}
wf.temp <- wf.temp %>%
  gather(key = Month, value = WF, 15:length(wf.temp)) %>% 
  mutate(Month2 = Month,
         Date = substr(Month2,2,11),
         Year = substr(Month2,2,5),
         Month = as.factor(as.integer(substr(Month2,7,8)))) %>% 
  dplyr::select(-Month2) %>% 
  mutate(Date = as.Date(Date, format = "%Y.%m.%d"),
         DOY = yday(Date)) %>% 
  group_by(ID, Year) %>% 
  mutate(WF = replace_na(WF,0),
         WFS = (WF-min(WF,na.rm=TRUE))/(max(WF,na.rm=TRUE)-min(WF,na.rm=TRUE))) %>% 
  group_by(ID, Year) %>% 
  mutate(WFR = max(WF)-min(WF)) %>% 
  ungroup() %>% 
  mutate(WFS = ifelse(WFR == 0 & is.na(WFS), 0.5, WFS)) %>% 
  dplyr::select(1:14, Date, Year, Month, DOY, WF, WFS, WFR)
```

###### Rejoin data

```{r}
wf.rm <- wf %>% 
  filter(!ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB", "USNC4"))

wf <- wf.rm %>% 
  bind_rows(wf.temp) %>% 
  arrange(ID, Date)
```

###### Look at data patterns

```{r}
wf %>% 
  gather(key = "wf_var", value = "value", 19) %>% 
  ggplot(aes(Date, value, color = wf_var)) +
  geom_point() +
  facet_wrap(~ID, scale = 'free')
```

Data are now complete (rice, mangrove, and lake sites still ~0 but cannot be corrected, USNC4 was a wetland and has been corrected)

###### Write data

```{r}
write.csv(wf, paste(loc, "grids/extracted/", "wf.csv", sep = ""), row.names =  F)
```

##### WorldClim 2.0

```{r warning = F}
setwd(loc)
raster.names <-list.files(paste(loc, "grids/worldclim/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/worldclim/", sep = "", raster.names))
plot(stack[[7]])
points(site.coords)
```

###### Extract worldclim data

```{r}
worldclim <- as_tibble(raster::extract(stack, site.coords)) 
worldclim <- cbind(sites, worldclim) 
head(worldclim)
```

###### Check for missing data

```{r}
worldclim %>% 
  filter(is.na(wc2.0_bio_30s_02))
```
HKMPM data are missing. 

###### Look at HKMPM location:

```{r}
coords_subset <- site.coords[sites$ID %in% c("HKMPM"),] 
coords_subset_names <- sites$ID[sites$Latitude %in% coords_subset]

bounding_boxes <- bounding_box(coords_subset)

bounding_boxes_maps <- crop(stack[[1]], bounding_boxes)

plot(bounding_boxes_maps)
points(114.02924,  22.49817)
```
Can move onto nearby coastline by shifting west

###### Shift HKMPM west:

```{r}
site.coords.temp <- sites %>% 
  filter(ID %in% c("HKMPM")) %>% 
  mutate(Longitude = Longitude + 0.05) %>% 
  dplyr::select(Longitude, Latitude) %>% 
  as.matrix() %>% unname()

sites.temp <- sites %>% 
  filter(ID %in% c("HKMPM")) 
         
worldclim.temp <- as_tibble(raster::extract(stack, site.coords.temp))
worldclim.temp <- cbind(sites.temp, worldclim.temp)
head(worldclim.temp)
```
###### Rejoin

```{r}
worldclim <- worldclim %>% 
  filter(!ID == "HKMPM") %>% 
  bind_rows(worldclim.temp) %>% 
  arrange(ID)
```

###### Create new name vector to match 19 products
```{r}
newnames <- c("wc_mat", "wc_dtr", "wc_iso", "wc_ts",
       "wc_mtqm", "wc_mtcm", "wc_tr", "wc_mtwtq",
       "wc_mtdq", "wc_mtwq", "wc_mtcq", "wc_map",
       "wc_pwtm", "wc_pdm", "wc_ps", "wc_pwtq",
       "wc_pdq", "wc_pwq", "wc_pcq")

oldnames <- names(worldclim)[15:33]
worldclim <- worldclim %>% 
 rename_at(vars(oldnames), ~ newnames)
```
###### Write worlclim data

```{r}
write.csv(worldclim, paste(loc, "grids/extracted/worldclim.csv", sep = ""), row.names = F)
```


#### *Grid Data Merge*

###### Read in all .csv files

```{r, warning = F}
setwd(loc)
csv.names <- list.files(paste(loc, "grids/extracted/", sep  = ""), pattern = "csv$", full.names = FALSE)
geospatial <- lapply(paste(loc, "grids/extracted/", csv.names, sep = ""), read.csv) 
names(geospatial) <- csv.names
```

###### Join static data

```{r, message = F, warning = F}

metadata_names <- names(geospatial$canopyht.csv)[2:14]

remove_metadata <- function(x) {
  x %>% 
    dplyr::select(-metadata_names)
}

canopyht <- remove_metadata(geospatial$canopyht.csv)
cti <- remove_metadata(geospatial$cti.csv)
worldclim <- remove_metadata(geospatial$worldclim.csv)
soilgrids <- remove_metadata(geospatial$soilgrids.csv)
earthenv <- remove_metadata(geospatial$earthenv.csv)

geospatial_data_static <- canopyht %>% 
  left_join(cti) %>% 
  left_join(worldclim) %>% 
  left_join(soilgrids) %>% 
  left_join(earthenv)

```
###### Join temporal data

```{r}
geospatial_data_temporal <- as_tibble(geospatial$wf.csv) %>% dplyr::select(-DOY, -Date) %>% # start with wetland fraction then add others
  left_join(geospatial$terraclimate.csv) %>% 
  left_join(geospatial$ns_depo.csv) %>% 
  left_join(geospatial$vcf.csv) %>% 
  left_join(geospatial$rpot) 
```
###### Merge static and temporal data

```{r}
geospatial_data <- geospatial_data_temporal %>% 
  full_join(geospatial_data_static)
```

###### Look at `geospatial_data` NAs

```{r}
nas <- geospatial_data %>% 
  group_by(ID, Latitude, Longitude) %>% 
  summarize_all(~sum(is.na(.))) %>% 
  dplyr::select(-Year, -Month) %>% 
  ungroup() %>% 
  gather(key = variable, value = NAs, 15:85) %>% 
  filter(NAs > 0)
nas
```

There is some missing data for 2000 for the Terraclimate temporal products.  Can impute at later step using caret.

###### Write out NA file

```{r}
write.csv(nas, paste(loc, "/grids/extracted/extracted_nas.csv", sep = ""), row.names = FALSE)
```

###### Write out `geospatial_data` file

```{r}
write.csv(geospatial_data, paste(loc, "/grids/extracted-final/geospatial.csv", sep = ""), row.names = FALSE)
```

#### *Merge geospatial and modis data*

```{r}
geospatial_data <- read.csv(paste(loc, "grids/extracted-final/geospatial.csv", sep = "")) %>% as_tibble()

modis_data <- read.csv(paste(loc, "modis/modis-processed/modis-processed.csv", sep = "")) %>% 
  dplyr::select(-NDSI, -NDVI, -EVI, -LAI, -NDWI, -SRWI, -LSWI, -LSTN) %>% 
  rename(NDSI = NDSI_F, NDVI = NDVI_F, EVI = EVI_F, LAI = LAI_F, NDWI = NDWI_F, SRWI = SRWI_F, LSWI = LSWI_F, LSTN = LSTN_F) %>% 
  as_tibble()

modis_geo <- modis_data %>% left_join(geospatial_data) %>% 
  dplyr::select(ID, Site.Name, Data.Policy, Country, Latitude, Longitude, IGBP, Site.Classification, Upland.Class, DOI,
                DOI.Reference, Site.Personnel, Base.File.Host, Upscaling, Year, Month, Week, everything()) 
```

###### Write out modis_geo data file

```{r}
write.csv(modis_geo, paste(loc, "training-data/modis_geospatial.csv", sep = ""), row.names = FALSE)
```

#### *QAQC Plots for all Predictors*

##### Read in `modis_geospatial.csv`:

```{r}
modis_geo <- read.csv(paste(loc, "training-data/modis_geospatial.csv", sep = ""))
```

##### Computed Predictors (rpot)

```{r}
modis_geo %>% 
  ggplot(aes(Month, rpot)) +
  geom_line() +
  facet_wrap(~ID) +
  labs(y = expression("Potential Radiation at the top of the Atmosphere (W m"^{-2}*")")) +
  scale_x_continuous(breaks = c(6, 12), labels = c(6, 12)) + 
  my_theme
ggsave(paste(loc, "grids/extracted-qc/rpot.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

First QAQC evaluation issues:

  + Missing data for CADBB; JPSwl; PHRlf; RUSAM; SESto; USBgl; USBrw; USIcs 
  + 3 of these are site mis-matches between modis and geospatial data (CADBB, RUSAM, SESto, USBgl, USBrw do not exist in `fluxnet-ch4-site-metadata.csv`)
  + The others are possibly mis-matches in spelling (JPSwL, PHRlF, USICs is the correct spelling, in `fluxnet-ch4-site-metadata.csv`)

Second QAQC evaluation:
  
  + No issues. Corrected modis input data site names.

##### WorldClim Predictors

###### Get Indices

```{r}
names(modis_geo)
```

Need 85-103.

###### Plot all worldclim

```{r}
oldnames <- names(modis_geo)[85:103]
newnames <- letters[1:19]

names_key <- setNames(oldnames, newnames)

modis_geo %>% 
  rename_at(vars(oldnames), ~ newnames) %>% 
  gather(key = "wc_var", value = "value", 85:103) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, value, color = group)) +
  geom_point() +
  facet_wrap(~wc_var, scales = "free", labeller = as_labeller(names_key)) +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

ggsave(paste(loc, "grids/extracted-qc/worldclim.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Ranges look reasonable. No outliers.

##### EarthEnv Predictors

Need 110-136

###### Plot earthenv

```{r}
oldnames <- names(modis_geo)[110:136]
newnames <- c(letters, "ab")

names_key <- setNames(oldnames, newnames)

modis_geo %>% 
  rename_at(vars(oldnames), ~ newnames) %>% 
  gather(key = "earthenv", value = "value", 110:136) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, value, color = group)) +
    geom_point() +
    facet_wrap(~earthenv, scales = 'free', labeller = as_labeller(names_key)) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/earthenv.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

##### Terraclimate Predictors

Need 69-74.

###### Plot the actual values

```{r}
# aet
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, aet, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID, scales = 'free') +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration (mm/month)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aet.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil water
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, soilwater, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture (mm/month)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwater.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

###### Plot the ranges

```{r}
# aet
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Year, aetR, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration Annual Range (mm)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aetR.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil water
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Year, soilwaterR, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture Annual Range (mm)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwaterR.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

###### Plot the seasonality

```{r}
# aetS
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, aetS, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration Annual Seasonality (unitless)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aetS.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil waterS
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, soilwaterS, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture Annual Seasonality (unitless)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwaterS.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

QC issues:

  + `aet` looks fine
  + `soilwater` seasonality is very variable within some sites, and obviously wrong in locations where wetlands are not the dominant component of the pixel
  (e.g., US-TW1 through US-Twt, which are delta marshes embedded in a dryland Mediterranean climate)

##### VCF Predictors 

Need 79-81

```{r}
modis_geo %>% 
  gather(key = "vcf_var", value = "value", 79:81) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, value, color = group)) +
  geom_point() +
  facet_wrap(~vcf_var, scales = "free") +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/vcf.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

###### Does each cover type (shrub, tree, bare) sum to 100?
```{r}
modis_geo %>% 
  mutate(vcf_sum = bare + shrub + tree) %>% 
    ggplot(aes(ID, vcf_sum)) +
    geom_point() +
    my_theme
ggsave(paste(loc, "grids/extracted-qc/vcf_var_sum.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

First QC passed: everything looks good, and cover types at each site sum to 100.


##### CTI Predictors 

```{r}
modis_geo %>% 
  dplyr::select(ID, cti) %>% 
  group_by(ID) %>% 
  summarize(ID = ID[1], cti = cti[1]) %>% 
  ggplot(aes(ID, cti, label = ID)) +
  geom_point() +
  geom_text(hjust = 0, nudge_x = 0.1) +
      my_theme +
  labs(y = "Compound Topographic Index (GeoMorpho90m") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/cti.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

First QAQC issues:

  - USHRA seems anomalously high. Go and check why in raster prep. 
      + After inspection, corrected USHRA to USHRC value, given proximity and no visible diff. in sites. (i.e. CTI product issue)
  - Other patterns seem to make sense, `CHDav` is in the alps and should have a low/negative CTI

##### Wetland Fraction (WF) Predictors 

66 (WF)
67 (WFS)
68 (WFR)

###### Plot the actual values

```{r}
# WF
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, WF, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID, scales = 'free') +
  my_theme +
  theme(axis.text.x = element_blank()) +
  labs(y = "Wetland Fraction (WAD2M)")
ggsave(paste(loc, "grids/extracted-qc/wad2m-wf.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

WF QC: 

  - HKMPM, JPSwL, KRCRK, USHRA, USHRC and USNC4 all show 0 wetland fraction year-round
      + Correct HKMPM and JPSwL to 1 year-round, because they are a mangrove and lake respectively (always flooded)
      + Map and shift the others coords to get a better value

###### Plot the ranges

```{r}
# WFR
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, WFR, color = group)) +
    geom_point(size = 0.5, angle = 90) +
  my_theme +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(y = "Wetland Fraction Range (WAD2M)")
ggsave(paste(loc, "grids/extracted-qc/wad2m-wfr.pdf", sep = ""),
       width = 40, height = 30, units = c("cm"), dpi = 300)
```

###### Plot the seasonality

```{r}
# WFS
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, WFS, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID, scales = 'free') +
  my_theme +
  theme(axis.text.x = element_blank()) +
  labs(y = "Wetland Fraction Seasonality (WAD2M)")
ggsave(paste(loc, "grids/extracted-qc/wad2m-wfs.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

##### canopyht Predictors

```{r}
modis_geo %>% 
  dplyr::select(ID, canopyht) %>% 
  group_by(ID) %>% 
  summarize(ID = ID[1], canopyht = canopyht[1]) %>% 
  ggplot(aes(ID, canopyht, label = ID)) +
  geom_point() +
  geom_text(hjust = 0, nudge_x = 0.1) +
      my_theme +
  labs(y = "Canpoy Height (m)") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/canopyht.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Looks OK!

##### NS Deposition Predictors

Get 75-78

```{r}
modis_geo %>% 
  gather(key = "ns_depo_var", value = "value", 75:78) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  group_by(ID, ns_depo_var) %>% 
  summarize(ID = ID[1], ns_depo_var = ns_depo_var[1], value = value[1], group = group[1]) %>% 
  ggplot(aes(ID, value, color = group, label = ID)) +
  geom_point() +
  geom_text(hjust = 0, nudge_x = 0.5, size = 2) +
  facet_wrap(~ns_depo_var, scales = "free") +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/ns_depo.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Looks fine. 

##### SoilGrids250m Predictors

Get 104-109.

```{r}
modis_geo %>% 
  gather(key = "soilgrids_var", value = "value", 104:109) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  group_by(ID, soilgrids_var) %>% 
  summarize(ID = ID[1], soilgrids_var = soilgrids_var[1], value = value[1], group = group[1]) %>% 
  ggplot(aes(ID, value, color = group, label = ID)) +
  geom_point() +
  geom_text(hjust = 0, nudge_x = 0.5, size = 2) +
  facet_wrap(~soilgrids_var, scales = "free") +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/soilgrids.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Looks fine!

#### 5. Finalize Training Data

##### Merge Flux and Geospatial Data

```{r}
fluxes <- read.csv(paste(loc, "fluxnet-ch4-data/weekly_flat/weekly_subset_meta.csv", sep = ""))
modis_geo <- read.csv(paste(loc, "training-data/modis_geospatial.csv", sep = ""))
names(fluxes)
names(modis_geo)
```

```{r}
modis_geo_subset <- modis_geo %>% 
  dplyr::select(ID, Year, Month, Week, 18:136)

flux_modis_geo_weekly <- fluxes %>% 
  left_join(modis_geo_subset)
```

##### Get wetland flux sites only (45 sites):

```{r}
wetland_ids <- levels(as.factor(fluxes$ID))

flux_modis_geo_weekly <- flux_modis_geo_weekly %>% 
  filter(ID %in% wetland_ids)
```

##### Write merged dataset `flux_modis_geo_weekly`:

```{r}
write.csv(flux_modis_geo_weekly, paste(loc, "/training-data/flux_modis_geo_weekly.csv", sep = ""), row.names = F)
names(flux_modis_geo_weekly)
```

##### Output Flux QC Plots

```{r}
min(flux_modis_geo_weekly$Year)
max(flux_modis_geo_weekly$Year)
years <- c(2006:2018)

# scales fixed
for(i in 1:length(years)) {
  flux_modis_geo_weekly %>% 
  filter(Year == years[i]) %>% 
    mutate(obs_threshold = ifelse(imputed < 0.6, "yes", "no")) %>% 
  ggplot(aes(Week, FCH4, color = factor(obs_threshold))) + 
    geom_point() +
    facet_wrap(~ID) +
    # scale_y_log10() +
    labs(y = expression("FCH4 (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste(loc, "fluxnet-ch4-data/flux-qc/", years[i], "_fixed.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
}

# scales free
for(i in 1:length(years)) {
  flux_modis_geo_weekly %>% 
  filter(Year == years[i]) %>% 
    mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
  ggplot(aes(Week, FCH4, color = factor(obs_threshold))) + 
    geom_point() +
    facet_wrap(~ID, scales = 'free') +
    # scale_y_log10() +
    labs(y = expression("FCH4 (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste(loc, "fluxnet-ch4-data/flux-qc/", years[i], "_free.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
}
```

Fluxes QC: 
  - Remove `USSne` Data for 2016 (wetland not yet restored) 

#### 5. (cont) Reload All Data (`flux_modis_geo_weekly`)

```{r}
all_data <- read.csv(paste(loc, "training-data/flux_modis_geo_weekly.csv", sep = "")) 
names(all_data)

```

##### Adjust Site Data: remove USSne year 2016

```{r}
all_data <- all_data %>%  
  mutate(remove = ifelse(ID == "USSne" & Year == 2016, 1, 0)) %>% 
  filter(remove == 0) %>% dplyr::select(-remove)
```

##### Look at data, and extent of missing data

```{r}
str(all_data)
```

##### Summarize missingness in predictors:

```{r}
all_data %>% 
   dplyr::select(-`Site.Name`, -`Data.Policy`, -Country, -IGBP, -`Site.Classification`, -DOI,  -`DOI.Reference`, -`Site.Personnel`, 
                -`Upland.Class`, -`Base.File.Host`, -Upscaling, -FCH4, -FCH4_UNC, -imputed) %>% 
  group_by(ID) %>% 
  summarize_all(list(~sum(is.na(.)) / n() )) %>% 
  write.csv(paste(loc, "training-data/missingness/predictor_missingness_weekly.csv", sep = ""), row.names = F)

```

##### Remove extraneous variables and non-class-double data and impute the first 40 predictors

Using predict function will not work with mixed-classes.
Most of the missing data are in the first EC tower and MODIS variables.


```{r}
all_data <- all_data %>% 
  dplyr::select(-PPFD_IN, -NETRAD, -H, -RH, -USTAR, -WS, -TS, -SWC, -WTD, -LAI, -LAI_msc, -LAI_mean, -LAI_min, -LAI_max, -LAI_amp)

all_data_impute <- all_data %>% 
  dplyr::select(-ID, -`Site.Name`, -`Data.Policy`, -Country, -IGBP, -`Site.Classification`, -DOI,  -`DOI.Reference`, -`Site.Personnel`, 
                -`Upland.Class`, -`Base.File.Host`, -Upscaling, -FCH4, -FCH4_UNC, -imputed) 

impute_names <- all_data_impute %>% names()


pp <- preProcess(all_data_impute, method = "bagImpute")
all_data_impute_pp <- predict(pp, all_data_impute)
rm(pp)
all_data[impute_names] <- all_data_impute_pp
```

##### Summarize missingness after imputation in predictors:

```{r}
all_data %>% 
   dplyr::select(-`Site.Name`, -`Data.Policy`, -Country, -IGBP, -`Site.Classification`, -DOI,  -`DOI.Reference`, -`Site.Personnel`,
                -`Upland.Class`, -`Base.File.Host`, -Upscaling, -FCH4, -FCH4_UNC, -imputed) %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)) / n() )) %>% 
  write.csv(paste(loc, "training-data/missingness/predictor_missingness_after_impute_weekly.csv", sep = ""), row.names = F)
```

##### There are still some missing data, check where:

```{r}
all_data %>% 
  filter(is.na(wc_mtdq))
```
##### Fix by set Month 2, Week 8 equal to Month 2 Week 7: [ for daily data only ]

```{r}
# all_data[all_data$Year == 2016 & all_data$Month == 2 & all_data$Week == 8 , 57:145] <- all_data[all_data$Year == 2016 & all_data$Month == 2 & all_data$Week == 7 , 57:145]
```

##### Summarize missingness after manual imputation in predictors: [ for daily data only ]

```{r}
# all_data %>% 
#    dplyr::select(-`Site.Name`, -`Data.Policy`, -Country, -IGBP, -`Site.Classification`, -DOI,  -`DOI.Reference`, -`Site.Personnel`,
#                 -`Upland.Class`, -`Base.File.Host`, -Upscaling, -FCH4, -FCH4_F_UNC, -imputed) %>%
#   group_by(ID) %>%
#   summarize_all(list(~sum(is.na(.)) / n() )) %>% 
#   write.csv(paste(loc, "training-data/missingness/predictor_missingness_after_impute_after_manual.csv", sep = ""), row.names = F)
```

##### Add lagged variables

```{r}
all_data <- all_data %>% 
  group_by(ID) %>% 
  mutate(rpot_LAG4 = lag(rpot, 4, default = median(rpot[1:4], na.rm=TRUE)), ## generic monthly seasonality (4, 8, 12)
         rpot_LAG8 = lag(rpot, 8, default = median(rpot[1:8], na.rm=TRUE)), 
         rpot_LAG12 = lag(rpot, 12, default = median(rpot[1:12], na.rm=TRUE)), 
         rpot_LEAD4 = lead(rpot, 4, default = median(rpot[(length(rpot)-4):length(rpot)], na.rm=TRUE)), 
         rpot_LEAD8 = lead(rpot, 8, default = median(rpot[(length(rpot)-8):length(rpot)], na.rm=TRUE)), 
         rpot_LEAD12 = lead(rpot, 12, default = median(rpot[(length(rpot)-12):length(rpot)], na.rm=TRUE)), 
         
         WFS_LAG4 = lag(WFS, 4, default = median(WFS[1:4], na.rm=TRUE)), ## RS monthly seasonality (4, 8, 12)
         WFS_LAG8 = lag(WFS, 8, default = median(WFS[1:8], na.rm=TRUE)), 
         WFS_LAG12 = lag(WFS, 12, default = median(WFS[1:12], na.rm=TRUE)), 
         WFS_LEAD4 = lead(WFS, 4, default = median(WFS[(length(WFS)-4):length(WFS)], na.rm=TRUE)), 
         WFS_LEAD8 = lead(WFS, 8, default = median(WFS[(length(WFS)-8):length(WFS)], na.rm=TRUE)), 
         WFS_LEAD12 = lead(WFS, 12, default = median(WFS[(length(WFS)-12):length(WFS)], na.rm=TRUE)),
         
         LSTN_LAG1 = lag(LSTN, 1, default = median(LSTN[1:1], na.rm=TRUE)),  ## RS 1-day seasonality (1, 2, 3)
         LSTN_LAG2 = lag(LSTN, 2, default = median(LSTN[1:2], na.rm=TRUE)), 
         LSTN_LAG3 = lag(LSTN, 3, default = median(LSTN[1:3], na.rm=TRUE)), 
         LSTN_LEAD1 = lead(LSTN, 1, default = median(LSTN[(length(LSTN)-1):length(LSTN)], na.rm=TRUE)), 
         LSTN_LEAD2 = lead(LSTN, 2, default = median(LSTN[(length(LSTN)-2):length(LSTN)], na.rm=TRUE)), 
         LSTN_LEAD3 = lead(LSTN, 3, default = median(LSTN[(length(LSTN)-3):length(LSTN)], na.rm=TRUE)),
         
         NDSI_LAG1 = lag(NDSI, 1, default = median(NDSI[1:1], na.rm=TRUE)), 
         NDSI_LAG2 = lag(NDSI, 2, default = median(NDSI[1:2], na.rm=TRUE)), 
         NDSI_LAG3 = lag(NDSI, 3, default = median(NDSI[1:3], na.rm=TRUE)), 
         NDSI_LEAD1 = lead(NDSI, 1, default = median(NDSI[(length(NDSI)-1):length(NDSI)], na.rm=TRUE)), 
         NDSI_LEAD2 = lead(NDSI, 2, default = median(NDSI[(length(NDSI)-2):length(NDSI)], na.rm=TRUE)), 
         NDSI_LEAD3 = lead(NDSI, 3, default = median(NDSI[(length(NDSI)-3):length(NDSI)], na.rm=TRUE)),
         
         NDVI_LAG1 = lag(NDVI, 1, default = median(NDVI[1:1], na.rm=TRUE)), 
         NDVI_LAG2 = lag(NDVI, 2, default = median(NDVI[1:2], na.rm=TRUE)), 
         NDVI_LAG3 = lag(NDVI, 3, default = median(NDVI[1:3], na.rm=TRUE)), 
         NDVI_LEAD1 = lead(NDVI, 1, default = median(NDVI[(length(NDVI)-1):length(NDVI)], na.rm=TRUE)), 
         NDVI_LEAD2 = lead(NDVI, 2, default = median(NDVI[(length(NDVI)-2):length(NDVI)], na.rm=TRUE)), 
         NDVI_LEAD3 = lead(NDVI, 3, default = median(NDVI[(length(NDVI)-3):length(NDVI)], na.rm=TRUE)),
         
         EVI_LAG1 = lag(EVI, 1, default = median(EVI[1:1], na.rm=TRUE)), 
         EVI_LAG2 = lag(EVI, 2, default = median(EVI[1:2], na.rm=TRUE)), 
         EVI_LAG3 = lag(EVI, 3, default = median(EVI[1:3], na.rm=TRUE)), 
         EVI_LEAD1 = lead(EVI, 1, default = median(EVI[(length(EVI)-1):length(EVI)], na.rm=TRUE)), 
         EVI_LEAD2 = lead(EVI, 2, default = median(EVI[(length(EVI)-2):length(EVI)], na.rm=TRUE)), 
         EVI_LEAD3 = lead(EVI, 3, default = median(EVI[(length(EVI)-3):length(EVI)], na.rm=TRUE)),
         
         NDWI_LAG1 = lag(NDWI, 1, default = median(NDWI[1:1], na.rm=TRUE)), 
         NDWI_LAG2 = lag(NDWI, 2, default = median(NDWI[1:2], na.rm=TRUE)), 
         NDWI_LAG3 = lag(NDWI, 3, default = median(NDWI[1:3], na.rm=TRUE)), 
         NDWI_LEAD1 = lead(NDWI, 1, default = median(NDWI[(length(NDWI)-1):length(NDWI)], na.rm=TRUE)), 
         NDWI_LEAD2 = lead(NDWI, 2, default = median(NDWI[(length(NDWI)-2):length(NDWI)], na.rm=TRUE)), 
         NDWI_LEAD3 = lead(NDWI, 3, default = median(NDWI[(length(NDWI)-3):length(NDWI)], na.rm=TRUE)),
         
         SRWI_LAG1 = lag(SRWI, 1, default = median(SRWI[1:1], na.rm=TRUE)), 
         SRWI_LAG2 = lag(SRWI, 2, default = median(SRWI[1:2], na.rm=TRUE)), 
         SRWI_LAG3 = lag(SRWI, 3, default = median(SRWI[1:3], na.rm=TRUE)), 
         SRWI_LEAD1 = lead(SRWI, 1, default = median(SRWI[(length(SRWI)-1):length(SRWI)], na.rm=TRUE)), 
         SRWI_LEAD2 = lead(SRWI, 2, default = median(SRWI[(length(SRWI)-2):length(SRWI)], na.rm=TRUE)), 
         SRWI_LEAD3 = lead(SRWI, 3, default = median(SRWI[(length(SRWI)-3):length(SRWI)], na.rm=TRUE)),
         
         LSWI_LAG1 = lag(LSWI, 1, default = median(LSWI[1:1], na.rm=TRUE)), 
         LSWI_LAG2 = lag(LSWI, 2, default = median(LSWI[1:2], na.rm=TRUE)), 
         LSWI_LAG3 = lag(LSWI, 3, default = median(LSWI[1:3], na.rm=TRUE)), 
         LSWI_LEAD1 = lead(LSWI, 1, default = median(LSWI[(length(LSWI)-1):length(LSWI)], na.rm=TRUE)), 
         LSWI_LEAD2 = lead(LSWI, 2, default = median(LSWI[(length(LSWI)-2):length(LSWI)], na.rm=TRUE)), 
         LSWI_LEAD3 = lead(LSWI, 3, default = median(LSWI[(length(LSWI)-3):length(LSWI)], na.rm=TRUE)),
         
         
         soilwater_LAG4 = lag(soilwater, 4, default = median(soilwater[1:4], na.rm=TRUE)),  ## MET monthly seasonality (4, 8, 12)
         soilwater_LAG8 = lag(soilwater, 8, default = median(soilwater[1:8], na.rm=TRUE)), 
         soilwater_LAG12 = lag(soilwater, 12, default = median(soilwater[1:12], na.rm=TRUE)), 
         soilwater_LEAD4 = lead(soilwater, 4, default = median(soilwater[(length(soilwater)-4):length(soilwater)], na.rm=TRUE)), 
         soilwater_LEAD8 = lead(soilwater, 8, default = median(soilwater[(length(soilwater)-8):length(soilwater)], na.rm=TRUE)), 
         soilwater_LEAD12 = lead(soilwater, 12, default = median(soilwater[(length(soilwater)-12):length(soilwater)], na.rm=TRUE)),
         
         soilwaterS_LAG4 = lag(soilwaterS, 4, default = median(soilwaterS[1:4], na.rm=TRUE)),  
         soilwaterS_LAG8 = lag(soilwaterS, 8, default = median(soilwaterS[1:8], na.rm=TRUE)), 
         soilwaterS_LAG12 = lag(soilwaterS, 12, default = median(soilwaterS[1:12], na.rm=TRUE)), 
         soilwaterS_LEAD4 = lead(soilwaterS, 4, default = median(soilwaterS[(length(soilwaterS)-4):length(soilwaterS)], na.rm=TRUE)), 
         soilwaterS_LEAD8 = lead(soilwaterS, 8, default = median(soilwaterS[(length(soilwaterS)-8):length(soilwaterS)], na.rm=TRUE)), 
         soilwaterS_LEAD12 = lead(soilwaterS, 12, default = median(soilwaterS[(length(soilwaterS)-12):length(soilwaterS)], na.rm=TRUE)),
         
         aet_LAG4 = lag(aet, 4, default = median(aet[1:4], na.rm=TRUE)), 
         aet_LAG8 = lag(aet, 8, default = median(aet[1:8], na.rm=TRUE)), 
         aet_LAG12 = lag(aet, 12, default = median(aet[1:12], na.rm=TRUE)), 
         aet_LEAD4 = lead(aet, 4, default = median(aet[(length(aet)-4):length(aet)], na.rm=TRUE)), 
         aet_LEAD8 = lead(aet, 8, default = median(aet[(length(aet)-8):length(aet)], na.rm=TRUE)), 
         aet_LEAD12 = lead(aet, 12, default = median(aet[(length(aet)-12):length(aet)]), na.rm=TRUE),
         
         aetS_LAG4 = lag(aetS, 4, default = median(aetS[1:4], na.rm=TRUE)),  
         aetS_LAG8 = lag(aetS, 8, default = median(aetS[1:8], na.rm=TRUE)), 
         aetS_LAG12 = lag(aetS, 12, default = median(aetS[1:12], na.rm=TRUE)), 
         aetS_LEAD4 = lead(aetS, 4, default = median(aetS[(length(aetS)-4):length(aetS)], na.rm=TRUE)), 
         aetS_LEAD8 = lead(aetS, 8, default = median(aetS[(length(aetS)-8):length(aetS)], na.rm=TRUE)), 
         aetS_LEAD12 = lead(aetS, 12, default = median(aetS[(length(aetS)-12):length(aetS)], na.rm=TRUE)),
         
         
         TA_LAG1 = lag(TA, 1, default = median(TA[1:1], na.rm=TRUE)),  ## MET daily seasonality (1, 2, 3)
         TA_LAG2 = lag(TA, 2, default = median(TA[1:2], na.rm=TRUE)), 
         TA_LAG3 = lag(TA, 3, default = median(TA[1:3], na.rm=TRUE)), 
         TA_LEAD1 = lead(TA, 1, default = median(TA[(length(TA)-1):length(TA)], na.rm=TRUE)), 
         TA_LEAD2 = lead(TA, 2, default = median(TA[(length(TA)-2):length(TA)], na.rm=TRUE)), 
         TA_LEAD3 = lead(TA, 3, default = median(TA[(length(TA)-3):length(TA)], na.rm=TRUE)), 
         
         SW_IN_LAG1 = lag(SW_IN, 1, default = median(SW_IN[1:1], na.rm=TRUE)),  
         SW_IN_LAG2 = lag(SW_IN, 2, default = median(SW_IN[1:2], na.rm=TRUE)),  
         SW_IN_LAG3 = lag(SW_IN, 3, default = median(SW_IN[1:3], na.rm=TRUE)), 
         SW_IN_LEAD1 = lead(SW_IN, 1, default = median(SW_IN[(length(SW_IN)-1):length(SW_IN)], na.rm=TRUE)), 
         SW_IN_LEAD1 = lead(SW_IN, 2, default = median(SW_IN[(length(SW_IN)-2):length(SW_IN)], na.rm=TRUE)), 
         SW_IN_LEAD3 = lead(SW_IN, 3, default = median(SW_IN[(length(SW_IN)-3):length(SW_IN)], na.rm=TRUE)), 
         
         LW_IN_LAG1 = lag(LW_IN, 1, default = median(LW_IN[1:1], na.rm=TRUE)), 
         LW_IN_LAG2 = lag(LW_IN, 2, default = median(LW_IN[1:2], na.rm=TRUE)), 
         LW_IN_LAG3 = lag(LW_IN, 3, default = median(LW_IN[1:3], na.rm=TRUE)), 
         LW_IN_LEAD1 = lead(LW_IN, 1, default = median(LW_IN[(length(LW_IN)-1):length(LW_IN)], na.rm=TRUE)), 
         LW_IN_LEAD2 = lead(LW_IN, 2, default = median(LW_IN[(length(LW_IN)-2):length(LW_IN)], na.rm=TRUE)), 
         LW_IN_LEAD3 = lead(LW_IN, 3, default = median(LW_IN[(length(LW_IN)-3):length(LW_IN)], na.rm=TRUE)), 
         
         PA_LAG1 = lag(PA, 1, default = median(PA[1:1], na.rm=TRUE)),  
         PA_LAG2 = lag(PA, 2, default = median(PA[1:2], na.rm=TRUE)), 
         PA_LAG3 = lag(PA, 3, default = median(PA[1:3], na.rm=TRUE)), 
         PA_LEAD1 = lead(PA, 1, default = median(PA[(length(PA)-1):length(PA)], na.rm=TRUE)),
         PA_LEAD2 = lead(PA, 2, default = median(PA[(length(PA)-2):length(PA)], na.rm=TRUE)),
         PA_LEAD3 = lead(PA, 3, default = median(PA[(length(PA)-3):length(PA)], na.rm=TRUE)), 
         
         VPD_LAG1 = lag(VPD, 1, default = median(VPD[1:1], na.rm=TRUE)), 
         VPD_LAG2 = lag(VPD, 2, default = median(VPD[1:2], na.rm=TRUE)), 
         VPD_LAG3 = lag(VPD, 3, default = median(VPD[1:3], na.rm=TRUE)), 
         VPD_LEAD1 = lead(VPD, 1, default = median(VPD[(length(VPD)-1):length(VPD)], na.rm=TRUE)),
         VPD_LEAD2 = lead(VPD, 2, default = median(VPD[(length(VPD)-2):length(VPD)], na.rm=TRUE)),
         VPD_LEAD3 = lead(VPD, 3, default = median(VPD[(length(VPD)-3):length(VPD)], na.rm=TRUE)), 
         
         P_LAG1 = lag(P, 1, default = median(P[1:1], na.rm=TRUE)),  
         P_LAG2 = lag(P, 2, default = median(P[1:2], na.rm=TRUE)),  
         P_LAG3 = lag(P, 3, default = median(P[1:3], na.rm=TRUE)), 
         P_LEAD1 = lead(P, 1, default = median(P[(length(P)-1):length(P)], na.rm=TRUE)), 
         P_LEAD2 = lead(P, 2, default = median(P[(length(P)-2):length(P)], na.rm=TRUE)),
         P_LEAD3 = lead(P, 3, default = median(P[(length(P)-3):length(P)], na.rm=TRUE)), 
         
         
         NEE_LAG1 = lag(NEE, 1, default = median(NEE[1:1], na.rm=TRUE)),  ## FLUX daily seasonality (1, 2, 3)
         NEE_LAG2 = lag(NEE, 2, default = median(NEE[1:2], na.rm=TRUE)),
         NEE_LAG3 = lag(NEE, 3, default = median(NEE[1:3], na.rm=TRUE)), 
         NEE_LEAD1 = lead(NEE, 1, default = median(NEE[(length(NEE)-1):length(NEE)], na.rm=TRUE)), 
         NEE_LEAD2 = lead(NEE, 2, default = median(NEE[(length(NEE)-2):length(NEE)], na.rm=TRUE)), 
         NEE_LEAD3 = lead(NEE, 3, default = median(NEE[(length(NEE)-3):length(NEE)], na.rm=TRUE)), 
         
         GPP_LAG1 = lag(GPP, 1, default = median(GPP[1:1], na.rm=TRUE)),  
         GPP_LAG2 = lag(GPP, 2, default = median(GPP[1:2], na.rm=TRUE)),
         GPP_LAG3 = lag(GPP, 3, default = median(GPP[1:3], na.rm=TRUE)), 
         GPP_LEAD1 = lead(GPP, 1, default = median(GPP[(length(GPP)-1):length(GPP)], na.rm=TRUE)), 
         GPP_LEAD2 = lead(GPP, 2, default = median(GPP[(length(GPP)-2):length(GPP)], na.rm=TRUE)), 
         GPP_LEAD3 = lead(GPP, 3, default = median(GPP[(length(GPP)-3):length(GPP)], na.rm=TRUE)), 
         
         RECO_LAG1 = lag(RECO, 1, default = median(RECO[1:1], na.rm=TRUE)),  
         RECO_LAG2 = lag(RECO, 2, default = median(RECO[1:2], na.rm=TRUE)),  
         RECO_LAG3 = lag(RECO, 3, default = median(RECO[1:3], na.rm=TRUE)), 
         RECO_LEAD1 = lead(RECO, 1, default = median(RECO[(length(RECO)-1):length(RECO)], na.rm=TRUE)), 
         RECO_LEAD2 = lead(RECO, 2, default = median(RECO[(length(RECO)-2):length(RECO)], na.rm=TRUE)),
         RECO_LEAD3 = lead(RECO, 3, default = median(RECO[(length(RECO)-3):length(RECO)], na.rm=TRUE)), 
         
         LE_LAG1 = lag(LE, 1, default = median(LE[1:1], na.rm=TRUE)), 
         LE_LAG2 = lag(LE, 2, default = median(LE[1:2], na.rm=TRUE)),
         LE_LAG3 = lag(LE, 3, default = median(LE[1:3], na.rm=TRUE)), 
         LE_LEAD1 = lead(LE, 1, default = median(LE[(length(LE)-1):length(LE)], na.rm=TRUE)), 
         LE_LEAD2 = lead(LE, 2, default = median(LE[(length(LE)-2):length(LE)], na.rm=TRUE)), 
         LE_LEAD3 = lead(LE, 3, default = median(LE[(length(LE)-3):length(LE)], na.rm=TRUE))) %>% 
  ungroup()
```

##### Look at first 32 predictors (after imputation)

```{r}
names(all_data_impute)[6:15]
```

```{r}
# NEE
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, NEE_LAG3, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Net Ecosystem Exchange ("*mu*"mol m"^{-2}*" s"^{-1}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/NEE.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# GPP
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, GPP, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Gross Primary Production ("*mu*"mol m"^{-2}*" s"^{-1}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/GPP.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# RECO
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, RECO, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Ecosystem Respiration ("*mu*"mol m"^{-2}*" s"^{-1}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/RECO.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# SW_IN
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, SW_IN, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Incoming Shortwave Radiation (W m"^{-2}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/SW_IN.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# LW_IN
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, LW_IN, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Incoming Longwave Radiation (W m"^{-2}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/LW_IN.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# LE
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, LE, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Latent Heat Flux (W m"^{-2}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/LE.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# TA
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, TA, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Air Temperature ("*degree*"C)")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/TA.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# PA
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, PA, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Air Pressure (kPa)")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/PA.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# VPD
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, VPD, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Vapor Pressure Deficit (hPa)")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/VPD.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# P
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 0.857, "yes", "no")) %>% 
    ggplot(aes(Week, P, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Daily Precipitation (mm)")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/weekly/P.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

```

##### Filter RECO values:

```{r}
all_data <- all_data %>% 
mutate(RECO = ifelse(RECO > 20, 0, RECO))
```

##### Output: Write complete `all_data`

```{r}
write.csv(all_data, paste(loc, "/training-data/all_data.csv", sep = ""), row.names = F)
```

##### Output: Table of Final FLUXNET-CH4 Inputs: 

  + **Manually edit to create new file `list_of_predictors_metadata.csv` with class and information content variables

```{r}
names(all_data)[21:ncol(all_data)] %>% write.csv(paste(loc, "/training-data/list_of_predictors.csv", sep = ""))
```

#### 6. Forward Feature Selection (+clustering)

##### Read in data and remove upland class

```{r}
all_data <- read.csv(paste(loc, "/training-data/all_data.csv", sep = "")) %>% 
  dplyr::select(-`Upland.Class`)
```

##### Subset only days with at least one observation

```{r}
all_data <- all_data %>% 
  filter(imputed < 0.857) %>% 
  dplyr::select(-imputed)
dim(all_data)
```

##### Remove other stray NAs using complete cases

```{r}
all_data <- all_data %>% 
  filter(complete.cases(.))
dim(all_data)
```

##### Setup LOSOCV (cluster sites based on location)

```{r}
site_loc <- all_data %>% 
  group_by(ID) %>% 
  summarize(Latitude = Latitude[1],
            Longitude = Longitude[1]) 

x <- site_loc$Longitude
y <- site_loc$Latitude
xy <- SpatialPointsDataFrame(matrix(c(x, y), ncol = 2), 
                             data.frame(ID = site_loc$ID),
                             proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84"))

# calculate Euclidian distance 
site_dist <- xy %>% distm()

# cluster
site_clusters <- hclust(as.dist(site_dist), method = 'complete')

# define 300 km threshold
d <- 300000
xy$Cluster <- cutree(site_clusters, h=d) 
xy <- xy %>% as_tibble() %>% 
  dplyr::select(ID, Cluster)

# rejoin folds
all_data <- all_data %>% 
  left_join(xy, by = c("ID")) %>%
  dplyr::select(Cluster = Cluster, everything())
```

##### Summarize clusters now that data have been filtered

```{r}
# summarize folds and sites
nclusters <- max(all_data$Cluster)
nsites <- length(levels(factor(all_data$ID)))

# check data count per site and cluster
cluster_bysite <- all_data %>% 
  mutate(index = 1, 
         total = length(index),
         Class = `Site.Classification`) %>% 
  group_by(ID, Cluster, Latitude) %>% 
  summarize(Longitude = Longitude[1],
            Class = Class[1],
            count = sum(index),
            percent = count/total[1]*100) 

# cluster names
cluster_bycluster <- cluster_bysite %>% ungroup() %>% 
  mutate(total = sum(count)) %>% 
  group_by(Cluster) %>% 
  summarize(Class = Class[1],
            Latitude = mean(Latitude),
            Longitude = mean(Longitude),
            cluster_name = ID[1],
            count = sum(count),
            percent = count/total[1]*100) %>% 
  mutate(Band = ifelse(Latitude < 30, "Tropics", NA),
         Band = ifelse(Latitude < 60 & Latitude > 30, "Temperate", Band),
         Band = ifelse(Latitude > 60, "Boreal", Band)) 

# output cluster stats
write.csv(cluster_bysite, paste(loc, "/training-data/clusters/cluster_bysite.csv", sep = ""), row.names = F)
write.csv(cluster_bycluster, paste(loc, "/training-data/clusters/cluster_bycluster.csv", sep = ""), row.names = F)
```

##### Output training data (with clusters)


##### Visualize clusters on global map

```{r}
# ggplot theme
theme_map <- theme_bw() +
  theme(panel.border = element_blank(),
        axis.title=element_text(size=14), axis.text=element_text(size=14),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_blank(),
        legend.position = "none",
        strip.text = element_text(face="bold", size=10),
        strip.background = element_rect(fill='white', colour='white',size=1))

# get cluster stats
cluster_count <- read.csv(paste0(loc, "training-data/clusters/cluster_bycluster.csv")) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"))

# get WAD2M
raster.names <- list.files(paste0(loc, "grids/WAD2M/"), pattern = "nc$", full.names = F)
stack <- stack(paste0(loc, "grids/WAD2M/", raster.names[1]))

# get maximum wetland extent (August)
max <- stack[[8]]

# convert to spatial dataframe
max_spdf <- as(max, "SpatialPixelsDataFrame")
max_df <- as.data.frame(max_spdf)
colnames(max_df) <- c("value", "x", "y")

st_crs(cluster_count) <- 4236

# get colors
col_vector <-  c("#E78AC3","#F1E2CC","#33A02C","#377EB8","#E41A1C","#999999")

# load worldmap
world <- ne_countries(scale = "medium", returnclass = "sf")

world <- map_data("world")

ggplot() +
  geom_polygon(data = world, aes(x = long, y = lat, group = group), fill = 'light grey') +
  geom_raster(data=max_df, aes(x=x, y=y, fill = log10(value)), alpha=1) +
  geom_sf(data = cluster_count, aes(color = factor(Class), size = percent), alpha = 0.9) + 
  xlab(NULL) + ylab(NULL) +
  scale_color_manual(values = col_vector) +
  # scale_shape_manual(values = c(15,16,21,22,25)) +
  scale_fill_gradient2(name = "WAD2M Wetlands (%)",
                       limits = c(-3,2),
                       labels = c(0.001,0.01,0.1,1,10,100),
                       low = "light grey",
                       mid = "light grey",
                      midpoint = log10(0.001),
                      high = "black",
                      space = "Lab",
                      na.value = "light grey",
                      guide = "colourbar") +
  ggtitle("LOOCV Clusters", subtitle = paste0("(43 sites in ", length(unique(cluster_count$Cluster))," clusters)")) + 
  theme_map +
  theme(legend.position = 'right',
        legend.direction = 'vertical')
ggsave(paste0(loc, "training-data/clusters/clusters_map.png"),
       width = 24, height = 20, units = c("cm"), dpi = 900)
```

```{r}
write.csv(all_data, paste(loc, "training-data/final.csv", sep  = ""), row.names = F)
```

##### Rank predictors using FFS (ALL)

#### 6. (cont.) Read in final data and predictor metadata

```{r}
all_data <- read.csv(paste(loc, "training-data/final.csv", sep = ""))
pred.metadata <- read.csv(paste(loc, "/training-data/list_of_predictors_metadata.csv", sep = ""))
```

###### Select features

```{r}
feat <- pred.metadata$Predictor
feat_l <- length(feat)
data_l <- length(all_data$Cluster)
```

###### Create pairwise feat combinations for FFS

```{r}
feat_pairs_full <- combn(feat, 2, simplify = FALSE)
length(feat_pairs_full)
```

###### Remove pairs of static predictors (unlikely to be informative)

```{r}
static.predictors <- pred.metadata %>% 
  filter(Content == "Spatial") %>% 
  dplyr::select(Predictor, Content)

feat_pairs_n <- length(feat_pairs_full)

feat_pairs_remove <- bind_cols(feat_pairs_full) %>% 
  gather(key = "pair", value = "Predictor") %>% 
  mutate(pair = str_remove(pair, "...")) %>% 
  left_join(static.predictors) %>% 
  filter(!is.na(Content)) %>% 
  group_by(pair) %>% 
  summarize(n_static = n()) %>% 
  filter(n_static == 2) %>% 
  dplyr::select(pair) %>% pull() %>% as.numeric()

feat_pairs <- feat_pairs_full[-feat_pairs_remove]

length(feat_pairs) 
```

###### Save `feat_pairs` to view

```{r}
bind_cols(feat_pairs) %>% 
  gather(key = Pair, value = Predictor) %>% 
  mutate(Pair = str_remove(Pair,'...')) %>% 
  write.csv(paste(loc, "training-data/feat/feat_pairs.csv", sep = ""), row.names = FALSE)
```

#### 6. (cont.) FFS of first predictor pair

##### Get fold number and fold names

```{r}
set.seed(23)

# data inner folds 
folds <- all_data %>% mutate(Cluster = as.factor(Cluster)) %>% dplyr::select(Cluster) %>% pull() %>% levels() %>% length()
fold_names <- all_data %>% group_by(Cluster) %>% summarize(Name = ID[1]) 
```

##### Setup ML folds

```{r}
train_feat <- list()
train_label <- list()
validate_data <- list()

for (i in 1:folds) {
  train_label[[i]] <- all_data %>% filter(!Cluster == i) %>% dplyr::select(FCH4) %>% pull()
  train_feat[[i]] <- all_data %>% filter(!Cluster == i) %>% dplyr::select(feat)
  validate_data[[i]] <- all_data %>% filter(Cluster == i) %>% dplyr::select(Cluster, FCH4, feat)
}

folds_index <- list() # need list within list (each inner list has all fold site except LOSO fold)
for (i in 1:folds) {
  x <- list()
  folds_index[[i]] <- x
}

for (i in 1:folds) {
  for (j in 1:folds) {
    folds_index[[i]][[j]] <- all_data %>% 
      filter(!Cluster == i) %>% 
      mutate(index = 1:n()) %>% 
      filter(!Cluster == j) %>% 
      dplyr::select(index) %>% 
      pull()
  }
  folds_index[[i]] <- folds_index[[i]][-i]
}
length(folds_index) == length(train_label) & length(folds_index) == length(train_feat)  & length(folds_index) == length(validate_data)
```

##### Run FFS First Pair in Parallel (need to re-run at some point)

```{r}
set_index <- c(1:3)

# numCores <- detectCores()

best_pair <- c("TA", "canopyht")
best_pair <- list(best_pair)

feat_pairs[[1]]

rf_pred <- lapply(best_pair, fit_all_pairs)

for(i in 1:10){
  rf_pred[[i]] <- rf_pred[[i]] %>% 
    mutate(feat_pair = set_index[i])
}

for(i in 1:10){
  write.csv(rf_pred[[i]], paste(loc, "ffs/first-pair/rf-pred/rf_pred_", set_index[i], ".csv", sep = ""), row.names = F)
}
```

##### Compute FFS First Pair metrics  

May need to read in `rf_pred` csv files again.

```{r}
rf.metrics <- list()

files <- list.files(paste(loc, "ffs/first-pair/rf-pred/", sep = ""))
rf_pred <- lapply(read.csv, files)

compute_metrics <- function(rf_pred) {
    rf_pred %>% 
    dplyr::select(FCH4, FCH4P) %>% 
    summarize(samples = n(),
              PMARE = 100/n() * sum(abs(FCH4 - FCH4P) / abs(FCH4)),
              R2 = cor(FCH4P, FCH4)^2,
              NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
              MAE = sum(abs(FCH4 - FCH4P))/n(),
              nMAE = MAE/sd(FCH4),
              MeanO = mean(FCH4),
              MedO = median(FCH4),
              sdO = sd(FCH4),
              MeanP = mean(FCH4P),
              MedP = median(FCH4P),
              sdP = sd(FCH4P),
              nSD = sdP/sdO,
              Bias = mean(FCH4P - FCH4),
              cBias = abs(Bias)/sum(abs(Bias))*100,
              predictors = list(feat_pairs[[i]]) )
}

rf.metrics <- lapply(rf_pred, compute_metrics)

rf.metrics.all <- bind_rows(rf.metrics) %>% 
  arrange(MAE) %>% 
  rowwise() %>% 
  mutate(pred1 = predictors[1],
         pred2 = predictors[2]) %>% 
  dplyr::select(-predictors)

write.csv(rf.metrics.all, paste(loc, "/ffs/first-pair/rf-metrics/rf.metrics.all.csv", sep = ""), row.names = F)

```

##### Look at metrics distribution

```{r}
rf.metrics.all %>% 
  ggplot(aes(R2)) +
  geom_histogram() + my_theme
ggsave(paste(loc, "ffs/first-pair/rf-metrics/R2_distribution.png", sep = ""),
       width = 8, height = 15, units = c("cm"), dpi = 300) 

rf.metrics.all %>% 
  ggplot(aes(MAE)) +
  geom_histogram() + my_theme
ggsave(paste(loc, "ffs/first-pair/rf-metrics/MAE_distribution.png", sep = ""),
       width = 8, height = 15, units = c("cm"), dpi = 300) 
```

##### get the min MAE and amx R2 (are they the same?)

```{r}
max(rf.metrics.all$R2)
min(rf.metrics.all$MAE)
rf.metrics.all
```

Yes both are for `canopyht` and `TA`.

##### look at top 10 Predictor Pairs

```{r}
rf.metrics.all %>% 
  arrange(MAE) %>% 
  head(10) %>% 
  dplyr::select(pred1, pred2)
```
##### Stepwise FFS (for first 13 steps, to avoid local minima)

```{r}
feat <- pred.metadata$Predictor
feat_original <- feat
data_l <- length(all_data$Cluster)


feat_best <- c("TA", "canopyht")
feat_best_index <- c(which(feat == "TA"), 
                     which(feat == "canopyht"))

feat <- feat[- c(feat_best_index) ]
feat_l <- length(feat)
```

##### Setup ML folds

```{r}
train_feat <- list()
train_label <- list()
validate_data <- list()

for (i in 1:folds) {
  train_label[[i]] <- all_data %>% filter(!Cluster == i) %>% dplyr::select(FCH4) %>% pull()
  train_feat[[i]] <- all_data %>% filter(!Cluster == i) %>% dplyr::select(feat_original)
  validate_data[[i]] <- all_data %>% filter(Cluster == i) %>% dplyr::select(Cluster, FCH4, feat_original)
}

length(train_label) & length(folds_index) == length(train_feat)  & length(folds_index) == length(validate_data)
```

##### Update compute metrics function (remove predictor list variable of length 2)

```{r}
compute_metrics <- function(rf_pred) {
    rf_pred %>% 
    dplyr::select(FCH4, FCH4P) %>% 
    summarize(samples = n(),
              PMARE = 100/n() * sum(abs(FCH4 - FCH4P) / abs(FCH4)),
              R2 = cor(FCH4P, FCH4)^2,
              NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
              MAE = sum(abs(FCH4 - FCH4P))/n(),
              nMAE = MAE/sd(FCH4),
              MeanO = mean(FCH4),
              MedO = median(FCH4),
              sdO = sd(FCH4),
              MeanP = mean(FCH4P),
              MedP = median(FCH4P),
              sdP = sd(FCH4P),
              nSD = sdP/sdO,
              Bias = mean(FCH4P - FCH4),
              cBias = abs(Bias)/sum(abs(Bias))*100) 
}
```

##### Train multiple RFs (with CV) for FFS to select the next 10 best predictors

```{r}
feat_initial <- 2
feat_best_row <- list()
rf.metrics.all <- list()

for(k in 2:10){
  
# create index for a list of features
feat_l <- length(feat)
set_index <- c(1:feat_l)
feat_list <- as.list(feat)

numCores <- detectCores()

# call fit a stepwise function to train RFs using one additional predictor
rf_pred <- mclapply(feat_list, fit_stepwise, mc.cores = 6)

for(l in 1:feat_l){
  rf_pred[[l]] <- rf_pred[[l]] %>% 
    mutate(feat_single = feat[l])
}

for(l in 1:feat_l){
  write.csv(rf_pred[[l]], paste(loc, "ffs/stepwise/rf-pred/rf_pred_", k, "thstep_", set_index[l], "_", feat[l],  ".csv", sep = ""), row.names = F)
}

# call compute_metrics function
rf.metrics <- lapply(rf_pred, compute_metrics)

# bind rows and append feature column, then arrange by MAE
rf.metrics.all[[k]] <- bind_rows(rf.metrics) %>% 
  cbind(feat) %>% 
  arrange(MAE) 

# save the row of the best performing new predictor
feat_best_row[[k]] <- rf.metrics.all[[k]] %>% 
    filter(MAE == min(MAE))
  
# get the specific predictor name
feat_best_add <- feat_best_row[[k]] %>% 
    dplyr::select(feat) %>% 
    pull() %>% as.character()
  
# add to initial best_feat
feat_best <- c(feat_best, feat_best_add)
  
# add one to feat_initial size
feat_initial <- feat_initial + 1
  
# remove feat_best from feat
feat_best_add_index <- which(feat == feat_best_add)
feat <- feat[- feat_best_add_index ]
  
print(paste('added', feat_best_add))

}

```

##### Save all output from stepwise FFS

```{r}
feat_best_row_all <- bind_rows(feat_best_row)

for(i in 1:length(rf.metrics.all)) {
  rf.metrics.all[[i]] <- rf.metrics.all[[i]] %>% 
    mutate(iteration = i)
}
rf_metrics_all <- bind_rows(rf.metrics.all)
  
write.csv(feat_best,
          paste(loc, "ffs/stepwise/feat_best.csv", sep = ""),
          row.names = FALSE)

write.csv(feat_best_row_all, 
          paste(loc, "ffs/stepwise/feat_best_row_all.csv", sep = ""),
          row.names = FALSE)

write.csv(rf_metrics_all,
          paste(loc, "ffs/stepwise/rf_metrics_all.csv", sep = ""),
          row.names = FALSE)

```

##### Reload initial pair 

```{r}
feat_best_initial <- read.csv(paste(loc, "ffs/first-pair/rf-metrics/rf.metrics.all.csv", sep = "")) %>% 
  mutate(feat = paste(pred1, pred2, sep = ", ")) %>% 
  dplyr::select(-pred1, -pred2)
feat_best_row_all <- read.csv(paste(loc,  "ffs/stepwise/feat_best_row_all.csv", sep = ""))

feat_best_initial <- feat_best_initial %>% 
  filter(MAE == min(MAE)) 

feat_best_row_final <- feat_best_initial %>% 
  bind_rows(feat_best_row_all) %>% 
  mutate(predictors = 1:n()+1)

write.csv(feat_best_row_final, 
          paste(loc, "ffs/stepwise/feat_best_row_final.csv", sep = ""),
          row.names = F)
```

##### Output plots for evolution of predictors

```{r}
feat_best <- feat_best_row_final %>% dplyr::select(feat) %>% pull()

feat_best_row_final %>% 
  ggplot(aes(predictors, R2)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = c(2:12), labels = feat_best) +
  my_theme +
  theme(axis.text.x = element_text(angle = -45, hjust = 0))
ggsave(paste(loc, "ffs/stepwise/R2_evolve.png", sep = ""),
       width = 15, height = 12, units = c("cm"), dpi = 300)

feat_best_row_final %>% 
  ggplot(aes(predictors, MAE)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = c(2:12), labels = feat_best) +
  my_theme +
  theme(axis.text.x = element_text(angle = -45, hjust = 0))
ggsave(paste(loc, "ffs/stepwise/MAE_evolve.png", sep = ""),
       width = 15, height = 12, units = c("cm"), dpi = 300)

```

#### 7. Cross Validation (Train, Predict)

##### Read in training data

```{r}
data <- read.csv(paste(loc, "training-data/final.csv", sep = ""))
```

##### Read in feat list from FFS results

```{r}
feat <- read.csv(paste(loc, "ffs/stepwise/feat_best_row_final.csv", sep = "")) %>%
  dplyr::select(feat) %>% pull() %>% as.character()
feat <- c("TA", "canopyht", feat[2:length(feat)])
feat <- feat[1:6] # select best
feat_l <- length(feat)
```

##### Finalize label and feat data

```{r}
data_l <- length(data$Cluster)

folds <- data %>% dplyr::select(Cluster) %>% max()
fold_names <- data %>% group_by(Cluster) %>% summarize(Name = ID[1]) 

# set up folds
train_feat <- list()
train_label <- list()
test_data <- list()

for (i in 1:folds) {
  train_label[[i]] <- data %>% filter(!Cluster == i) %>% dplyr::select(FCH4) %>% pull()
  train_feat[[i]] <- data %>% filter(!Cluster == i) %>% dplyr::select(feat)
  test_data[[i]] <- data %>% filter(Cluster == i) %>% dplyr::select(Cluster, FCH4, feat)
}

folds_index <- list() # need list within list (each inner list has all fold site except LOSO fold)
for (i in 1:folds) {
  x <- list()
  folds_index[[i]] <- x
}

for (i in 1:folds) {
  for (j in 1:folds) {
    folds_index[[i]][[j]] <- data %>% 
      filter(!Cluster == i) %>% 
      mutate(index = 1:n()) %>% 
      filter(!Cluster == j) %>% 
      dplyr::select(index) %>% 
      pull()
  }
  folds_index[[i]] <- folds_index[[i]][-i]
}
```

##### Train RF

```{r}
tgrid <- list()
myControl <- list()

## Create tune-grid
tgrid <- expand.grid(
  mtry = c(2, 4, 6),
  splitrule = "variance", 
  min.node.size = c(5, 50, 100)
)

rf_model <- list()
for (i in 1:folds) {
  
  ## Create trainControl object
  myControl <- trainControl(
    method = "cv",
    allowParallel = TRUE,
    verboseIter = TRUE, 
    returnData = FALSE,
    index = folds_index[[i]]
  )
  
  ## train rf on folds
  rf_model[[i]] <- train(
    x = train_feat[[i]], 
    y = train_label[[i]],
    num.trees = 100, # start at 10xfeat, drop to 100
    method = 'ranger',
    trControl = myControl,
    tuneGrid = tgrid,
    importance = 'permutation',
    metric = "MAE"
  )
  print(i)
}

# save loocv model ensemble
saveRDS(rf_model, paste(loc, "ensembles/cv/best6.rds", sep = "")) # local dir.
```

##### Look at r2 and MAE for all models 

```{r}
mean_MAE <- c()
mean_r2 <- c()
tune_r2 <- list()
tune_MAE <- list()
tune_r2se <- list()
tune_MAEse <- list()
for (i in 1:length(rf_model)){
  mean_MAE[i] <- sqrt(rf_model[[i]]$finalModel$prediction.error) # final model, out of bag metrics
  mean_r2[i] <- rf_model[[i]]$finalModel$r.squared
  tune_r2[[i]] <- rf_model[[i]]$results[,c(5)] # inner-fold cross validation metrics
  tune_MAE[[i]] <- rf_model[[i]]$results[,c(4)]
  tune_r2se[[i]] <- rf_model[[i]]$results[,c(6)]
  tune_MAEse[[i]] <- rf_model[[i]]$results[,c(4)]
  
}
mean(mean_MAE); mean(mean_r2) # final model for the fold, out of bag metrics


tune_r2s <- cbind(rf_model[[1]]$results[,c(1,3)],bind_cols(tune_r2)) %>%  
  gather(key = "fold", value = "R2", 3:22)
tune_MAEs <- cbind(rf_model[[1]]$results[,c(1,3)],bind_cols(tune_MAE)) %>% 
  gather(key = "fold", value = "MAE", 3:22)

mean(tune_MAEs$MAE); mean(tune_r2s$R2)  # inner MAE and R2

metrics <- c("final_fold_oob_MAE", "final_fold_oob_R2", "inner_cv_MAE", "inner_cv_R2")
values <- c(mean(mean_MAE), mean(mean_r2), mean(tune_MAEs$MAE), mean(tune_r2s$R2) )

as_tibble(cbind(metrics, values)) %>% 
  write.csv(paste(loc, "ensembles/cv/best6_cv_metrics.csv", sep = ""),
            row.names = F)
```

##### Get all hold out predictions

```{r}
rf.pred <- list()
for (i in 1:folds) {
  rf.pred[[i]] <- data %>% 
    filter(Cluster== i) %>%   
    mutate(FCH4P = predict(rf_model[[i]], .),
           index = 1:n())
}
rf.pred.all <- bind_rows(rf.pred)

write.csv(rf.pred.all, 
          paste(loc, "predictions/cv/best6_pred.csv", sep = ""),
          row.names = F)
```

##### Quick look at pred vs obs

```{r}
class_col_vector <- rep(c("#E78AC3","#F1E2CC","#33A02C","#377EB8","#E41A1C","#999999"),10)

rf.pred.all %>% 
  ggplot(aes(FCH4, FCH4P)) +
  geom_abline(slope = 1) +
  geom_point(alpha = 0.2) +
  xlab(label = expression("FCH4 (nmol m"^{-2}*" s"^{-1}*")")) +
  ylab(label = expression("FCH4P (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste(loc, "predictions/cv/best6_pvo.png", sep = ""),
       width = 15, height = 8, units = c("cm"), dpi = 300)
```

#### 7. (cont.) Cross Validation (Evaluate)
##### Read csv

```{r}
rf.pred.all <- read.csv(paste(loc, "predictions/cv/best6_pred.csv", sep = ""))
```

##### Get nice plot colors

```{r}
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
col_vector_values = sample(col_vector, length(levels(factor(rf.pred.all$Cluster))), replace=FALSE)
col_vector_values[1:6]

class_col_vector <- c("#E78AC3","#F1E2CC","#33A02C","#377EB8","#E41A1C","#999999")
```

##### Create a Climate-by-latitude class

```{r}
rf.pred.all <- rf.pred.all %>% 
  mutate(Climate = ifelse(Latitude > 70 | Latitude < -70, "1", NA),
         Climate = ifelse(Latitude < 70 & Latitude > 50 | Latitude > -70 & Latitude < -50, "2", Climate),
         Climate = ifelse(Latitude < 50 & Latitude > 23 | Latitude > -50 & Latitude < -23, "3", Climate),
         Climate = ifelse(Latitude < 23 & Latitude > 0 | Latitude > -23 & Latitude < 0, "4", Climate)) %>% 
  rename(Class = Site.Classification)
```

##### Look at FCH4 predictions histograms

```{r}
rf.pred.all %>% 
  dplyr::select(FCH4,FCH4P) %>% 
  gather(key = "Flux", value = "Value", 1:2) %>% 
  ggplot(aes(Value, fill = Flux)) +
  geom_histogram(aes(Value), fill = 'black') +
  geom_histogram(alpha = 0.3) +
  scale_color_manual(values = class_col_vector) +
  facet_wrap(~Flux) +
  my_theme
ggsave(paste(loc, "predictions/cv/simple_histograms.png", sep = ""),
       width = 20, height = 10, units = c("cm"), dpi = 300)
```

##### Trying using bias correction (notice histograms show a loss of very high and very low fluxes)

```{r}
splinemod <- smooth.spline(y=rf.pred.all$FCH4, x=rf.pred.all$FCH4P, spar = 2)
plot(rf.pred.all$FCH4P,rf.pred.all$FCH4, xlim = c(0,1000), ylim = c(0,1000))
abline(0,1,col = 'blue')
lines(splinemod, col="white", lwd=5)

```

There is no real effect. 

##### Visualize bias (in residuals)

```{r}
ungroup(rf.pred.all) %>% 
  mutate(index = 1:n()) %>% 
  ggplot(aes(x = NULL, y = FCH4P - FCH4, fill = Class)) +
  geom_abline(intercept = 0) +
  geom_boxplot(width = 2, alpha = 0.8) + my_theme + 
  theme(axis.line.x = element_blank(), axis.title.x = element_blank(), 
        axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  ylab(label = expression("Error (nmol m"^{-2}*" s"^{-1}*")")) +
  scale_fill_manual(values = class_col_vector) 
ggsave(paste(loc, "predictions/cv/residuals_error.png", sep = ""),
       width = 13, height = 15, units = c("cm"), dpi = 300)
```

###### Global Metrics

```{r}
global <- ungroup(rf.pred.all) %>% 
  # filter(!ID == "USOWC") %>%
  summarize(samples = n(),
            R2 = cor(FCH4P, FCH4)^2,
            NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
            MAE = sum(abs(FCH4 - FCH4P))/n(),
            nMAE = MAE/sd(FCH4),
            MeanO = mean(FCH4),
            MedO = median(FCH4),
            sdO = sd(FCH4),
            MeanP = mean(FCH4P),
            MedP = median(FCH4P),
            sdP = sd(FCH4P),
            nSD = sdP/sdO,
            Bias = mean(FCH4P - FCH4),
            cBias = abs(Bias)/sum(abs(Bias))*100) 

global_ID <- ungroup(rf.pred.all) %>% 
    # filter(!ID == "USOWC") %>%
  group_by(ID) %>% 
  summarize(samples = n(),
            R2 = cor(FCH4P, FCH4)^2,
            NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
            MAE = sum(abs(FCH4 - FCH4P))/n(),
            nMAE = MAE/sd(FCH4),
            MeanO = mean(FCH4),
            MedO = median(FCH4),
            sdO = sd(FCH4),
            MeanP = mean(FCH4P),
            MedP = median(FCH4P),
            sdP = sd(FCH4P),
            nSD = sdP/sdO,
            Bias = mean(FCH4P - FCH4),
            cBias = abs(Bias)/sum(abs(Bias))*100) 

global_ID_monthly <- ungroup(rf.pred.all) %>% 
      # filter(!ID == "USOWC") %>%
  group_by(ID, Month) %>% 
  summarize(samples = n(),
            R2 = cor(FCH4P, FCH4)^2,
            NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
            MAE = sum(abs(FCH4 - FCH4P))/n(),
            nMAE = MAE/sd(FCH4),
            MeanO = mean(FCH4),
            MedO = median(FCH4),
            sdO = sd(FCH4),
            MeanP = mean(FCH4P),
            MedP = median(FCH4P),
            sdP = sd(FCH4P),
            nSD = sdP/sdO,
            Bias = mean(FCH4P - FCH4),
            cBias = abs(Bias)/sum(abs(Bias))*100) 

global <- bind_rows(global, global_ID, global_ID_monthly)

write.csv(global, paste(loc, "predictions/cv/global.csv", sep = ""),
          row.names = F)


global <- ungroup(rf.pred.all) %>% 
  filter(!ID == "USOWC") %>%
  summarize(samples = n(),
            R2 = cor(FCH4P, FCH4)^2,
            NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
            MAE = sum(abs(FCH4 - FCH4P))/n(),
            nMAE = MAE/sd(FCH4),
            MeanO = mean(FCH4),
            MedO = median(FCH4),
            sdO = sd(FCH4),
            MeanP = mean(FCH4P),
            MedP = median(FCH4P),
            sdP = sd(FCH4P),
            nSD = sdP/sdO,
            Bias = mean(FCH4P - FCH4),
            cBias = abs(Bias)/sum(abs(Bias))*100) 

global_ID <- ungroup(rf.pred.all) %>% 
    filter(!ID == "USOWC") %>%
  group_by(ID) %>% 
  summarize(samples = n(),
            R2 = cor(FCH4P, FCH4)^2,
            NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
            MAE = sum(abs(FCH4 - FCH4P))/n(),
            nMAE = MAE/sd(FCH4),
            MeanO = mean(FCH4),
            MedO = median(FCH4),
            sdO = sd(FCH4),
            MeanP = mean(FCH4P),
            MedP = median(FCH4P),
            sdP = sd(FCH4P),
            nSD = sdP/sdO,
            Bias = mean(FCH4P - FCH4),
            cBias = abs(Bias)/sum(abs(Bias))*100) 

global_ID_monthly <- ungroup(rf.pred.all) %>% 
      filter(!ID == "USOWC") %>%
  group_by(ID, Month) %>% 
  summarize(samples = n(),
            R2 = cor(FCH4P, FCH4)^2,
            NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
            MAE = sum(abs(FCH4 - FCH4P))/n(),
            nMAE = MAE/sd(FCH4),
            MeanO = mean(FCH4),
            MedO = median(FCH4),
            sdO = sd(FCH4),
            MeanP = mean(FCH4P),
            MedP = median(FCH4P),
            sdP = sd(FCH4P),
            nSD = sdP/sdO,
            Bias = mean(FCH4P - FCH4),
            cBias = abs(Bias)/sum(abs(Bias))*100) 

global <- bind_rows(global, global_ID, global_ID_monthly)


write.csv(global,
          paste(loc, "predictions/cv/global_wo_usowc.csv", sep = ""), 
          row.names = F)
```

###### Site-mean metrics

```{r}
## performance metrics broken down by site-means, seasonal cycle and weekly/daily anomalies
# site-means
sitemean <- ungroup(rf.pred.all) %>% 
  group_by(ID) %>%
  # filter(!ID == "USOWC") %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            IGBP = IGBP[1],
            Class = NA) %>% 
  ungroup() %>% 
  summarize(Name = "Site means",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = NA) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# by-ID, grouped by Class 
sitemean_class <- ungroup(rf.pred.all) %>% 
    # filter(!ID == "USOWC") %>%
  group_by(ID) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1]) %>%  
  ungroup() %>% 
  group_by(Class) %>% 
  summarize(Name = "Site means Class",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO)) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# by-ID, grouped by Climate 
sitemean_climate <- ungroup(rf.pred.all) %>% 
    # filter(!ID == "USOWC") %>%
  group_by(ID) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Climate = Climate[1],
            Class = NA) %>%  
  ungroup() %>% 
  group_by(Climate) %>% 
  summarize(Name = "Site means Climate",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO), 
            Class = NA) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# metrics (by-site for component)
sitemean_id <- ungroup(rf.pred.all) %>% 
    # filter(!ID == "USOWC") %>%
  group_by(ID) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1]) %>%  
  ungroup() %>% 
  group_by(ID) %>% 
  summarize(Name = "Site means ID",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = Class[1]) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# join sitemean
sitemean_data <- bind_rows(sitemean, sitemean_class, sitemean_climate, sitemean_id)
write.csv(sitemean_data, paste(loc, "predictions/cv/sitemean.csv", sep = ""),
          row.names = F)

## METRICS WITHOUT USOWC
# site-means
sitemean <- ungroup(rf.pred.all) %>% 
  group_by(ID) %>%
  filter(!ID == "USOWC") %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            IGBP = IGBP[1],
            Class = NA) %>% 
  ungroup() %>% 
  summarize(Name = "Site means",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = NA) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# by-ID, grouped by Class 
sitemean_class <- ungroup(rf.pred.all) %>% 
    filter(!ID == "USOWC") %>%
  group_by(ID) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1]) %>%  
  ungroup() %>% 
  group_by(Class) %>% 
  summarize(Name = "Site means Class",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO)) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# by-ID, grouped by Climate 
sitemean_climate <- ungroup(rf.pred.all) %>% 
    filter(!ID == "USOWC") %>%
  group_by(ID) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Climate = Climate[1],
            Class = NA) %>%  
  ungroup() %>% 
  group_by(Climate) %>% 
  summarize(Name = "Site means Climate",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO), 
            Class = NA) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# metrics (by-site for component)
sitemean_id <- ungroup(rf.pred.all) %>% 
    filter(!ID == "USOWC") %>%
  group_by(ID) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1]) %>%  
  ungroup() %>% 
  group_by(ID) %>% 
  summarize(Name = "Site means ID",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = Class[1]) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# join sitemean
sitemean_data <- bind_rows(sitemean, sitemean_class, sitemean_climate, sitemean_id)
write.csv(sitemean_data, paste(loc, "predictions/cv/sitemean_wo_usowc.csv", sep = ""),
          row.names = F)

```

##### Other sitemean plots

```{r}

# plot histogram of site year performance: BIAS
sitemean_data %>% 
  filter(Name == "Site means ID") %>% 
  dplyr::select(ID, Class, MAE, nMAE, Bias, cBias) %>% 
  gather(key = "Metric", value = "Score", 3:6) %>% 
  filter(Metric %in% c("Bias", "cBias")) %>% 
  ggplot(aes(Score, fill = Class)) +
  geom_histogram() +
  facet_wrap(~Metric, scales = 'free', ncol = 1) +
  scale_fill_manual(values = rep(class_col_vector,8)) +
  my_theme
ggsave(paste(loc, "predictions/cv/sitemean_histogram_bias.png", sep = ""),
       width = 13, height = 24, units = c("cm"), dpi = 300)

# plot histogram of site year performance: MAE
sitemean_data %>% 
  filter(Name == "Site means ID") %>% 
  dplyr::select(ID, Class, MAE, nMAE, Bias, cBias) %>% 
  gather(key = "Metric", value = "Score", 3:6) %>% 
  filter(Metric %in% c("MAE", "nMAE")) %>% 
  ggplot(aes(Score, fill = Class)) +
  geom_histogram() +
  facet_wrap(~Metric, scales = 'free', ncol = 1) +
  scale_fill_manual(values = rep(class_col_vector,8)) +
  my_theme
ggsave(paste(loc, "predictions/cv/sitemean_histogram_mae.png", sep = ""),
       width = 13, height = 24, units = c("cm"), dpi = 300)

# global site-mean comparison
ungroup(rf.pred.all) %>% 
  # group_by(ID, Year) %>% 
  group_by(ID) %>% 
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1]) %>% 
  ggplot(aes(MeanO, MeanP, col = Class)) +
  # scale_x_log10(limits = c(6,700)) +
  # scale_y_log10(limits = c(6,700)) +
  geom_point(size = 4, alpha = 0.85) +
  geom_abline(slope = 1, intercept = 0, colour = 'grey') +
  scale_color_manual(values = class_col_vector) +
  my_theme +
  labs(x= expression('Obs. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'), y = expression('Pred. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'))
ggsave(paste(loc, "predictions/cv/sitemean_pvo.png", sep = ""),
       width = 20, height = 13, units = c("cm"), dpi = 300)

# global site-mean comparison log10
ungroup(rf.pred.all) %>% 
  # group_by(ID, Year) %>% 
  group_by(ID) %>% 
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1]) %>% 
  ggplot(aes(MeanO, MeanP, col = Class)) +
  scale_x_log10(limits = c(6,700)) +
  scale_y_log10(limits = c(6,700)) +
  geom_point(size = 4, alpha = 0.85) +
  geom_abline(slope = 1, intercept = 0, colour = 'grey') +
  scale_color_manual(values = class_col_vector) +
  my_theme +
  labs(x= expression('Obs. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'), y = expression('Pred. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'))
ggsave(paste(loc, "predictions/cv/sitemean_pvo_log10.png", sep = ""),
       width = 20, height = 20, units = c("cm"), dpi = 300)

# by Climate/class
ungroup(rf.pred.all) %>% 
  # group_by(ID, Year) %>% 
  group_by(ID, Year) %>% 
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1],
            Climate = Climate[1]) %>% 
  ggplot(aes(MeanO, MeanP, col = Class)) +
  # scale_x_log10(limits = c(3,600)) +
  # scale_y_log10(limits = c(3,600)) +
  geom_point(size = 3, alpha = 0.85) +
  geom_abline(slope = 1, intercept = 0, colour = 'grey') +
  scale_color_manual(values = class_col_vector) +
  facet_wrap(~Climate, scales = 'free', ncol = 1) +
  my_theme +
  labs(x= expression('Obs. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'), y = expression('Pred. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'))
ggsave(paste(loc, "predictions/cv/sitemean_pvo_climate.png", sep = ""),
       width = 13, height = 36, units = c("cm"), dpi = 300)
```

##### Mean Seasonal Cycles

```{r}
# by-ID
msc <- ungroup(rf.pred.all) %>% 
    # filter(!ID == "USOWC") %>% 
  group_by(ID, Month) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            IGBP = IGBP[1],
            Class = NA) %>%  
  ungroup() %>% 
  summarize(Name = "msc global",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = NA) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# by-ID, grouped by Class 
msc_class <- ungroup(rf.pred.all) %>% 
    # filter(!ID == "USOWC") %>% 
  group_by(ID, Month) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1]) %>%  
  ungroup() %>% 
  group_by(Class) %>% 
  summarize(Name = "msc class",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO)) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100) 

# by-ID, grouped by Climate 
msc_climate <- ungroup(rf.pred.all) %>% 
  # filter(!ID == "USOWC") %>% 
  group_by(ID, Month) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Climate = Climate[1], 
            Class = NA) %>%  
  ungroup() %>% 
  group_by(Climate) %>% 
  summarize(Name = "msc climate",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = NA) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100) 

# metrics (by-site for component)
msc_id <- ungroup(rf.pred.all) %>% 
    # filter(!ID == "USOWC") %>% 
  group_by(ID, Month) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Climate = Climate[1], 
            Class = Class[1]) %>%  
  ungroup() %>% 
  group_by(ID) %>% 
  summarize(Name = "msc ID",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = Class[1]) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100) 

# join msc
msc_data <- bind_rows(msc, msc_class, msc_climate, msc_id)
write.csv(msc_data, paste(loc, "predictions/cv/msc.csv", sep = ""),
          row.names = F)

### WITHOUT US-OWC
# by-ID
msc <- ungroup(rf.pred.all) %>% 
    filter(!ID == "USOWC") %>%
  group_by(ID, Month) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            IGBP = IGBP[1],
            Class = NA) %>%  
  ungroup() %>% 
  summarize(Name = "msc global",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = NA) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# by-ID, grouped by Class 
msc_class <- ungroup(rf.pred.all) %>% 
      filter(!ID == "USOWC") %>%
  group_by(ID, Month) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Class = Class[1]) %>%  
  ungroup() %>% 
  group_by(Class) %>% 
  summarize(Name = "msc class",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO)) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100) 

# by-ID, grouped by Climate 
msc_climate <- ungroup(rf.pred.all) %>% 
    filter(!ID == "USOWC") %>%
  group_by(ID, Month) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Climate = Climate[1], 
            Class = NA) %>%  
  ungroup() %>% 
  group_by(Climate) %>% 
  summarize(Name = "msc climate",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = NA) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100) 

# metrics (by-site for component)
msc_id <- ungroup(rf.pred.all) %>% 
      filter(!ID == "USOWC") %>%
  group_by(ID, Month) %>%
  summarize(MeanO = mean(FCH4, na.rm=TRUE),
            MeanP = mean(FCH4P, na.rm=TRUE),
            MedO = median(FCH4, na.rm=TRUE),
            MedP = median(FCH4P, na.rm=TRUE),
            Climate = Climate[1], 
            Class = Class[1]) %>%  
  ungroup() %>% 
  group_by(ID) %>% 
  summarize(Name = "msc ID",
            Samples = n(),
            R2 = cor(MeanP, MeanO)^2,
            NSE = 1 - sum((MeanO - MeanP)^2) / sum((MeanO - mean(MeanO))^2),
            MAE = sum(abs(MeanO - MeanP))/n(),
            nMAE = MAE/sd(MeanO),
            MeanFCH4 = mean(MeanO),
            MedFCH4 = median(MeanO),
            sdFCH4 = sd(MeanO),
            MeanFCH4P = mean(MeanP),
            MedFCH4P = median(MeanP),
            sdFCH4P = sd(MeanP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(MeanP - MeanO),
            Class = Class[1]) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100) 

# join msc
msc_data <- bind_rows(msc, msc_class, msc_climate, msc_id)
write.csv(msc_data, paste(loc, "predictions/cv/msc_wo_usowc.csv", sep = ""),
          row.names = F)

```

##### Other MSC plots

```{r}

# plot histogram of msc performance: R2
msc_data %>% 
  filter(Name == "msc ID",
         NSE > 0) %>% 
  dplyr::select(ID, Class, R2, NSE, MAE, nMAE, Bias, cBias) %>% 
  gather(key = "Metric", value = "Score", 3:8) %>% 
  filter(Metric %in% c("R2", "NSE")) %>% 
  ggplot(aes(Score, fill = Class)) +
  geom_histogram(bins = 15) +
  facet_wrap(~Metric, ncol = 1) +
  scale_fill_manual(values = rep(class_col_vector,8)) +
  my_theme
ggsave(paste(loc, "predictions/cv/msc_histogram_r2.png", sep = ""),
       width = 13, height = 24, units = c("cm"), dpi = 300)


# plot histogram of msc performance: BIAS
msc_data %>% 
  filter(Name == "msc ID") %>% 
  dplyr::select(ID, Class, R2, NSE, MAE, nMAE, Bias, cBias) %>% 
  gather(key = "Metric", value = "Score", 3:8) %>% 
  filter(Metric %in% c("Bias", "cBias")) %>% 
  ggplot(aes(Score, fill = Class)) +
  geom_histogram() +
  facet_wrap(~Metric, scales = 'free', ncol = 1) +
  scale_fill_manual(values = rep(class_col_vector,8)) +
  my_theme
ggsave(paste(loc, "predictions/cv/msc_histogram_bias.png", sep = ""),
       width = 13, height = 24, units = c("cm"), dpi = 300)

# plot histogram of msc performance: MAE
msc_data %>% 
  filter(Name == "msc ID") %>% 
  dplyr::select(ID, Class,R2, NSE, MAE, nMAE, Bias, cBias) %>% 
  gather(key = "Metric", value = "Score", 3:8) %>% 
  filter(Metric %in% c("MAE", "nMAE")) %>% 
  ggplot(aes(Score, fill = Class)) +
  geom_histogram() +
  facet_wrap(~Metric, scales = 'free', ncol = 1) +
  scale_fill_manual(values = rep(class_col_vector,8)) +
  my_theme
ggsave(paste(loc, "predictions/cv/msc_histogram_mae.png", sep = ""),
       width = 13, height = 24, units = c("cm"), dpi = 300)

# global msc comparison
ungroup(rf.pred.all) %>% 
  group_by(ID) %>%
  mutate(annual_meanO = mean(FCH4, na.rm=TRUE),
         annual_meanP = mean(FCH4P, na.rm=TRUE)) %>% 
  group_by(ID, Month) %>% 
  summarize(annual_meanO = annual_meanO[1],
            annual_meanP = annual_meanP[1],
            mscO = abs(mean(FCH4, na.rm=TRUE)-annual_meanO),
            mscP = abs(mean(FCH4P, na.rm=TRUE)-annual_meanP),
            IGBP = IGBP[1],
            Climate = Climate[1],
            Class = Class[1],
            Months = n()) %>% 
  ungroup() %>% 
  ggplot(aes(mscO, mscP, col = Class)) +
  # scale_x_log10(limits = c(3,600)) +
  # scale_y_log10(limits = c(3,600)) +
  geom_point(size = 3, alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, colour = 'grey') +
  scale_color_manual(values = class_col_vector) +
  my_theme +
  labs(x= expression('Obs. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'), y = expression('Pred. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'))
ggsave(paste(loc, "predictions/cv/msc_pvo.png", sep = ""),
       width = 20, height = 10, units = c("cm"), dpi = 300)

# MSC plots Climate
ungroup(rf.pred.all) %>% 
  group_by(ID) %>%
  mutate(annual_meanO = mean(FCH4, na.rm=TRUE),
         annual_meanP = mean(FCH4P, na.rm=TRUE)) %>% 
  group_by(ID, Month) %>% 
  summarize(annual_meanO = annual_meanO[1],
            annual_meanP = annual_meanP[1],
            mscO = abs(mean(FCH4, na.rm=TRUE)-annual_meanO),
            mscP = abs(mean(FCH4P, na.rm=TRUE)-annual_meanP),
            IGBP = IGBP[1],
            Climate = Climate[1],
            Class = Class[1],
            Months = n()) %>% 
  ungroup() %>% 
  ggplot(aes(mscO, mscP, col = Class)) +
  geom_abline(slope = 1, intercept = 0, colour = 'black') +
  geom_point(size = 2) +
  facet_wrap(~Climate, ncol = 2, scales = 'free') +
  scale_color_manual(values = class_col_vector) +
  my_theme +
  labs(x= expression("Observed "*CH[4]*' MSC (nmol m'^{-2}*' s'^{-1}*')'), y = expression("Predicted "*CH[4]*' MSC (nmol m'^{-2}*' s'^{-1}*')')) 
ggsave(paste(loc, "predictions/cv/msc_pvo_climate.png", sep = ""),
       width = 24, height = 20, units = c("cm"), dpi = 300)


## FINAL FIGURE 
# MSC plots ID
climate_names <- c(`1` = "Tundra", `2` = "Boreal", `3` = "Temperate", `4` = "Tropical")

ungroup(rf.pred.all) %>% 
  group_by(ID) %>%
  mutate(annual_meanO = mean(FCH4, na.rm=TRUE),
         annual_meanP = mean(FCH4P, na.rm=TRUE)) %>% 
  group_by(ID, Month) %>% 
  summarize(annual_meanO = annual_meanO[1],
            annual_meanP = annual_meanP[1],
            mscO = abs(mean(FCH4, na.rm=TRUE)-annual_meanO),
            mscP = abs(mean(FCH4P, na.rm=TRUE)-annual_meanP),
            IGBP = IGBP[1],
            Climate = Climate[1],
            Months = n()) %>% 
  ungroup() %>% 
  ggplot(aes(mscO, mscP, col = ID)) +
  geom_abline(slope = 1, intercept = 0, colour = 'black') +
  geom_point(size = 2) +
  geom_smooth(method = 'lm', se = FALSE) +
  facet_wrap(~Climate, ncol = 2, scales = 'free', labeller = labeller(Climate = climate_names)) +
  # scale_y_continuous(limits = c(1,200)) +
  # scale_x_continuous(limits = c(1,200)) +
  scale_color_manual(values = rep(col_vector_values,2)) +
  theme(legend.position = "right",
        legend.direction = 'vertical') +
  my_theme +
  labs(x= expression("Observed "*CH[4]*' MSC (nmol m'^{-2}*' s'^{-1}*')'), y = expression("Predicted "*CH[4]*' MSC (nmol m'^{-2}*' s'^{-1}*')')) 
ggsave(paste(loc, "predictions/cv/msc_pvo_id.png", sep = ""),
       width = 20, height = 26, units = c("cm"), dpi = 300)

# timeseries
ungroup(rf.pred.all) %>% 
  group_by(ID) %>%
  mutate(annual_meanO = mean(FCH4, na.rm=TRUE),
         annual_meanP = mean(FCH4P, na.rm=TRUE)) %>% 
  group_by(ID, Month) %>% 
  summarize(annual_meanO = annual_meanO[1],
            annual_meanP = annual_meanP[1],
            mscO = mean(FCH4, na.rm=TRUE)-annual_meanO,
            mscP = mean(FCH4P, na.rm=TRUE)-annual_meanP,
            IGBP = IGBP[1],
            Months = n()) %>% 
  ggplot(aes(Month, mscO)) +
  geom_point(size = 3) +
  geom_point(aes(Month, mscP), col = 'orange', size = 3) +
  facet_wrap(~ID,scales = 'free',ncol=8) +
  # scale_y_log10(limits = c(0.01,1000), breaks = c(0.001,0.1,10,1000), labels = c(0.001,0.1,10,1000)) +
  scale_x_continuous(breaks = c(1:12), limits = c(1,12), labels = c("J","F","M","A","M","J","J","A","S","O","N","D")) +
  my_theme +
  labs(x= "Month", y = expression(CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')')) 
ggsave(paste(loc, "predictions/cv/msc_timeseries.png", sep = ""),
       width = 45, height = 25, units = c("cm"), dpi = 300)

## FINAL FIGURE
# timeseries
ungroup(rf.pred.all) %>% 
  filter(ID %in% c("SEDeg", "DEHte","BWNxr", "FRLGt", "USORv", "NZKop")) %>% 
  group_by(ID) %>%
  mutate(annual_meanO = mean(FCH4, na.rm=TRUE),
         annual_meanP = mean(FCH4P, na.rm=TRUE)) %>% 
  group_by(ID, Month) %>% 
  summarize(annual_meanO = annual_meanO[1],
            annual_meanP = annual_meanP[1],
            mscO = mean(FCH4, na.rm=TRUE)-annual_meanO,
            mscP = mean(FCH4P, na.rm=TRUE)-annual_meanP,
            IGBP = IGBP[1],
            Climate = Climate[1],
            Months = n()) %>% 
  ggplot(aes(Month, mscO)) +
  geom_point(size = 2) +
  geom_point(aes(Month, mscP), col = 'orange', size = 2) +
  facet_wrap(~ID, scales = 'free', ncol=3, labeller = labeller(Climate = climate_names)) +
  # scale_y_log10(limits = c(0.01,1000), breaks = c(0.001,0.1,10,1000), labels = c(0.001,0.1,10,1000)) +
  scale_x_continuous(breaks = c(1:12), limits = c(1,12), labels = c("J","F","M","A","M","J","J","A","S","O","N","D")) +
  my_theme +
  labs(x= "Month", y = expression(CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')')) 
ggsave(paste(loc, "predictions/cv/msc_highlowights.png", sep = ""),
       width = 24, height = 14, units = c("cm"), dpi = 300)


```

##### Anomalies (i.e., excursions from mean seasonal cycle)

```{r}
# first create mean seasonal cycle dataframe
pred.msc <- ungroup(rf.pred.all) %>% 
  group_by(ID, Month) %>% 
  summarize(mscO = mean(FCH4, na.rm=TRUE),
            mscP = mean(FCH4P, na.rm=TRUE),
            Years = n()) 

# then left join back onto full dataframe
rf.pred.msc <- rf.pred.all %>% left_join(pred.msc, by = c("ID","Month")) 

# global monthly anomaly summary
anomaly <- ungroup(rf.pred.msc) %>% 
  group_by(ID, Year, Month) %>%
  summarize(mscO = mscO[1],
            Years = Years[1],
            FCH4 = mean(FCH4),
            FCH4P = mean(FCH4P)) %>% 
  mutate(anomalyO = FCH4 - mscO,
         anomalyP = FCH4P - mscO) %>% 
  ungroup() %>% 
  summarize(Name = "anomaly",
            Samples = n(),
            R2 = cor(anomalyP, anomalyO)^2,
            NSE = 1 - sum((anomalyO - anomalyP)^2) / sum((anomalyO - mean(anomalyO))^2),
            MAE = sum(abs(anomalyO - anomalyP))/n(),
            nMAE = MAE/sd(anomalyO),
            MeanFCH4 = mean(anomalyO),
            MedFCH4 = median(anomalyO),
            sdFCH4 = sd(anomalyO),
            MeanFCH4P = mean(anomalyP),
            MedFCH4P = median(anomalyP),
            sdFCH4P = sd(anomalyP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(anomalyP - anomalyO)) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# by-ID, grouped by CLASS 
anomaly_class <- ungroup(rf.pred.msc) %>% 
  group_by(ID, Year, Month) %>%
  summarize(mscO = mscO[1],
            Years = Years[1],
            FCH4 = mean(FCH4),
            FCH4P = mean(FCH4P),
            Class = Class[1]) %>% 
  mutate(anomalyO = FCH4 - mscO,
         anomalyP = FCH4P - mscO) %>% 
  ungroup() %>% 
  group_by(Class) %>% 
  summarize(Name = "Class",
            Samples = n(),
            R2 = cor(anomalyP, anomalyO)^2,
            NSE = 1 - sum((anomalyO - anomalyP)^2) / sum((anomalyO - mean(anomalyO))^2),
            MAE = sum(abs(anomalyO - anomalyP))/n(),
            nMAE = MAE/sd(anomalyO),
            MeanFCH4 = mean(anomalyO),
            MedFCH4 = median(anomalyO),
            sdFCH4 = sd(anomalyO),
            MeanFCH4P = mean(anomalyP),
            MedFCH4P = median(anomalyP),
            sdFCH4P = sd(anomalyP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(anomalyP - anomalyO)) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# by-ID, grouped by Climate 
anomaly_climate <- ungroup(rf.pred.msc) %>% 
  group_by(ID, Year, Month) %>%
  summarize(mscO = mscO[1],
            Years = Years[1],
            FCH4 = mean(FCH4),
            FCH4P = mean(FCH4P),
            Climate = Climate[1]) %>% 
  mutate(anomalyO = FCH4 - mscO,
         anomalyP = FCH4P - mscO) %>% 
  ungroup() %>% 
  group_by(Climate) %>% 
  summarize(Name = "Climate",
            Samples = n(),
            R2 = cor(anomalyP, anomalyO)^2,
            NSE = 1 - sum((anomalyO - anomalyP)^2) / sum((anomalyO - mean(anomalyO))^2),
            MAE = sum(abs(anomalyO - anomalyP))/n(),
            nMAE = MAE/sd(anomalyO),
            MeanFCH4 = mean(anomalyO),
            MedFCH4 = median(anomalyO),
            sdFCH4 = sd(anomalyO),
            MeanFCH4P = mean(anomalyP),
            MedFCH4P = median(anomalyP),
            sdFCH4P = sd(anomalyP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(anomalyP - anomalyO)) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# now group by ID
anomaly_id <- ungroup(rf.pred.msc) %>% 
  group_by(ID, Year, Month) %>%
  summarize(mscO = mscO[1],
            Years = Years[1],
            FCH4 = mean(FCH4),
            FCH4P = mean(FCH4P),
            Climate = Climate[1]) %>% 
  mutate(anomalyO = FCH4 - mscO,
         anomalyP = FCH4P - mscO) %>% 
  ungroup() %>% 
  group_by(ID) %>% 
  summarize(Name = "ID",
            Samples = n(),
            R2 = cor(anomalyP, anomalyO)^2,
            NSE = 1 - sum((anomalyO - anomalyP)^2) / sum((anomalyO - mean(anomalyO))^2),
            MAE = sum(abs(anomalyO - anomalyP))/n(),
            nMAE = MAE/sd(anomalyO),
            MeanFCH4 = mean(anomalyO),
            MedFCH4 = median(anomalyO),
            sdFCH4 = sd(anomalyO),
            MeanFCH4P = mean(anomalyP),
            MedFCH4P = median(anomalyP),
            sdFCH4P = sd(anomalyP),
            nSD = sdFCH4P/sdFCH4,
            Bias = mean(anomalyP - anomalyO)) %>% 
  ungroup() %>% 
  mutate(cBias = abs(Bias)/sum(abs(Bias))*100)

# join anomalies
anomaly_data <- bind_rows(anomaly, anomaly_class, anomaly_climate, anomaly_id)
write.csv(anomaly_data, paste(loc, "predictions/cv/anomaly.csv", sep=""),
          row.names=FALSE)

# global anomalies 
rf.pred.msc %>% 
  group_by(ID, Year, Month) %>% 
  summarize(mscO = mscO[1],
            mscP = mscP[1],
            Years = Years[1],
            FCH4 = FCH4[1],
            FCH4P = FCH4P[1],
            Class = Class[1],
            Climate = Climate[1]) %>% 
  mutate(anomalyO = abs(FCH4 - mscO),
         anomalyP = abs(FCH4P - mscP)) %>%  
  ungroup() %>% 
  ggplot(aes(anomalyO, anomalyP, col = Class)) +
  # scale_x_log10(limits = c(0.01,1000)) +
  # scale_y_log10(limits = c(0.01,1000)) +
  geom_point(size = 2, alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, colour = 'grey') +
  scale_color_manual(values = class_col_vector) +
  # facet_wrap(~Climate, scales = 'free', ncol = 1) +
  my_theme +
  labs(x= expression('Obs. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'), y = expression('Pred. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'))
ggsave(paste(loc, "predictions/cv/anomalies_pvo.png", sep = ""),
       width = 20, height = 10, units = c("cm"), dpi = 300)

# now calculate monthly anomalies  
rf.pred.msc %>% 
  group_by(ID, Year, Month) %>% 
  summarize(mscO = mscO[1],
            mscP = mscP[1],
            Years = Years[1],
            FCH4 = FCH4[1],
            FCH4P = FCH4P[1],
            Class = Class[1],
            Climate = Climate[1]) %>% 
  mutate(anomalyO = abs(FCH4 - mscO),
         anomalyP = abs(FCH4P - mscP)) %>%  
  ungroup() %>% 
  ggplot(aes(anomalyO, anomalyP, col = Class)) +
  # scale_x_log10(limits = c(0.01,1000)) +
  # scale_y_log10(limits = c(0.01,1000)) +
  geom_point(size = 2, alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, colour = 'grey') +
  scale_color_manual(values = class_col_vector) +
  facet_wrap(~Climate, scales = 'free', ncol = 1) +
  my_theme +
  labs(x= expression('Obs. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'), y = expression('Pred. '*CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')'))
ggsave(paste(loc, "predictions/cv/anomalies_pvo_climate.png", sep = ""),
       width = 13, height = 36, units = c("cm"), dpi = 300)

# timeseries
rf.pred.msc %>% 
  group_by(ID, Year, Month) %>% 
  summarize(mscO = mscO[1],
            mscP = mscP[1],
            Years = Years[1],
            FCH4 = FCH4[1],
            FCH4P = FCH4P[1],
            Class = Class[1],
            Climate = Climate[1]) %>% 
  mutate(anomalyO = abs(FCH4 - mscO),
         anomalyP = abs(FCH4P - mscP)) %>%  
  ungroup() %>% 
  ggplot(aes(Month, anomalyO)) +
    geom_point(col = 'black', size = 3) +
    geom_point(aes(Month, anomalyP), col = 'orange', size = 3, alpha = 0.5) +
    facet_wrap(~ID, scales = 'free', ncol = 8) +
    # scale_y_log10(limits = c(0.01,1000), breaks = c(0.001,0.1,10,1000), labels = c(0.001,0.1,10,1000)) +
    scale_x_continuous(breaks = c(1:12), limits = c(1,12), labels = c("J","F","M","A","M","J","J","A","S","O","N","D")) +
    my_theme +
    labs(x= "Month", y = expression(CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')')) 
ggsave(paste(loc, "predictions/cv/anomalies_timeseries.png", sep = ""),
       width = 45, height = 25, units = c("cm"), dpi = 300)

```

#### 8. Variable Importance

##### VarImp

```{r}
# read rf model
rf_model <- readRDS(paste(loc, "ensembles/cv/best6.rds", sep = ""))

# get folds
folds <- length(rf_model)

## look at variable importance, create table of all site rankings
variable.imp <- list()
var.imp.ranks <- list()
variable.imp.single <- list()
for (i in 1:folds) {
  variable.imp[[i]] <- varImp(rf_model[[i]], scale = FALSE)
}
var.imp.names <- rownames(variable.imp[[1]]$importance)

for (i in 1:folds) {
  var.imp.ranks[[i]] <- variable.imp[[i]]$importance$Overall
  variable.imp.single[[i]] <- cbind(var.imp.names, var.imp.ranks[[i]])
  variable.imp.single[[i]] <- variable.imp.single[[i]] %>% as_tibble() %>% mutate(V2 = as.integer(V2))
  variable.imp.single[[i]] <- variable.imp.single[[i]] %>% arrange(desc(V2)) %>% dplyr::select(var.imp.names,V2)
}

for (i in 1:folds) {
  names(variable.imp.single[[i]]) <- c("Var",paste("Imp",i,sep=""))
  variable.imp.single[[i]] <- variable.imp.single[[i]] %>% arrange(Var)
}

# get full table of rf pred impotance 
variable.importance <- as_tibble(bind_cols(variable.imp.single)) %>% 
  dplyr::select(1,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,
                32,34,36,38,40,42,44,46,48,50,52) %>% group_by(Var...1) %>% 
  mutate(AvgImp = mean(c(Imp1,Imp2,Imp3,Imp4,Imp5,Imp6,Imp7,Imp8,Imp9,Imp10,
                         Imp11,Imp12,Imp13,Imp14,Imp15,Imp16,Imp17,Imp18,
                         Imp19,Imp20,Imp21,Imp22,Imp23,Imp24,Imp25,Imp26))) %>% 
  arrange(desc(AvgImp)) %>% 
  ungroup() %>% 
  mutate(ScaledAvgImp = AvgImp/max(AvgImp))

view(variable.importance)

write.csv(variable.importance,
          paste(loc, "varimp/best6.csv", sep = ""),
          row.names = FALSE)

# read in data again
variable.importance <- read.csv(paste(loc, "varimp/best6.csv", sep = ""))

# get effective number of predictors
sum(variable.importance$ScaledAvgImp)

# read in predictor summary
pred.summary <- read.csv(paste(loc, "training-data/list_of_predictors_metadata.csv", sep = "")) %>% 
  dplyr::select(Var = Predictor, Information = Content, Class)

# var imp plot
variable.importance %>% 
  rename(Var = Var...1) %>% 
  left_join(pred.summary, by = "Var") %>% 
  mutate(Information = as.character(Information),
         Class = as.character(Class),
         Information = ifelse(is.na(Information), "Spatiotemporal", Information),
         Class = ifelse(is.na(Class), "Biomet", Class),
         Class = ifelse(Var == "LSWI_LAG2", "Land Cover", Class),
         Var = fct_reorder(Var, ScaledAvgImp)) %>% 
  ggplot(aes(x = Var, y = ScaledAvgImp)) +
    geom_segment(aes(x = Var, xend = Var, y = 0, yend = ScaledAvgImp), color = 'grey', stat = "identity") +
    geom_point(aes(color = Class, shape = Information), size = 4) +
    scale_color_manual(values = c("Turquoise", "Dark Blue", "Dark Green", "Brown","Grey")) +
    # scale_fill_manual(values = c("Light Green", "Dark Grey", "Dark Blue", "Brown", "Dark Green")) +
    scale_shape_manual(values = c(15,16,17)) +
    coord_flip() +
    xlab("Predictor") + ylab("Importance") +
    my_theme +
    theme(legend.position='bottom', legend.box = 'vertical')
ggsave(paste(loc, "varimp/pred_rank.png", sep = ""),
       width = 18, height = 13, units = c("cm"), dpi = 300)
```

##### Variable Responses

```{r}
# read rf model
rf_model <- readRDS(paste(loc, "ensembles/cv/best6.rds", sep = ""))

# get feature names
variable.importance <- varImp(rf_model[[1]], scale = TRUE)
feat <- rownames(variable.importance$importance) %>% rev()

# Read in full csv
train <- read.csv(paste(loc, "predictions/cv/best6_pred.csv", sep = "")) # local path

# look at all relationships
train %>% 
  # filter(!ID == "USOWC") %>% 
  dplyr::select(FCH4P = FCH4P, FCH4, feat) %>% 
  gather(key = "Predictor", value = "Value", 3:8) %>% 
  ggplot(aes(Value, FCH4)) + 
  geom_point(alpha = 0.05, size = 1) +
  geom_point(aes(Value, FCH4P), alpha = 0.05, size = 1, color = "orange") +
  facet_wrap(~Predictor, scales = 'free') +
  ylab(label =  expression("FCH4 (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste(loc, "varimp/var_response_all.png", sep = ""),
       width = 25, height = 22, units = c("cm"), dpi = 300)
```


##### Partial Dependency Plots

```{r}
# read mc rf model
rf_model <- readRDS(paste0(loc, "ensembles/cv/best6.rds"))

# Read mc training data csv
train <- read.csv(paste0(loc, "training-data/final.csv")) # local path

# read folds index
folds_index <- readRDS(paste0(loc, "training-data/folds_index.rds")) # local path

# pdp
ice.TA <- rf_model[[1]] %>% 
  partial(pred.var = c("canopyht"), train = train, plot.engine = "ggplot2", ice = FALSE, center = TRUE)
plotPartial(ice.TA, colorkey= TRUE) 

```


#### 9. Upscaled Model with Monte Carlo (MC)
##### Forcing Data

Six final predictors:

    + canopyht (dispersion = sd of 25 km buffer for MC)
    + wc_pwtm (dispersion = sd of 25 km buffer for MC)
    + wc_mtwq (dispersion = sd of 25 km buffer for MC)

    + TA (Tower measured, then mapped to MERRA2 2001-2018, extract with buffer)
    + TA_LAG2 (Tower measured, then MERRA2 2001-2018, imposed lag, extract with buffer)
    + EVI_LAG3 (MODIS, imposed lag, fixed uncertainty = 0.015)
    


Details on Final MC [Here](https://docs.google.com/spreadsheets/d/10-veJchCsyrG2azK3UkuAGGwju4vjgIqcxeps1m9EDU/edit#gid=1658702303)

##### 1) Define the uncertainty around static grid data by extracting with buffer

##### Get site metadata (coordinates)

```{r}
sites <- read.csv(paste(loc, "fluxnet-ch4-data/metadata/fluxnet-ch4-site-metadata.csv", sep = "")) %>% 
  mutate(ID = paste(substr(ID, 1, 2), substr(ID, 4, 6), sep = ""))
site.coords <- cbind(sites$Longitude, sites$Latitude) # (x, y)
site.num <- length(site.coords[,1])
```

##### Global Canopy Height

```{r}
raster.names <- list.files(paste(loc, "grids/global-canopy-height/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/global-canopy-height/", raster.names, sep = ""))
canopyht <- as_tibble(raster::extract(stack, site.coords, buffer = 25000, fun = sd, na.rm = T)) 
canopyht <- cbind(sites, canopyht) %>% 
  rename(canopyht_sd = value)
head(canopyht)
```

###### Check for missing data

```{r}
canopyht %>% 
  mutate(missing = is.na(canopyht_sd)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(canopyht, paste(loc, "mc/extracted/", "canopyht_sd.csv", sep = ""), row.names =  F)
```

##### WorldClim 2.0

```{r warning = F}
setwd(loc)
raster.names <- list.files(paste(loc, "grids/worldclim/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/worldclim/", sep = "", raster.names[c(9,13)]))
plot(stack[[1]])
points(site.coords)
```

###### Extract worldclim data

```{r}
worldclim <- as_tibble(raster::extract(stack, site.coords, buffer = 25000, fun = sd, na.rm = T)) 
worldclim <- cbind(sites, worldclim) 
head(worldclim)
```

###### Check for missing data

```{r}
worldclim %>% 
  filter(is.na(wc2.0_bio_30s_09))

worldclim %>% 
  filter(is.na(wc2.0_bio_30s_13))
```
HKMPM data are missing. 

###### Look at HKMPM location:

```{r}
coords_subset <- site.coords[sites$ID %in% c("HKMPM"),] 
coords_subset_names <- sites$ID[sites$Latitude %in% coords_subset]

bounding_boxes <- bounding_box(coords_subset)

bounding_boxes_maps <- crop(stack[[1]], bounding_boxes)

plot(bounding_boxes_maps)
points(114.02924,  22.49817)
```
Can move onto nearby coastline by shifting west

###### Shift HKMPM west:

```{r}
site.coords.temp <- sites %>% 
  filter(ID %in% c("HKMPM")) %>% 
  mutate(Longitude = Longitude + 0.05) %>% 
  dplyr::select(Longitude, Latitude) %>% 
  as.matrix() %>% unname()

sites.temp <- sites %>% 
  filter(ID %in% c("HKMPM")) 
         
worldclim.temp <- as_tibble(raster::extract(stack, site.coords.temp, buffer = 25000, fun = sd, na.rm = T))
worldclim.temp <- cbind(sites.temp, worldclim.temp)
head(worldclim.temp)
```
###### Rejoin

```{r}
worldclim <- worldclim %>% 
  filter(!ID == "HKMPM") %>% 
  bind_rows(worldclim.temp) %>% 
  arrange(ID)
```

###### Create new name vector to match 2 products

```{r}
newnames <- c("wc_mtdq_sd", 
       "wc_pwtm_sd")

oldnames <- names(worldclim)[15:16]
worldclim <- worldclim %>% 
 rename_at(vars(oldnames), ~ newnames)
```

###### Write worlclim data

```{r}
write.csv(worldclim, paste(loc, "mc/extracted/worldclim_sd.csv", sep = ""), row.names = F)
```

##### 2) Perform MC Simulations

##### Read in loocv training data

```{r}
data <- read.csv(paste(loc, "training-data/final.csv", sep = "")) %>% 
  dplyr::select(Cluster, ID, Year, Month, Week, FCH4, FCH4_UNC, TA, TA_LAG2, canopyht, wc_mtdq,  wc_pwtm, EVI_LAG3)
head(data)
```

##### Read in standard deviations (static uncertainties)

```{r}
canopyht <- read.csv(paste(loc, "mc/extracted/canopyht_sd.csv", sep = ""))
worldclim <- read.csv(paste(loc, "mc/extracted/worldclim_sd.csv", sep = ""))
static <- left_join(canopyht, worldclim) %>% 
  filter(Upscaling == 1) %>% 
  dplyr::select(ID, canopyht_sd, wc_mtdq_sd, wc_pwtm_sd)
names(static)
names(data)
```

##### Merge with training data

```{r}
data <- data %>% 
  left_join(static, by = c("ID"))
```

##### Generate 1000 simulated observations (of the weekly data set)

```{r warning = F, echo = F, message = F}
set.seed(23)

# static data
data_temp <- data %>% 
  dplyr::select(ID, wc_pwtm, wc_mtdq, canopyht, wc_pwtm_sd, wc_mtdq_sd, canopyht_sd) %>% 
  distinct()

data_temp
data_temp <- data_temp[-c(2,8,22,31,34,37,39,41,49),] # remove duplicates (unexpected issue)
  
data1 <- data_temp %>% 
  group_by(ID) %>% 
  mutate(wc_mtdq_mc = list(rnorm(1000, mean = wc_mtdq, sd = wc_mtdq_sd)),
         wc_pwtm_mc = list(rnorm(1000, mean = wc_pwtm, sd = wc_pwtm_sd)),
         canopyht_mc = list(rnorm(1000, mean = canopyht, sd = canopyht_sd))) %>% 
  unnest(cols = c(wc_mtdq_mc, wc_pwtm_mc, canopyht_mc)) %>% 
  group_by(ID) %>% 
  mutate(mc_i = 1:n()) %>% 
  dplyr::select(mc_i, ID, wc_mtdq_mc, wc_pwtm_mc, canopyht_mc)

# check the spread for different sites
data1 %>% 
  filter(ID == "CASCB") %>% 
  ggplot(aes(canopyht_mc)) +
  geom_histogram()

# temporal data
data2 <- data %>% 
  dplyr::select(Cluster, ID, Year, Month, Week, FCH4, FCH4_UNC, TA, TA_LAG2, EVI_LAG3) %>% 
  rowwise() %>% 
  mutate(FCH4_mc = list(rlaplace(1000, location = FCH4, scale = FCH4_UNC/2)),
         TA_mc = list(rnorm(1000, mean = TA, sd = 0.5)),
         TA_LAG2_mc = list(rnorm(1000, mean = TA_LAG2, sd = 0.5)),
         EVI_LAG3_mc = list(rnorm(1000, mean = EVI_LAG3, sd = 0.015))) %>% 
  unnest(cols = c(FCH4_mc, TA_mc, TA_LAG2_mc, EVI_LAG3_mc)) %>% 
  group_by(ID, Year, Month, Week) %>% 
  mutate(mc_i = 1:n()) %>% 
  dplyr::select(mc_i, Cluster, ID, Year, Month, Week, FCH4, FCH4_UNC, FCH4_mc, TA_mc, TA_LAG2_mc, EVI_LAG3_mc)

# check the spread for different sites
data2 %>% 
  filter(ID == "USMyb" & Year == "2014" & Week == "13") %>% 
  ggplot(aes(FCH4_mc)) +
  geom_histogram()

# rejoin temporal and static
data2 <- data2 %>% left_join(data1, by = c("mc_i", "ID")) %>% arrange(mc_i)
```

###### For each observation (week/row) sample 500/1000, with replacement, and split into 500 training datasets

```{r}
data3 <- data2 %>% 
  group_by(ID, Year, Month, Week) %>% 
  sample_n(500, replace = T) %>% 
  arrange(mc_i) %>% 
  group_by(mc_i) %>% 
  group_split() 


data4 <- data3 %>% bind_rows() %>% dplyr::select(mc_i, everything()) 
```

##### Now drop IDs at random from each of the 500 datasets (and always drop USOWC)

```{r}
cluster.drop <- sample(1:26, 500, replace = T)

data5 <- list()
for(i in 1:500){
  data5[[i]] <- data4 %>% 
    filter(mc_i == i) %>% 
    filter(Cluster != cluster.drop[i] & ID != "USOWC")
}

# bind rows
data6 <- data5 %>% bind_rows()

saveRDS(data6, 
        paste(loc, "mc/data/1000mc_500bs.rds", sep = ""))
```

##### Identify which site was left out then subset those data

```{r warning = F, message = F, echo = F}
ID.names <- data4 %>% dplyr::select(ID) %>% mutate(ID = factor(ID)) %>% distinct()

# get IDs for each mc_i
IDs <- list()
for(i in 1:500){
  IDs[[i]] <- data6 %>% 
    filter(mc_i == i) %>% 
    dplyr::select(ID) %>% 
    distinct()
}

# get the held-out site for each mc_i
ID.drop.list <- list()
for(i in 1:500){
  ID.drop.list[[i]] <- ID.names %>% 
    anti_join(IDs[[i]]) %>% 
    mutate(Cluster = cluster.drop[i],
           mc_i = i) %>% 
    dplyr::select(mc_i, Cluster, ID)
}

# compile
ID.drop <-  bind_rows(ID.drop.list) 

write.csv(ID.drop, 
          paste0(loc, "/mc/bootstrap-site-dropped.csv"),
          row.names = F)
```

##### Finally, subset the held-out site data from each mc_i

```{r}
data7 <- list()
for(i in 1:500){
  data7[[i]] <- data4 %>% 
    filter(mc_i == i & ID %in% ID.drop.list[[i]]$ID )
  
}

data7 <- bind_rows(data7)

saveRDS(data7, 
        paste(loc, "mc/data/1000mc_500bs_heldout.rds", sep = ""))
```

#### 9. (cont.) MC Model Training

##### Read in training data

```{r}
data <- readRDS(paste0(loc, "/mc/data/1000mc_500bs.rds")) # local path
str(data)
```

##### Get features

```{r}
names(data)
mc.feat <- data %>% 
  dplyr::select(10:15) %>% 
  names()
```

##### Set up for model training

```{r}
feat <- mc.feat
feat_l <- length(feat)
data_l <- length(data$mc_i)

# data folds 
mc_iterations <- data %>% dplyr::select(mc_i) %>% max()
folds <- data %>% dplyr::select(Cluster) %>% max()
fold_names <- data %>% group_by(Cluster) %>% summarize(Name = ID[1])
iteration_l <- length(data$mc_i)/mc_iterations
```

##### Setup folds

```{r}
# set up mc iterations
train_feat <- list()
train_label <- list()

folds_retained <- list()
for(i in 1:500){
  folds_retained[[i]] <- c(1:26) %>% as_tibble() %>% anti_join(as_tibble(cluster.drop[i]))
}

for (i in 1:500) {
  train_label[[i]] <- data %>% filter(mc_i == i) %>% dplyr::select(FCH4_mc) %>% pull()
  train_feat[[i]] <- data %>% filter(mc_i == i) %>% dplyr::select(feat)
}

# for inner-cv for hyperparam, nested within mc iterations
folds_index <- list() 
for(i in 1:500){
  x <- list()
  folds_index[[i]] <- x
}

for (i in 1:500) {
  for (j in 1:25){
    folds_index[[i]][[j]] <- data %>% 
      filter(mc_i == i) %>% 
      mutate(index = 1:n()) %>% 
      filter(!Cluster == folds_retained[[i]]$value[j]) %>% 
      dplyr::select(index) %>% 
      pull()
  }
}

saveRDS(folds_index, paste0(loc, 'mc/data/folds_index.rds'))
```

##### Train RF

```{r echo = F, warning = F, message = F}
folds_index <- readRDS(paste0(loc, "mc/data/folds_index.rds"))

set.seed(23)

## set up lists 
tgrid <- list()
myControl <- list()

## Create tune-grid
tgrid <- expand.grid(
  .mtry = c(2,4,6),
  .splitrule = "variance", 
  .min.node.size = c(iteration_l*0.001,iteration_l*0.01,iteration_l*0.05)
)

rf_model <- list()
for (i in 1:500) {
    
    ## Create trainControl object
    myControl <- trainControl(
      method = "cv",
      allowParallel = TRUE,
      verboseIter = F, 
      returnData = FALSE,
      index = folds_index[[i]]
    )

    ## train rf on folds
    rf_model[[i]] <- train(
      x = train_feat[[i]], 
      y = train_label[[i]],
      method = 'ranger',
      trControl = myControl,
      tuneGrid = tgrid,
      num.trees = 60,
      importance = "permutation",
      metric = 'MAE'
    )
    print(i)

}

saveRDS(rf_model, 
        paste0(loc, "ensembles/mc/rf.rds"),
        ) # local dir.

```


##### Look at r2 and MAE for all models 

```{r}
mean_MAE <- c()
mean_r2 <- c()
tune_r2 <- list()
tune_MAE <- list()
tune_r2se <- list()
tune_MAEse <- list()
for (i in 1:length(rf_model)){
  mean_MAE[i] <- sqrt(rf_model[[i]]$finalModel$prediction.error) # final model, out of bag metrics
  mean_r2[i] <- rf_model[[i]]$finalModel$r.squared
  tune_r2[[i]] <- rf_model[[i]]$results[,c(5)] # inner-fold cross validation metrics
  tune_MAE[[i]] <- rf_model[[i]]$results[,c(4)]
  tune_r2se[[i]] <- rf_model[[i]]$results[,c(6)]
  tune_MAEse[[i]] <- rf_model[[i]]$results[,c(4)]
  
}
mean(mean_MAE); mean(mean_r2) # final model for the fold, out of bag metrics


tune_r2s <- cbind(rf_model[[1]]$results[,c(1,3)],bind_cols(tune_r2)) %>%  
  gather(key = "fold", value = "R2", 3:502)
tune_MAEs <- cbind(rf_model[[1]]$results[,c(1,3)],bind_cols(tune_MAE)) %>% 
  gather(key = "fold", value = "MAE", 3:502)

mean(tune_MAEs$MAE); mean(tune_r2s$R2)  # inner MAE and R2

metrics <- c("final_fold_oob_MAE", "final_fold_oob_R2", "inner_cv_MAE", "inner_cv_R2")
values <- c(mean(mean_MAE), mean(mean_r2), mean(tune_MAEs$MAE), mean(tune_r2s$R2) )

as_tibble(cbind(metrics, values)) %>% 
  write.csv(paste(loc, "ensembles/mc/mc_metrics.csv", sep = ""),
            row.names = F)
```

##### Get all hold out predictions

```{r}
# read in held out data
data_test <- readRDS(paste0(loc, "mc/data/1000mc_500bs_heldout.rds"))

rf.pred <- list()
for (i in 1:length(rf_model)) {
  rf.pred[[i]] <- data_test %>% 
    filter(!ID == "USOWC",
           mc_i == i) %>%   
    mutate(FCH4P = predict(rf_model[[i]], .),
           index = 1:n(),
           cluster_drop = cluster.drop[i])
}
rf.pred.all <- bind_rows(rf.pred)

write.csv(rf.pred.all, paste0(loc, "mc/predictions/rf_mc.csv"))
```

##### Look at pred vs obs

```{r}
rf.pred.all %>% 
  arrange(cluster_drop) %>% 
  filter(mc_i %in% c(1:26)) %>%
  ggplot(aes(FCH4, FCH4P)) +
  # stat_binhex(aes(fill=log(..count..))) +
  geom_point() +
  geom_abline(slope = 1, color = 'dark red') +
  facet_wrap(~mc_i, scales = 'free') +
  # scale_x_continuous(limits = c(0,1000)) +
  # scale_y_continuous(limits = c(0,1000)) +
  xlab(label = expression("FCH4 (nmol m"^{-2}*" s"^{-1}*")")) +
  ylab(label = expression("FCH4P (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste0(loc, "mc/predictions/rf_mc_heldout.png", sep = ""),
       width = 24, height = 24, units = c("cm"), dpi = 300)
```

#### 9. (cont.) MC Validation and Error Scaling

##### Read in MC data

```{r}
data <- read.csv(paste0(loc, "mc/predictions/rf_mc.csv"))
```

##### Summarize fluxes 

```{r}
# get the mean and variance for each week
summary <- data %>% 
  group_by(ID, Year, Month, Week) %>% 
  summarize(meanFCH4 = mean(FCH4, na.rm = TRUE),
            meanFCH4_mc = mean(FCH4_mc, na.rm = TRUE),
            meanFCH4P = mean(FCH4P, na.rm = TRUE),
            sdFCH4P = sd(FCH4P, na.rm = TRUE))

# free axes
summary %>% 
  # filter(ID == "CASCB") %>% 
  mutate(week_dec = Week/52,
         year_dec = Year+week_dec) %>% 
  ggplot(aes(year_dec, meanFCH4P)) + 
  geom_errorbar(aes(year_dec, ymin = meanFCH4P - 2*sdFCH4P, ymax = meanFCH4P + 2*sdFCH4P), alpha = 0.9, color = 'grey') +
  geom_point(color = "blue", alpha = 0.7) + 
  geom_point(aes(year_dec, meanFCH4_mc), size = 0.5) +
  facet_wrap(~ID, scales = 'free') + 
  labs(x = "Decimal Year", y = expression("FCH4 Predicted (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste0(loc, "mc/predictions/timeseries.png", sep = ""),
       width = 50, height = 35, units = c("cm"), dpi = 300)

# free x axis
summary %>% 
  # filter(ID == "CASCB") %>% 
  mutate(week_dec = Week/52,
         year_dec = Year+week_dec) %>% 
  ggplot(aes(year_dec, meanFCH4P)) + 
  geom_errorbar(aes(year_dec, ymin = meanFCH4P - 2*sdFCH4P, ymax = meanFCH4P + 2*sdFCH4P), alpha = 0.9, color = 'grey') +
  geom_point(color = "blue", alpha = 0.7) + 
  geom_point(aes(year_dec, meanFCH4), size = 0.5) +
  facet_wrap(~ID, scales = 'free_x') + 
  labs(x = "Decimal Year", y = expression("FCH4 Predicted (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste0(loc, "mc/predictions/timeseries_fixed.png", sep = ""),
       width = 50, height = 35, units = c("cm"), dpi = 300)

# correlations
summary %>% 
  ggplot(aes(meanFCH4, meanFCH4P)) + 
  geom_point(color = "black", alpha = 0.9) + 
  facet_wrap(~ID) + 
  labs(x = expression("FCH4 Observed (nmol m"^{-2}*" s"^{-1}*")"), 
       y = expression("FCH4 Predicted (nmol m"^{-2}*" s"^{-1}*")")) +
  geom_abline(slope = 1, color = 'dark blue') +
  my_theme
ggsave(paste0(loc, "mc/predictions/correlations.png", sep = ""),
       width = 50, height = 35, units = c("cm"), dpi = 300)

```

##### Output held-out data summary

```{r}
hold_out <- readRDS(paste0(loc, "mc/data/1000mc_500bs_heldout.rds"))

hold_out %>% 
  group_by(mc_i) %>% 
  summarize(n()) %>% 
  write.csv(paste0(loc, "mc/data/heldout_sample_size.csv"),
            row.names = F)

```

##### Test `mc_i` with mc `rf_model` to check fit on training data 

```{r}
data_train <- readRDS(paste0(loc, "mc/data/1000mc_500bs.rds"))

rf.pred.train1 <- data_train %>% 
    filter(!ID == "USOWC",
           mc_i == 1) %>%   
    mutate(FCH4P = predict(rf_model[[1]], .),
           index = 1:n())


rf.pred.train2 <- data_train %>% 
    filter(ID %in% c("FISi2", "FISii"),
           mc_i == 2) %>%   
    mutate(FCH4P = predict(rf_model[[2]], .),
           index = 1:n())

rf.pred.train <- rf.pred.train1 %>% bind_rows(rf.pred.train2)

write.csv(rf.pred.train, paste0(loc, "mc/predictions/rf_mc_train_example.csv"))

# free axes
rf.pred.train %>% 
  mutate(week_dec = Week/52,
         year_dec = Year+week_dec) %>% 
  ggplot(aes(year_dec, FCH4P)) + 
  geom_point(color = "blue", alpha = 0.7) + 
  geom_point(aes(year_dec, FCH4), size = 0.5) +
  facet_wrap(~ID, scales = "free") + 
  labs(x = "Decimal Year", y = expression("FCH4 Predicted (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste0(loc, "mc/predictions/timeseries_train_example.png", sep = ""),
       width = 50, height = 35, units = c("cm"), dpi = 300)

# fixed x
rf.pred.train %>% 
  mutate(week_dec = Week/52,
         year_dec = Year+week_dec) %>% 
  ggplot(aes(year_dec, FCH4P)) + 
  geom_point(color = "blue", alpha = 0.7) + 
  geom_point(aes(year_dec, FCH4), size = 0.5) +
  facet_wrap(~ID, scales = "free_x") + 
  labs(x = "Decimal Year", y = expression("FCH4 Predicted (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste0(loc, "mc/predictions/timeseries_train_example_fixed.png", sep = ""),
       width = 50, height = 35, units = c("cm"), dpi = 300)

```




#### 10. Data Representativeness

##### Prepare Grids

##### Read in generic (for resampling)

```{r}
rpot <- stack(paste(loc, "grids/computed/Rpot.nc", sep = ""))
```

##### Prepare worldclim

```{r echo = F, message = F, warning = F}
wc_mtdq <- raster(paste(loc, "grids/worldclim/wc2.0_bio_30s_09.tif", sep = ""))
wc_pwtm <- raster(paste(loc, "grids/worldclim/wc2.0_bio_30s_13.tif", sep = ""))

wc_mtdq <- resample(wc_mtdq, rpot)
wc_pwtm <- resample(wc_pwtm, rpot)

writeRaster(wc_mtdq, paste(loc, "grids/ffs/wc_mtdq.tif", sep = ""), overwrite=TRUE)
writeRaster(wc_pwtm, paste(loc, "grids/ffs/wc_pwtm.tif", sep = ""), overwrite=TRUE)
```

##### Prepare canopy height

```{r}
canopyht <- raster(paste(loc, "grids/global-canopy-height/Simard_Pinto_3DGlobalVeg_JGR.tif", sep = ""))
canopyht_res <- resample(canopyht, rpot)
writeRaster(canopyht_res, paste(loc, "grids/ffs/canopyht.tif", sep = ""), overwrite=TRUE)
```

##### Air Temperature (MERRA2)

```{r}
raster.names <- list.files(paste(loc, "grids/merra2/", sep = ""), pattern = "nc4$", full.names = F) %>% as.list()
nc_data <- lapply(paste(loc, "grids/merra2/", raster.names, sep = ""), stack)
```

##### Save the print(nc) dump to a text file

```{r}
{
  sink('merra2_2001-2018_metadata.txt')
  print(nc_data)
  sink()
}
```

##### Look at metadatafile to see which variable to use

```{r}
# lon <- ncvar_get(nc_data[[1]], "lon")
# lat <- ncvar_get(nc_data[[1]], "lat")
# fillvalue <- ncatt_get(nc_data[[1]], "T2M", "_FillValue")
```

##### Get just temperature and close nc files

```{r}
# TA <- list()
# for(i in 1:length(nc_data)){
#   TA[[i]] <- ncvar_get(nc_data[[i]], "T2M")
# }
```

##### replace fill values with NA

```{r}
# TA_r <- list()
# 
# for(i in 1:length(nc_data)) {
#   
#   TA[[i]][TA[[i]] == fillvalue$value] <- NA
#   
#   TA_r[[i]] <- raster(t(TA[[i]]), xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0")) 
#   TA_r[[i]] <- flip(TA_r[[i]], direction = 'y')
# 
# }
```

##### Create Merra2 TA Stack

```{r}
TA_stack <- stack(nc_data[13:228])
TA_stack <- resample(TA_stack, rpot) # resample to 0.25degree
TA_stack <- TA_stack-273.15 # convert to celcius
writeRaster(TA_stack, paste(loc, "grids/ffs/TA.nc", sep = ""), format = "CDF", overwrite = T)
```

##### Create TA_msc and TA_mcs_LAG2

```{r}
msc <- list()
for(i in 1:12){
  msc[[i]] <- c(seq(i,204+i,12))
}

TA_msc <- list()
for(i in 1:12){
  TA_msc[[i]] <- mean(TA_stack[[ msc[[i]] ]])
}

TA_msc <- stack(TA_msc)
names(TA_msc) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")             
writeRaster(TA_msc, paste(loc, "grids/ffs/TA_msc.nc", sep = ""), format="CDF", overwrite = T)

TA_msc_LAG4 <- stack(TA_msc[[12]], TA_msc[[1:11]])
TA_msc_LAG2 <- TA_msc + (TA_msc_LAG4 - TA_msc)*0.5

writeRaster(TA_msc_LAG2, paste(loc, "grids/ffs/TA_msc_LAG2.nc", sep = ""), format="CDF", overwrite = T)
```

##### Create TA_LAG2

Fill end of stack with Dec from msc

```{r}
TA_LAG4 <- stack(TA_msc[[12]], TA_stack[[1:215]])
TA_LAG2 <- TA_stack + (TA_LAG4 - TA_stack)*0.5
writeRaster(TA_LAG2, paste(loc, "grids/ffs/TA_LAG2.nc", sep = ""), format="CDF", overwrite = T)
```

##### Create EVI_LAG3 

```{r}
raster.names <-list.files( paste(loc, "modis/grids/evi/", sep = ""), pattern = "tif$", full.names = FALSE)
evi <- stack(paste(loc, "modis/grids/evi/", raster.names, sep = ""))

msc <- list()
for(i in 1:12){
  msc[[i]] <- c(seq(i,204+i,12))
}

evi_msc <- list()
for(i in 1:12){
  evi_msc[[i]] <- mean(evi[[ msc[[i]] ]], na.rm = T)
}

evi_msc <- stack(evi_msc)
evi_msc <- resample(evi_msc, rpot) # resample to 0.25degree

names(evi_msc) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")             
writeRaster(evi_msc, paste(loc, "grids/ffs/EVI_msc.nc", sep = ""), format="CDF", overwrite = T)
```

##### Get EVI_msc_LAG3

```{r}
EVI_msc_LAG4 <- stack(evi_msc$Dec, evi_msc[[1:11]])
EVI_msc_LAG3 <- evi_msc + (EVI_msc_LAG4 - evi_msc)*0.75
writeRaster(EVI_msc_LAG3, paste(loc, "grids/ffs/EVI_msc_LAG3.nc", sep = ""), format="CDF", overwrite = T)
```

##### Get EVI_LAG3

```{r echo = F, message = F, warning = F}
evi <- resample(evi, rpot) # resample to 0.25degree
EVI_LAG3 <- stack(evi_msc$Dec, evi[[1:215]])
writeRaster(EVI_LAG3, paste(loc, "grids/ffs/EVI_LAG3.nc", sep = ""), format="CDF", overwrite = T)

```

#### 10. (cont.) Global Dissimilarity

##### Get colors

```{r}
# create color palette
# qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
# col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
# col_vector_values = sample(col_vector, 28, replace=F)
# 
# col_vector_values %>%
#   array(dim=c(28,1)) %>%
#   as_tibble() %>%
#   rename(color_id = V1) %>%
# write.csv(paste(loc, "dissimilarity/figures/color_ramp_constituency.csv", sep = ""), row.names = F)

col_vector_values <- read.csv(paste(loc, "dissimilarity/figures/200827_color_ramp_constituency.csv", sep = "")) %>% 
  dplyr::select(color_id) %>% 
  pull()
```


##### Import additional libraries

```{r}
source(paste(loc, "dissimilarity/rplots/tower_color_ls.r", sep=  ""))
source(paste(loc, "dissimilarity/rplots/themes/line_plot_theme.r", sep=  ""))
source(paste(loc, "dissimilarity/rplots/themes/map_theme.r", sep=  ""))
source(paste(loc, "dissimilarity/rplots/plots/custom_theme.R", sep=  ""))
options(stringsAsFactors = F)
```

##### Setup common extent and predictors

```{r}
# define common extent
com_ext <- extent(-180, 180,  -56, 85)

# Number of predictors being used for analysis
num_pred = 6
```

##### Read static grids

```{r}
canopyht <- raster(paste0(loc, 'grids/ffs/canopyht.tif')) %>% crop(com_ext)
wc_mtdq <- raster(paste0(loc, 'grids/ffs/wc_mtdq.tif')) %>% crop(com_ext)
wc_pwtm <- raster(paste0(loc, 'grids/ffs/wc_pwtm.tif')) %>% crop(com_ext)
```

##### Read temporal msc (limit dissim. analysis to msc)

```{r}
TA <- stack(paste0(loc, 'grids/ffs/TA_msc.nc')) %>% crop(com_ext)
TA_LAG2 <- stack(paste0(loc, 'grids/ffs/TA_msc_LAG2.nc')) %>% crop(com_ext)
EVI_LAG3 <- stack(paste0(loc, 'grids/ffs/EVI_msc_LAG3.nc')) %>% crop(com_ext)
```

##### Create a stack (without temporal predictors for now)

```{r}
all_stack <- list()
for (i in 1:12){
  all_stack[[i]]<- stack(canopyht, wc_mtdq, wc_pwtm) 
}
```

##### Define function to rescale static grids

```{r}
rescale01 <- function(x) {
  (x - cellStats(x, stat = 'min')) / (cellStats(x, stat = 'max') - cellStats(x, stat = 'min')) }
```

##### Rescale static grids

```{r}
scaled_data <- lapply(all_stack, rescale01)
```

##### Define function to rescale temporal grids

```{r}
rescale02 <- function(x) {
  (x - min(x, na.rm = T)) / (max(x, na.rm = T) - min(x, na.rm = T)) }
```

##### Rescale temporal grids

```{r}
scaled_data_2 <- rescale02(TA)
scaled_data_3 <- rescale02(TA_LAG2)
scaled_data_4 <- rescale02(EVI_LAG3)

```

###### Recombine and rename

```{r}
for (i in 1:12){
  all_stack[[i]] <- stack(scaled_data[[i]], 
                              scaled_data_2[[i]], 
                              scaled_data_3[[i]], 
                              scaled_data_4[[i]]) 
  names(all_stack[[i]]) <- c("canopyht", "wc_mtdq", "wc_pwtm",
                             "TA", "TA_LAG2", "EVI_LAG3")
}

all_stack_full <- stack(all_stack)
writeRaster(all_stack_full, paste0(loc, "dissimilarity/output/stacks/all.nc"), format='CDF', overwrite = T)
```

##### Read in all stack (for subsequent attempts)

```{r}
all_stack_full <- stack(paste0(loc, "dissimilarity/output/stacks/all.nc"))
                        
# recreate all_stack list
month_vector_st <- c(seq(1,72,6))
month_vector_end <- month_vector_st+5

all_stack <- list()
for (i in 1:12){
  all_stack[[i]] <- all_stack_full[[c(month_vector_st[i]:(month_vector_end[i]))]]
  names(all_stack[[i]]) <- c("canopyht", "wc_mtdq", "wc_pwtm",
                             "TA", "TA_LAG2", "EVI_LAG3")
}
```

###### set up weights

Weights come from scaled varimp output

```{r}
rescaling_weights <- c(100, # TA
                       68,  # wc_pwtm
                       67,  # wc_mtdq
                       50,  # TA_LAG2
                       27,  # canopyht
                       11   # EVI_LAG3
                       ) /100 

rescaling_weights <- c("TA" = rescaling_weights[1], 
                       "wc_pwtm" = rescaling_weights[2], 
                       "wc_mtdq" = rescaling_weights[3],
                       "TA_LAG2" = rescaling_weights[4], 
                       "canopyht" = rescaling_weights[5], 
                       "EVI_LAG3" = rescaling_weights[6])

for (i in 1:12){
  all_stack[[i]]$TA <- all_stack[[i]]$TA*rescaling_weights["TA"]
  all_stack[[i]]$wc_pwtm <- all_stack[[i]]$wc_pwtm*rescaling_weights["wc_pwtm"]
  all_stack[[i]]$wc_mtdq <- all_stack[[i]]$wc_mtdq*rescaling_weights["wc_mtdq"]
  all_stack[[i]]$TA_LAG2 <- all_stack[[i]]$TA_LAG2*rescaling_weights["TA_LAG2"]
  all_stack[[i]]$canopyht <- all_stack[[i]]$canopyht*rescaling_weights["canopyht"]
  all_stack[[i]]$EVI_LAG3 <- all_stack[[i]]$EVI_LAG3*rescaling_weights["EVI_LAG3"]
}

all_stack_weighted_full <- stack(all_stack)
writeRaster(all_stack_weighted_full, paste0(loc, "dissimilarity/output/stacks/all_weighted.nc"), format='CDF', overwrite = T)
```

##### Read in all stack weighted (for subsequent attempts)

```{r}
all_stack_weighted_full <- stack(paste0(loc, "dissimilarity/output/stacks/all_stack_weighted_full.nc"))
month_vector_st <- c(seq(1,72,6))
month_vector_end <- month_vector_st+5

all_stack <- list()
for (i in 1:12){
  all_stack[[i]] <- all_stack_weighted_full[[c(month_vector_st[i]:(month_vector_end[i]))]]
  names(all_stack[[i]]) <- c("canopyht", "wc_mtdq", "wc_pwtm",
                             "TA", "TA_LAG2", "EVI_LAG3")
}
```

##### Read wetland map ( Fw = fraction wetland )

Only use the first year of data (2000)

```{r}
Fw <- stack(paste0(loc, "grids/WAD2M/gcp-ch4_wetlands_2000-2018_025deg.nc"))
Fw <- Fw[[1:12]]
```

##### Crop out Antartica

```{r}
Fw <- crop(Fw, com_ext)
```

##### Mask out zero wetland areas

```{r}
all_stack_masked <- list()
for(i in 1:12){
  all_stack_masked[[i]] <- mask(all_stack[[i]], Fw[[i]])
}

all_stack_masked_full <- stack(all_stack_masked)
plot(all_stack[[6]])
writeRaster(all_stack_masked_full, paste0(loc, "dissimilarity/output/stacks/all_weighted_masked.nc"), format='CDF', overwrite = TRUE)
```

##### Read in all stack masked and weighted (for subsequent attempts)

```{r}
all_stack_masked_full <- stack(paste0(loc, "dissimilarity/output/stacks/all_weighted_masked.nc"))
month_vector_st <- c(seq(1,72,6))
month_vector_end <- month_vector_st+5

all_stack_masked <- list()
for (i in 1:12){
  all_stack_masked[[i]] <- all_stack_masked_full[[c(month_vector_st[i]:(month_vector_end[i]))]]
  names(all_stack_masked[[i]]) <- c("canopyht", "wc_mtdq", "wc_pwtm",
                             "TA", "TA_LAG2", "EVI_LAG3")
}

```

##### Convert monthly (as list) stack to dataframes

```{r}
all_stack_masked_df <- list()
for (i in 1:12){
  all_stack_masked_df[[i]] <- as.data.frame(as(all_stack_masked[[i]], "SpatialPixelsDataFrame")) %>% filter_all(all_vars(!is.na(.)))
}
Fw_df <- as.data.frame(as(Fw, "SpatialPixelsDataFrame")) %>% filter_all(all_vars(!is.na(.)))
```


##### Read tower data

```{r}
tower_data <- read.csv("/Volumes/Samsung_T5/Stanford/upch4_local/training-data/final.csv")
head(tower_data)

# Get coordinates from the tower data
coords <- tower_data %>% 
  dplyr::select(Month, Cluster, ID, Longitude, Latitude) %>% 
  group_by(Month, Cluster, ID) %>% unique() %>% 
  arrange(Month) 

# get it for each month
coords_month <- list()
for (i in 1:12){
  coords_month[[i]] <- coords %>% 
    filter(Month == i)
}

# check points line up
plot(all_stack[[7]]$EVI_LAG3)
points(coords[,4:5])
```

```{r}
# Extract at points
all_sites <- list()
for (i in 1:12){
  all_sites[[i]] <- raster::extract(all_stack[[i]], coords_month[[i]][,4:5]) %>% 
    as_tibble() %>% 
    bind_cols(coords_month[[i]]) %>% 
    dplyr::select(Month, Cluster, ID, Latitude, Longitude, everything())
}
all_sites <- bind_rows(all_sites)
```

##### Run dissimilarity analysis

```{r}
# set up lists and outputs
monthly_global_DI <- list()
all_sites_mean_DI <- list()

month_seq <- all_sites$Month # 26 clusters * 12 months
length(month_seq)

for (i in 1:12){ # months
  
  # Calculate eucledian distance between each tower (28) and each pixel (> 100000 pixels)
  distance <- proxy::dist(y = all_sites[c(6:length(all_sites))], 
                          x = all_stack_masked_df[[i]][1:(length(all_stack_masked_df[[i]])-2)], 
                          method = "Euclidean",
                          diag = FALSE, 
                          upper = FALSE)
  
  # Reformat to be usable for analysis
  distance <- as.data.frame(as.matrix.data.frame(distance))
  str(distance)
  
  names(distance) <- all_sites %>% dplyr::select(ID) %>% pull() %>% paste("_",month_seq,sep="")
  Clusters <- all_sites %>%  dplyr::select(Cluster) %>% pull()
  Cluster_names <- all_sites %>% group_by(Cluster) %>% summarize(ID = ID[1]) %>% pull()
  all_stack_masked_df_wdist <- cbind(all_stack_masked_df[[i]], distance)
  
  # Find the minimum distance in terms of the predictor variables at each pixel on Earth from the network of towers
  num_col = ncol(all_stack_masked_df_wdist)
  
  # get minimum distance, and the name of the closest tower
  all_stack_masked_df_wdist <- all_stack_masked_df_wdist %>% 
    mutate(min_dist = apply(all_stack_masked_df_wdist[, (num_pred + 3):num_col], MARGIN = 1, FUN = min, na.rm = TRUE),
           closest = c(apply(all_stack_masked_df_wdist[, (num_pred + 3):num_col], MARGIN = 1, FUN = which.min)),
           closest_tower = names(distance)[closest],
           closest_Cluster = Clusters[closest],
           closest_Cluster = Cluster_names[closest_Cluster])
  
  
  # Calculate eucledian distance between towers
  distance_network <- proxy::dist(y = all_sites[c(6:length(all_sites))], 
                                  x = all_sites[c(6:length(all_sites))], 
                                  method = "Euclidean",
                                  diag = FALSE, 
                                  upper = FALSE)
  
  # Reformat to be usable for analysis
  distance_network <- as.data.frame(as.matrix.data.frame(distance_network))
  str(distance_network)
  names(distance_network) <- all_sites %>% dplyr::select(ID) %>% pull() %>% paste("_",month_seq,sep="")
  all_sites_wdist <- cbind(all_sites, distance_network)
  
  num_col = ncol(all_sites_wdist)
  
  # get overall mean tower-tower distance
  all_sites_mean_distance <- all_sites_wdist %>% 
    na_if(0) %>% 
    mutate(mean_dist = apply(all_sites_wdist[, (num_pred + 6):num_col], MARGIN = 1, FUN = mean, na.rm = TRUE)) %>% 
    pull(mean_dist) %>% mean()
  
  # pixel DI =  min distance / mean distance
  all_stack_masked_df_wdist <- all_stack_masked_df_wdist %>% 
    mutate(DI = min_dist / all_sites_mean_distance)
  
  str(all_sites_wdist)
  
  # AOA (area of applicability) calculation
  # Step 0: Gather data
  all_sites_wdist <- all_sites_wdist %>% 
    dplyr::select(Month, Cluster, ID,c((num_pred + 6):num_col)) %>% 
    gather(key = "ID_2", value = "distance", 4:449) %>% 
    mutate(ID_2 = substr(ID_2,1,5)) %>% 
    unique()

  # Step 1: Get the minimum distance between each tower and the network (excluding other towers in the same Cluster)
  id_names <- all_sites_wdist %>% dplyr::select(ID) %>% pull() %>% factor() %>%  levels()
  Clusters <- all_sites_wdist %>% group_by(ID) %>% summarize(Clusters = Cluster[1]) %>% dplyr::select(Clusters) %>% pull()
  Clusters_vector <- c(rep(Clusters,each = 12)) %>% as_tibble() %>% rename(Cluster = value)

  all_sites_min_dist <- list()
  for (j in 1:43) {
    all_sites_min_dist[[j]] <- all_sites_wdist %>% 
      filter(!Cluster == Clusters[j]) %>% 
      filter(ID != ID_2) %>% 
      filter(ID_2 == id_names[j]) %>% 
      group_by(Month, ID_2) %>% 
      summarize(min_dist = min(distance))
  }
  
  all_sites_min_dist <- bind_rows(all_sites_min_dist) %>% 
    bind_cols(Clusters_vector) %>% 
    as_tibble() %>% arrange(Month, ID_2) %>% 
    dplyr::select(Month, Cluster, ID = ID_2, min_dist)
  
  
  # Step 2: Get the average distance between all tower training points (excluding towers in the same Cluster)
  all_sites_cvCluster_dist <- list()
  for (j in 1:43) {
    all_sites_cvCluster_dist[[j]] <- all_sites_wdist %>% 
      filter(!Cluster == Clusters[j]) %>% 
      filter(ID != ID_2) %>% 
      filter(!ID_2 == id_names[j]) %>% 
      summarize(ID = id_names[j],
                mean_cvCluster_dist = mean(distance))
  }
  
  all_sites_cvCluster_dist <- bind_rows(all_sites_cvCluster_dist) 
  
  
  # Step 3: Create a cvCluster tibble and calculate the DI for each site
  all_sites_DI <- all_sites_min_dist %>% left_join(all_sites_cvCluster_dist, by = c("ID")) %>% 
    mutate(DI = min_dist/mean_cvCluster_dist)
  
  all_sites_DI_AOA <- quantile(all_sites_DI$DI, 0.95) 
  # all_sites_DI_sd <- sd(all_sites_DI$DI) 
  # all_sites_AOA <- all_sites_DI_mean+all_sites_DI_sd # use this as AOA threshold
  
  # get output
  monthly_global_DI[[i]] <- all_stack_masked_df_wdist
  #
  
  ## Make an outline of the countries for the plot
  data(coastsCoarse)
  map_outline_df <- fortify(coastsCoarse)
  sPDF <- getMap()[getMap()$ADMIN!='Antarctica',]

  # # dissimilarity plots
  # ggplot() +
  #   geom_polygon(data=sPDF, aes(long, lat, group=group), fill="grey90") +
  #   geom_tile(data = all_stack_masked_df_wdist, aes(x = x, y = y, fill = DI)) +
  #   geom_point(data = coords[,4:5],
  #              aes(x = Longitude, y = Latitude), size = 1.5, color = "blue") +
  #   geom_path(data = map_outline_df, aes(long, lat, group = group), color = 'grey20', size = 0.3) +
  #   coord_equal() +
  #   scale_fill_gradient2(low = muted("blue"), mid = 'white', high = "red", midpoint = all_sites_DI_AOA) +
  #   scale_y_continuous(limits=c(-60, 85)) +
  #   labs(title = "Global Dissimilarity Index from 43 FLUXNET-CH4 Wetlands") +
  #   theme_map(8) +
  #   theme(legend.position = 'right')
  # 
  # ggsave(file = paste("all_DI_aoa",i,".png", sep=""),
  #        path = paste0(loc, "dissimilarity/figures/"),
  #        width = 188, height = 100, dpi = 600, units = "mm")
  
  # constituency plots
  ggplot() +
    geom_polygon(data=sPDF, aes(long, lat, group=group), fill="grey90") +
    geom_tile(data = all_stack_masked_df_wdist, aes(x = x, y = y, fill = closest_Cluster)) +
    geom_point(data = coords[,4:5],
               aes(x = Longitude, y = Latitude), size = 1.5, color = "blue") +
    geom_path(data = map_outline_df, aes(long, lat, group = group), color = 'grey20', size = 0.3) +
    coord_equal() +
    scale_fill_manual(values = col_vector_values) +
    scale_y_continuous(limits=c(-60, 85)) +
    labs(title = "Tower Constituency") +
    theme_map(8) +
    theme(legend.position = 'right')
    

  ggsave(file = paste("all_tower_constituency",i,".png", sep=""),
         path = paste0(loc, "dissimilarity/figures/"),
         width = 188, height = 100, dpi = 600, units = "mm")
}
```

##### Output the monthly distance and other metrics


```{r}
for(i in 1:12){
  monthly_global_DI[[i]] <- monthly_global_DI[[i]] %>% 
    as_tibble() %>% 
    dplyr::select(1:15,min_dist,closest_tower,closest_Cluster,DI) %>% 
    mutate(Month = i)
}

for(i in 1:12){
write.csv(monthly_global_DI[[i]], paste0(loc, "dissimilarity/output/stats/all_monthly_global_DI",i,".csv"), row.names = F)
}
```

##### Remove large stack

```{r}
rm("all_stack")
```

##### Create Global DI dataframe

```{r}
monthly_global_DI <- list()
for(i in 1:12){
monthly_global_DI[[i]] <- read.csv(paste0(loc, "dissimilarity/output/stats/all_monthly_global_DI",i,".csv"))
}

monthly_global_DI <- bind_rows(monthly_global_DI)

monthly_global_DI_subset <- list()
for(i in 1:12){
  monthly_global_DI_subset[[i]] <- monthly_global_DI %>% 
    filter(Month == i) %>% 
    dplyr::select(closest_fold = closest_Cluster, closest_tower, min_dist, DI, x, y) %>% 
    mutate(Month = i)
}
monthly_global_DI_subset <- bind_rows(monthly_global_DI_subset)
write.csv(monthly_global_DI_subset, paste0(loc, "dissimilarity/output/stats/all_monthly_global_DI_subset.csv"), row.names = F)
```

##### Look at the distribution of distances 

```{r}
monthly_global_DI %>% 
  dplyr::select(closest_tower, min_dist) %>% 
  mutate(closest_tower = as.factor(closest_tower)) %>% 
  ggplot(aes(min_dist)) +
  geom_histogram() +
  geom_vline(xintercept = median(monthly_global_DI$min_dist)) +
  my_theme
ggsave(paste0(loc, "dissimilarity/figures/all_min_dist_distribution.png"),
       width = 10, height = 8, units = c("cm"), dpi = 300)
```

##### Look at the distribution of dissimilarities 

```{r}
monthly_global_DI %>% 
  dplyr::select(closest_tower, DI) %>% 
  mutate(closest_tower = as.factor(closest_tower)) %>% 
  ggplot(aes(DI)) +
  geom_histogram() +
  geom_vline(xintercept = median(monthly_global_DI$DI)) +
  my_theme
ggsave(paste0(loc, "dissimilarity/figures/all_DI_distribution.png"),
       width = 10, height = 8, units = c("cm"), dpi = 300)
```

##### What is the summed annual dissimilarity and distance?

```{r}
monthly_global_DI %>% 
  dplyr::select(DI, min_dist) %>% 
  summarize(global_DI_sum = sum(DI),
            global_min_dist_sum = sum(min_dist),
            global_DI_mean = mean(DI),
            global_min_dist_mean = mean(min_dist)) %>% 
  write.csv(paste0(loc, "dissimilarity/output/stats/annual_DI_mindist.csv"), row.names = F)
```

##### What is the monthly dissimilairty and distance?

```{r}
monthly_global_DI %>% 
  dplyr::select(Month, DI, min_dist) %>% 
  group_by(Month) %>% 
  summarize(global_DI_sum = sum(DI),
            global_min_dist_sum = sum(min_dist),
            global_DI_mean = mean(DI),
            global_min_dist_mean = mean(min_dist)) %>% 
  write.csv(paste0(loc, "dissimilarity/output/stats/monthly_DI_mindist.csv"), row.names = F)
```

##### what is the summed AOA (annual)

```{r}
monthly_global_DI %>% 
  dplyr::select(DI) %>% 
  summarize(global_sum_AOA = sum(DI < all_sites_DI_AOA),
            global_sum_AOE = sum(DI > all_sites_DI_AOA),
            total_area = sum(global_sum_AOA,global_sum_AOE)) %>% 
  mutate(fractionAOA = global_sum_AOA / total_area) %>% 
  write.csv(paste0(loc, "dissimilarity/output/stats/annual_AOA_AOE.csv"), row.names = F)
```

##### what is the summed AOA (monthly)

```{r}
monthly_global_DI %>% 
  dplyr::select(Month, DI) %>% 
  group_by(Month) %>% 
  summarize(global_sum_AOA = sum(DI < all_sites_DI_AOA),
            global_sum_AOE = sum(DI > all_sites_DI_AOA),
            total_area = sum(global_sum_AOA,global_sum_AOE)) %>% 
  mutate(fractionAOA = global_sum_AOA / total_area) %>% 
  write.csv(paste0(loc, "dissimilarity/output/stats/monthly_AOA_AOE.csv"), row.names = F)
```

##### look at constituency size, and the relationship with mean dissimilarity from the tower 

```{r}
monthly_global_DI %>% 
  dplyr::select(closest_tower, DI) %>% 
  mutate(closest_tower = substr(closest_tower,1,5)) %>% 
  mutate(closest_tower = as.factor(closest_tower),
         count = 1)  %>% 
  group_by(closest_tower) %>% 
  summarize(pixels = sum(count),
            DI = mean(DI)) %>% 
  ungroup() %>% 
  mutate(cover = pixels/sum(pixels)*100) %>% 
  arrange(desc(cover)) %>% 
  ggplot(aes(cover, DI)) + 
  geom_point() +
  labs(x = "Global Cover (%)", y = "Dissimilarity Index") +
  my_theme
ggsave(paste0(loc, "dissimilarity/figures/all_constituency_size_DI.png"),
       width = 15, height = 12, units = c("cm"), dpi = 300)
```

##### Figure of the rank of DI coverages (annual by ID)

```{r}
monthly_global_DI %>% 
  dplyr::select(closest_tower, DI) %>% 
  mutate(closest_tower = substr(closest_tower,1,5)) %>% 
  mutate(closest_tower = as.factor(closest_tower),
         count = 1) %>% 
  group_by(closest_tower) %>% 
  summarize(pixels = sum(count),
            DI = mean(DI)) %>% 
  ungroup() %>% 
  mutate(cover = pixels/sum(pixels)*100) %>% 
  arrange(desc(cover)) %>% 
  mutate(closest_tower = fct_reorder(closest_tower, cover)) %>% 
  ggplot(aes(x = closest_tower, y = cover)) +
  geom_segment(aes(x = closest_tower, xend = closest_tower, y = 0, yend = cover), color = 'grey', stat = "identity") +
  geom_point(size = 4) +
  coord_flip() +
  xlab("ID") + ylab("Global Annual Pixel Cover (%)") +
  my_theme
ggsave(paste0(loc, "dissimilarity/figures/all_constituency_size_rank.png"),
       width = 10, height = 20, units = c("cm"), dpi = 300)
```

##### Figure of the rank of coverages (monthly by cluster)

```{r}
monthly_global_DI_cover <- monthly_global_DI %>% 
  dplyr::select(Month, closest_Cluster, DI) %>% 
  mutate(closest_Cluster = as.factor(closest_Cluster),
         count = 1) %>% 
  group_by(Month, closest_Cluster) %>% 
  summarize(pixels = sum(count),
            DI = mean(DI)) %>% 
  group_by(Month) %>% 
  mutate(cover = pixels/sum(pixels)*100) 

 
for (i in 1:12){
  monthly_global_DI_cover %>% 
    filter(Month == i) %>% 
    mutate(closest_Cluster = fct_reorder(closest_Cluster, cover)) %>% 
    ggplot(aes(x = closest_Cluster, y = cover)) +
    geom_segment(aes(x = closest_Cluster, xend = closest_Cluster, y = 0, yend = cover), color = 'grey', stat = "identity") +
    geom_point(size = 4) +
    coord_flip() +
    facet_wrap(~Month) +
    labs(c = "ID", y = "Global Annual Pixel Cover (%)") +
    my_theme
ggsave(paste0(loc, "dissimilarity/figures/all_constituency_size_rank",i,".png"),
         width = 10, height = 20, units = c("cm"), dpi = 300)
}

```

##### figure of the rank of dissimilarities

```{r}
for (i in 1:12){
  monthly_global_DI_cover %>% 
    filter(Month == i) %>% 
    mutate(closest_Cluster = fct_reorder(closest_Cluster, DI)) %>% 
    ggplot(aes(x = closest_Cluster, y = DI)) +
    geom_segment(aes(x = closest_Cluster, xend = closest_Cluster, y = 0, yend = DI), color = 'grey', stat = "identity") +
    geom_point(size = 4) +
    coord_flip() +
    facet_wrap(~Month) +
    scale_y_continuous(limits = c(0,0.2)) +
    xlab("Closest Cluster") +
    my_theme
  ggsave(paste0(loc, "dissimilarity/figures/all_constituency_DI_rank",i,".png"),
         width = 10, height = 20, units = c("cm"), dpi = 300)
}
```

##### Output the monthly mean DI for each site (from all sites not in the same fold)

```{r}
write.csv(all_sites_DI, paste0(loc,"dissimilarity/output/stats/all_sites_DI.csv"), row.names = F) 
```

##### Mean network dissimilarity

```{r}
all_sites_DI <- read.csv(paste0(loc, "dissimilarity/output/stats/all_sites_DI.csv"))

all_sites_DI_plot <- all_sites_DI %>% 
  mutate(ID = as.factor(ID)) %>% 
  group_by(ID) %>% 
  summarize(DI = mean(DI)) %>% 
  arrange(DI) %>% 
  mutate(order = row_number())

xlabels <- c(as.character(all_sites_DI_plot$ID))

# note that DI in this plot includes year-round conditions (including winter in Arctic when wetlands frozen)
all_sites_DI_plot %>% 
  ggplot(aes(x = order, y =  DI)) +
  geom_segment(aes(x = order, xend = order, y = 0, yend = DI), color = 'grey', stat = "identity") +
  geom_point(size = 4) + 
  geom_label(aes(x=order, y=0.015, label = xlabels), hjust=1, label.size = 0) +
  # scale_x_discrete(breaks = all_sites_mean_DI_all_plot$order, labels = xlabels) +
  coord_flip() +
  ylab("Overall Mean Network Dissimilarity") +
  my_theme +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank())
ggsave(paste0(loc, "dissimilarity/figures/all_site_mean_DI.png"),
       width = 20, height = 25, units = c("cm"), dpi = 300)

all_sites_DI_plot <- all_sites_DI %>% 
  filter(Month %in% c(6,7,8)) %>% 
  mutate(ID = as.factor(ID)) %>% 
  group_by(ID) %>% 
  summarize(DI = mean(DI)) %>% 
  arrange(DI) %>% 
  mutate(order = row_number())

xlabels <- c(as.character(all_sites_DI_plot$ID))

# this plot includes only growing season conditions (June-August)
all_sites_DI_plot %>% 
  ggplot(aes(x = order, y =  DI)) +
  geom_segment(aes(x = order, xend = order, y = 0, yend = DI), color = 'grey', stat = "identity") +
  geom_point(size = 4) + 
  geom_label(aes(x=order, y=0.015, label = xlabels), hjust=1.3, label.size = 0) +
  # scale_x_discrete(breaks = all_sites_mean_DI_all_plot$order, labels = xlabels) +
  coord_flip() +
  ylab("Overall Mean Network Dissimilarity") +
  my_theme +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank())
ggsave(paste0(loc, "dissimilarity/figures/all_site_mean_DI_growing_season.png"),
       width = 20, height = 25, units = c("cm"), dpi = 300)


all_sites_DI %>% 
  group_by(Month) %>% 
  summarize(AOA_95 = quantile(DI, 0.95)) %>% 
  write.csv(paste0(loc, "dissimilarity/output/stats/monthly_DI_95.csv"), row.names = F)
```

#### 10. (cont.) Dissimilarity - Extrapolation Error

##### Get perfrmance and DI info

```{r}
site_month_metrics <- read.csv(paste0(loc, "predictions/cv/global.csv"))
site_month_DI <- read.csv(paste0(loc, "dissimilarity/output/stats/all_sites_DI.csv"))
data <- read.csv(paste0(loc, "training-data/final.csv")) # local path
fold_names <- data %>% group_by(Cluster) %>% summarize(Name = ID[1]) 
```

##### Plot histogram of site-month DIs

```{r}
site_month_DI %>% 
  left_join(fold_names) %>% 
  ggplot(aes(DI, fill = factor(Name))) +
  geom_histogram() +
  scale_fill_manual(values = col_vector_values) +
  facet_wrap(~Month) +
  my_theme
ggsave(paste0(loc, "dissimilarity/figures/all_sites_DI_monthly_histogram.png"),
       width = 20, height = 20, units = c("cm"), dpi = 300)
```


##### MC Prediction Variance and Error vs. Dissimilarity

##### Read in MC data

```{r}
data <- read.csv(paste0(loc, "mc/predictions/rf_mc.csv"))
```

##### Subset MC Prediction data

```{r}
pred <- data %>% dplyr::select(mc_i, Cluster, ID, Year, Month, Week, FCH4Pe = FCH4P)
```

##### Calculate the monthly mean and variance of each prediction across mc iterations

```{r}
pred_full <- pred %>% 
  group_by(Cluster, ID, Year, Month) %>% 
  summarize(mean = mean(FCH4Pe, na.rm = TRUE),
            var = var(FCH4Pe, na.rm = TRUE)) 
```

##### Read in DI

```{r}
di <- read.csv(paste0(loc, "dissimilarity/output/stats/all_sites_DI.csv"))
```

##### Set up model and create label function

```{r}
di_curve <- di %>% 
  left_join(pred_full, by = c("Cluster", "ID", "Month")) %>% 
  filter(!is.na(mean))

lm_eqn <- function(df){
  m <- lm(y ~ x, df);
  eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2, 
                   list(a = format(unname(coef(m)[1]), digits = 2),
                        b = format(unname(coef(m)[2]), digits = 2),
                        r2 = format(summary(m)$r.squared, digits = 3)))
  as.character(as.expression(eq));
}

```

##### Convert to simple data frame

No interesting result.

```{r}
df <- di_curve %>% dplyr::select(y = var, x = DI)

di_curve %>%
  filter(!ID == "USOWC") %>%
  ggplot(aes(DI, var)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  # geom_text(x = 0.2, y = 6000, label = lm_eqn(df), parse = TRUE, size = 5) +
  labs(x = "Dissimilairty Index", y = expression(atop("Extraplation Variance", "(nmol m"^{-2}*" s"^{-1}*")"))) +
  my_theme
ggsave(paste0(loc, "dissimilarity/figures/di_vs_variance.png", sep = ""),
       width = 14, height = 12, units = c("cm"), dpi = 300)

summary(lm(y ~ x, data = df))
```


##### Look at relationship (MAE vs. DI)

```{r}
site_month_DI %>% 
  filter(ID != "USOWC") %>% 
  dplyr::select(ID, Month, min_dist, mean_cvCluster_dist, DI) %>% 
  right_join(site_month_metrics, by = c("ID", "Month")) %>% 
  ggplot(aes(DI, MAE)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  scale_y_continuous(limits = c(0,300)) +
  labs(x = "Dissimilarity Index", y = expression(atop("MAE", "(nmol m"^{-2}*" s"^{-1}*")"))) +
  my_theme
ggsave(paste0(loc, "dissimilarity/figures/error_vs_DI.png"),
       width = 15, height = 12, units = c("cm"), dpi = 300)
```


#### 11. Upscaling
##### Create Forcing Data

```{r}
# define common extent
com_ext <- extent(-180, 180,  -56, 85)

# Number of predictors being used for analysis
num_pred = 6
```

##### Read static grids

```{r}
canopyht <- raster(paste0(loc, 'grids/ffs/canopyht.tif')) %>% crop(com_ext)
wc_mtdq <- raster(paste0(loc, 'grids/ffs/wc_mtdq.tif')) %>% crop(com_ext)
wc_pwtm <- raster(paste0(loc, 'grids/ffs/wc_pwtm.tif')) %>% crop(com_ext)
```

##### Read temporal grids

```{r echo = F, message = F, warning = F}
TA <- stack(paste0(loc, 'grids/ffs/TA.nc')) %>% crop(com_ext)
TA_LAG2 <- stack(paste0(loc, 'grids/ffs/TA_LAG2.nc')) %>% crop(com_ext)
EVI_LAG3 <- stack(paste0(loc, 'grids/ffs/EVI_LAG3.nc')) %>% crop(com_ext)
```

##### Look at grids QC

```{r}
plot(EVI_LAG3[[1:7]])

evi_ts <- as_tibble(raster::extract(EVI_LAG3, site.coords))
  evi_ts <- cbind(sites, evi_ts)
```


##### Check if can fill gaps in `EVI_LAG3` with `EVI_msc_LAG3`

```{r}
EVI_msc_LAG3 <- stack(paste0(loc, "grids/ffs/EVI_msc_LAG3.nc")) %>% crop(com_ext)
plot(EVI_msc_LAG3[[7]])
```

##### Fill gaps with `EVI_msc_LAG3`

```{r}
month_index <- c(rep(1:12, 18))

for(i in 1:216){
  na_mask_temp <- is.na(EVI_LAG3[[i]])
  msc_temp <- crop(EVI_msc_LAG3[[month_index[i]]], na_mask_temp)
  EVI_LAG3[[i]] <- cover(EVI_LAG3[[i]], msc_temp)
}
```

##### Create a stack (without temporal predictors for now)

```{r}
upscaling_stack <- list()
for (i in 1:216){
  upscaling_stack[[i]]<- stack(canopyht, wc_mtdq, wc_pwtm, TA[[i]], TA_LAG2[[i]], EVI_LAG3[[i]]) 
  names(upscaling_stack[[i]]) <- c("canopyht", "wc_mtdq", "wc_pwtm", "TA", "TA_LAG2", "EVI_LAG3")
}

plot(upscaling_stack[[7]])
```

##### Write out upscaling forcing

```{r}
digit1 <- c(rep(0,99), rep(1,100), rep(2,17))
digit2 <- c(rep(0,9), rep(1:9, each = 10), rep(0:9, each = 10), rep(0, 10), rep(1, 7))
digit3 <- c(1:9, rep(0:9, 20), 0:6)

month_index <- bind_cols(digit1 = digit1, digit2 = digit2, digit3 = digit3) %>% unite(index, c(digit1, digit2, digit3), sep = "")

for(i in 1:216){
  writeRaster(upscaling_stack[[i]], paste0(loc, "grids/ffs/forcing/forcing_", month_index$index[i], ".nc"),format = "CDF", overwrite = T)
}
```



#### 11. (cont.) Upscaling: INSERT CLUSTER COMPUTING CODE (Etienne) 


#### 11. (cont.) Upscaling Evaluation 

##### Individual Bootstrap Sums 

```{r}
data <- read.csv(paste0(loc, "upscaling-output-raw/sums/boot_sum_TgCH4month_m1.csv")) %>% dplyr::select(2:5)

head(data)

# check that bootstrap mean equals Tg grids
data %>% 
  group_by(t) %>% 
  summarize(boot_mean = mean(sum_TgCH4month)) %>% head(30) 
```


##### Get the upscaling grid predictions at each site

```{r}
# read in site locations
sites <- read.csv(paste0(loc, "fluxnet-ch4-data/metadata/fluxnet-ch4-site-metadata.csv"))
site.coords <- cbind(sites$Longitude, sites$Latitude)

amflux.sites <- read.csv(paste0(loc, "fluxnet-ch4-data/metadata/ameriflux-validation-metadata.csv"))
val.coords <- cbind(amflux.sites$Longitude, amflux.sites$Latitude)

## read in etienne's nmol ensemble output
nmol <- nc_open(paste0(loc, "upscaling-output-raw/grids/upch4_v04_m1_nmolm2sec.nc"))
  
  attributes(nmol)

  mean <- ncvar_get(nmol, "mean_ch4")
  sd <- ncvar_get(nmol, "sd_ch4")
  var <- ncvar_get(nmol, "var_ch4")
  lon <- ncvar_get(nmol, "longitude")
  lat <- ncvar_get(nmol, "latitude", verbose = F)
  t <- ncvar_get(nmol, "Time")
  nc_close(nmol)
  
  mean.yr <- list()
  sd.yr <- list()
  var.yr <- list()
  for (j in 1:216) {
    mean.yr[[j]] <- raster(t(mean[,,j]), xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
    sd.yr[[j]] <- raster(t(sd[,,j]), xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
    var.yr[[j]] <- raster(t(var[,,j]), xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  }
  mean.all <- stack(mean.yr)
  sd.all <- stack(sd.yr)
  var.all <- stack(var.yr)
  
  # extract data at sites
  mean_nmol.site <- as_tibble(raster::extract(mean.all, site.coords))
  mean_nmol.val <- as_tibble(raster::extract(mean.all, val.coords))
  sd_nmol.site <- as_tibble(raster::extract(sd.all, site.coords))
  sd_nmol.val <- as_tibble(raster::extract(sd.all, val.coords))

  # join to site tables
  mean_nmol.site <- cbind(sites, mean_nmol.site)
  mean_nmol.val <- cbind(amflux.sites, mean_nmol.val)
  sd_nmol.site <- cbind(sites, sd_nmol.site)
  sd_nmol.val <- cbind(amflux.sites, sd_nmol.val)
  
  # gather into long files
  months <- rep(c(rep(1:12, each = 81)), 18) # 18 years x 12 months x 81 sites
  years <- c(rep(2001:2018, each = 12*81)) # 
  length(months) == length(years)

  mean_nmol.site <- mean_nmol.site %>% 
    gather(key = "layer", value = "mean_FCH4P_grid_nmol", 15:230) %>% 
    mutate(Year = years,
           Month = months) %>% 
    dplyr::select(1:14, Year, Month, mean_FCH4P_grid_nmol)
  
  sd_nmol.site <- sd_nmol.site %>% 
    gather(key = "layer", value = "sd_FCH4P_grid_nmol", 15:230) %>% 
    mutate(Year = years,
           Month = months) %>% 
    dplyr::select(ID, Year, Month, sd_FCH4P_grid_nmol)
  
  nmol.site <- mean_nmol.site %>% left_join(sd_nmol.site)
  
  # gather into long files
  months <- rep(c(rep(1:12, each = 7)), 18) # 18 years x 12 months x 7 sites
  years <- c(rep(2001:2018, each = 12*7)) # 
  length(months) == length(years)
  
   mean_nmol.val <- mean_nmol.val %>% 
    gather(key = "layer", value = "mean_FCH4P_grid_nmol", 5:220) %>% 
    mutate(Year = years,
           Month = months) %>% 
    dplyr::select(ID = Site.Id, 2:4, Year, Month, mean_FCH4P_grid_nmol)
   
   sd_nmol.val <- sd_nmol.val %>% 
     gather(key = "layer", value = "sd_FCH4P_grid_nmol", 5:220) %>% 
     mutate(Year = years,
            Month = months) %>% 
     dplyr::select(ID = Site.Id, Year, Month, sd_FCH4P_grid_nmol)
   
   nmol.val <- mean_nmol.val %>% left_join(sd_nmol.val)
   
write.csv(nmol.site, paste0(loc, "predictions/gridded/nmol/training-site-pred.csv"), row.names = F)
write.csv(nmol.val, paste0(loc, "predictions/gridded/nmol/val-site-pred.csv"), row.names = F)

```

##### Compare grid predictions with training and test (AmeriFlux validation) data

```{r}
site.pred <- read.csv(paste0(loc, "predictions/gridded/nmol/val-site-pred.csv"))

# select what we need
site.pred <- site.pred %>% 
  dplyr::select(ID, Year, Month, mean_FCH4P = mean_FCH4P_grid_nmol, sd_FCH4P = sd_FCH4P_grid_nmol) %>% 
  group_by(ID, Month) %>% 
  summarize(mean_FCH4P = mean(mean_FCH4P, na.rm=T),
            sd_FCH4P = mean(sd_FCH4P, na.rm=T)) 
```

###### PEQFR
```{r}
# read in site data
PEQFR <- read.csv(paste0(loc, "fluxnet-ch4-data/ameriflux-validation/csv/AMF_PE-QFR_BASE_HH_1-5.csv"), skip = 2) %>%
  mutate(ID = "PE-QFR",
         FCH4 = ifelse(FCH4 == -9999, NA, FCH4),
         Year = as.integer(substr(TIMESTAMP_END, 1, 4)),
         Month = as.integer(substr(TIMESTAMP_END, 5, 6)),
         Day = substr(TIMESTAMP_END, 7,8),
         Date = as_date(paste(Year,Month,Day)),
         DOY = yday(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week)) %>% 
  group_by(Month) %>% 
  summarize(FCH4 = mean(FCH4, na.rm = TRUE)) 

site.pred %>% 
  filter(ID == "PE-QFR") %>% 
  left_join(PEQFR) %>% 
  ggplot(aes(Month, FCH4)) + 
  geom_line(aes(y = FCH4), size = 0.5) +
  geom_point(shape = 21, fill = "white") +
  geom_line((aes(Month, mean_FCH4P))) +
  geom_ribbon(aes(ymin = mean_FCH4P - sd_FCH4P, ymax = mean_FCH4P + sd_FCH4P), alpha = 0.1) + # add predictions
  scale_color_manual(values = rep(c("dark blue", "dark red"), 4)) +
  scale_fill_manual(values = rep("grey70", 8)) +
  my_theme +
  labs(x= "Month", y = expression(CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')')) +
  scale_y_continuous(limits = c(0, 300)) +
  scale_x_continuous(breaks = c(1, 3, 5, 7, 9, 11))
ggsave(paste("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Predictions/Gridded/nmol_summary/figures/",today,"_PE-QFR.png", sep = ""),
       width = 10, height = 10, units = c("cm"), dpi = 300)
```

###### USALQ
```{r}
# read in site data
USALQ <- read.csv(paste0(loc, "fluxnet-ch4-data/ameriflux-validation/csv/AMF_US-ALQ_BASE_HH_5-5.csv"), skip = 2) %>%
  mutate(ID = "US-ALQ",
         FCH4 = ifelse(FCH4 == -9999, NA, FCH4),
         Year = as.integer(substr(TIMESTAMP_END, 1, 4)),
         Month = as.integer(substr(TIMESTAMP_END, 5, 6)),
         Day = substr(TIMESTAMP_END, 7,8),
         Date = as_date(paste(Year,Month,Day)),
         DOY = yday(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week)) %>% 
  group_by(Month) %>% 
  summarize(FCH4 = mean(FCH4, na.rm = TRUE)) 

site.pred %>% 
  filter(ID == "US-ALQ") %>% 
  left_join(USALQ) %>% 
  ggplot(aes(Month, FCH4)) + 
  geom_line(aes(y = FCH4), size = 0.5) +
  geom_point(shape = 21, fill = "white") +
  geom_line(aes(Month, mean_FCH4P)) +
  geom_ribbon(aes(ymin = mean_FCH4P - sd_FCH4P, ymax = mean_FCH4P + sd_FCH4P), alpha = 0.1) + # add predictions
  scale_color_manual(values = rep(c("dark blue", "dark red"), 4)) +
  scale_fill_manual(values = rep("grey70", 8)) +
  my_theme +
  labs(x= "Month", y = expression(CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')')) +
  scale_y_continuous(limits = c(0, 300)) +
  scale_x_continuous(breaks = c(1, 3, 5, 7, 9, 11))
ggsave(paste("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Predictions/Gridded/nmol_summary/figures/",today,"_US-ALQ.png", sep = ""),
       width = 10, height = 10, units = c("cm"), dpi = 300)
```

###### CACF2

```{r}
# read in site data
CACF2 <- read.csv(paste0(loc, "fluxnet-ch4-data/ameriflux-validation/csv/AMF_CA-CF2_BASE_HH_1-5.csv"), skip = 2) %>%
  mutate(ID = "CA-ARF",
         FCH4 = ifelse(FCH4 == -9999, NA, FCH4),
         Year = as.integer(substr(TIMESTAMP_END, 1, 4)),
         Month = as.integer(substr(TIMESTAMP_END, 5, 6)),
         Day = substr(TIMESTAMP_END, 7,8),
         Date = as_date(paste(Year,Month,Day)),
         DOY = yday(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week)) %>% 
  group_by(Month) %>% 
  summarize(FCH4 = mean(FCH4, na.rm = TRUE)) 

site.pred %>% 
  filter(ID == "CA-CF2") %>% 
  left_join(USALQ) %>% 
  ggplot(aes(Month, FCH4)) + 
  geom_line(aes(y = FCH4), size = 0.5) +
  geom_point(shape = 21, fill = "white") +
  geom_line(aes(Month, mean_FCH4P)) +
  geom_ribbon(aes(ymin = mean_FCH4P - sd_FCH4P, ymax = mean_FCH4P + sd_FCH4P), alpha = 0.1) + # add predictions
  scale_color_manual(values = rep(c("dark blue", "dark red"), 4)) +
  scale_fill_manual(values = rep("grey70", 8)) +
  my_theme +
  labs(x= "Month", y = expression(CH[4]*' Flux (nmol m'^{-2}*' s'^{-1}*')')) +
  scale_y_continuous(limits = c(0, 300)) +
  scale_x_continuous(breaks = c(1, 3, 5, 7, 9, 11))
ggsave(paste("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Predictions/Gridded/nmol_summary/figures/",today,"_US-ALQ.png", sep = ""),
       width = 10, height = 10, units = c("cm"), dpi = 300)
```


```{r}
joined_alq <- ensemble.site.pred %>% 
  filter(ID == "US-ALQ") %>% 
  left_join(USALQ)  %>% 
  filter(!Month == 1)

# US-ALQ
cor(joined_alq$mean_FCH4P, joined_alq$FCH4)^2
sum(abs(joined_alq$mean_FCH4P - joined_alq$FCH4))/(11*8)
sum(joined_alq$mean_FCH4P - joined_alq$FCH4)/(11*8)

joined_qfr <- ensemble.site.pred %>% 
  filter(ID == "PE-QFR") %>% 
  left_join(PEQFR) %>% 
  filter(!Month == 1)

# PE-QFR
cor(joined_qfr$mean_FCH4P, joined_qfr$FCH4)^2
sum(abs(joined_qfr$mean_FCH4P - joined_qfr$FCH4))/(11*8)
sum(joined_qfr$mean_FCH4P - joined_qfr$FCH4)/(11*8)

# split by even and odd
joined_qfr_odd <- ensemble.site.pred %>% 
  filter(ID == "PE-QFR") %>% 
  left_join(PEQFR) %>% 
  filter(!Month == 1 & member %in% c(1, 3, 5, 7))

cor(joined_qfr_odd$mean_FCH4P, joined_qfr_odd$FCH4)^2
sum(abs(joined_qfr_odd$mean_FCH4P - joined_qfr_odd$FCH4))/(11*8)
sum(joined_qfr_odd$mean_FCH4P - joined_qfr_odd$FCH4)/(11*8)

# split by even and odd
joined_qfr_even <- ensemble.site.pred %>% 
  filter(ID == "PE-QFR") %>% 
  left_join(PEQFR) %>% 
  filter(!Month == 1 & member %in% c(2, 4, 6, 8))

cor(joined_qfr_even$mean_FCH4P, joined_qfr_even$FCH4)^2
sum(abs(joined_qfr_even$mean_FCH4P - joined_qfr_even$FCH4))/(11*8)
sum(joined_qfr_even$mean_FCH4P - joined_qfr_even$FCH4)/(11*8)

```




```{r}
nmol <- stack(paste0(loc, "upscaling-output-raw/grids/upch4_v04_m1_nmolm2sec.nc"))
plot(nmol[[7]])

gc <- stack(paste0(loc, "upscaling-output-raw/grids/upch4_v04_m1_gCm2day.nc"))
plot(gc[[1:12]])

mgch4 <- stack(paste0(loc, "upscaling-output-raw/grids/upch4_v04_m1_mgCH4m2day_Aw.nc"))
plot(mgch4[[1:12]])

tgch4 <- nc_open(paste0(loc, "upscaling-output-raw/grids/upch4_v04_m1_TgCH4month_Aw.nc"))
tgch4_mean <- ncvar_get(tgch4, "mean_ch4")

lon <- ncvar_get(tgch4, "longitude")
lat <- ncvar_get(tgch4, "latitude", verbose = F)
t <- ncvar_get(tgch4, "Time")

fillvalue <- ncatt_get(tgch4, "mean_ch4", "_FillValue")

nc_close(tgch4) 

tgch4_sd[tgch4_mean == fillvalue$value] <- NA

tgch4_sd.slice <- tgch4_sd[, , 1] 

r <- list()
for(i in 1) {
  r[[i]] <- raster(t(tgch4_mean[, , 2]), xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
}

r_stack <- stack(r)

plot(r_stack[[1]])

plot(tgch4_sd[[1:12]])


sums <- cellStats(r_stack[[1]], sum)

plot(sums)
```

