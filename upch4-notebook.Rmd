---
title: "FLUXNET-CH4 Upscaling"
author: "Gavin McNicol"
date: "2/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages and ggplot theme:

```{r message = F}
rm(list=ls())
library(rlist)
library(tidyverse) 
library(lubridate)
library(raster)
library(oce) # for time series transformations
library(geosphere)
library(sf)
library(caret)
library(parallel)
source("code/ggplot_theme.R")
source("code/fit_all_pairs.R")
source("code/fit_stepwise.R")
```

**NOTE: This work flow uses many large files so most data is stored locally and requires hard filepaths**
The local file path used for all large files is: `/Volumes/Samsung_T5/Stanford/upch4_local/`.

Set the local head directory:

```{r}
loc <- "/Volumes/Samsung_T5/Stanford/upch4_local/"
```

#### Workflow

  1. Objective of Study
  2. Input Data
      + FLUXNET-CH4 Data
      + Gridded Data
  3. FLUXNET-CH4 Pre-Processing
      + FLUXNET-CH4 Data Preparation
          - Averaging
          - Variable Selection
  4. Gridded Data Pre-Processing
      + Grid Preparation
          - Potential Radiation (computed)
          - MODIS
      + Grid Extraction
          - Canopy Height
          - Computed (Rpot)
          - Compound Topographic Index
          - Earth Environment
          - N and S Deposition
          - SoilGrids
          - TerraClimate
          - Fractional Vegetation Cover (VCF)
          - Wetland Extent (WAD2M)
          - WorldClim 2.0
      + Merge Gridded Data
      + Gridded Data Quality Control
  5. Finalize Training Data
      + Merge Flux Data with Geospaital and MODIS Data
      + Subset Wetlands, Dates
      + Cluster Sites for Cross Validation
      + Finalize Data
          - Remove Extraneous
          - Impute Missing Data
          - Add Lagged Data
      + FLUXNET-CH4 Data Quality Control
      + Table of Final FLUXNET-CH4 Inputs
  6. Forward Feature Selection (FFS)
      + Filter Weekly Data
      + Feature Subset Experiments
      + FFS
          - First Pair
          - Additional Stepwise Features
          - FFS Evolution Plots
      + Summarize FFS Performance
  7. Cross Validation
      + ML Model Training
          - RF and Final Predictors or Subsets
          - XGB and Final Predictors or Subsets
          - ANN and Final Predictors or Subsets
          - RNN and Final Predictors or Subsets
      + Output Ensembles and Predictions
      + Validation 
          - Global Performance
          - Site-means
          - Monthly Seasonal Cycles
          - Monthly Anomalies
  8. Variable Importances
      + Variable Importance Rankings
      + Variable Responses
      + Partial Dependency Plots
      + ShapR
  9. Upscaled Model with Monte Carlo (MC)
      + Forcing Data
          - Mapping
          - Member Product Choices
          - Evaluate Product Divergence
      + Data Preparation
          - Extract Gridded Data for MC
          - Pre-Process FLUXNET-CH4 for MC
      + MC Simulations
      + MC ML Model Training
      + MC ML Model Validation
  10. Upscaling
      + Prepare Member Forcing Data
      + Run on Computing Cluster
          - Output Grids and Sums
      + Product Evaluation
          - Unweighted Wetland Fluxes (nmol)
          - Weighted Sums and Uncertainties (Tg)
          - Independent Validation
  11. Data Representativeness
      + Prepare Gridded Data
      + Global Dissimilarity
      + Tower Constituency
      + Extrapolation Errors
          - MC ML Model Training - Dissimilarity Only
          
[**Link to Workflow Figure**](https://drive.google.com/drive/folders/1jiFyqzoxxMpdtRLCwxCtzKpfILRNIW5K)
          
$~$            
      
#### 1. Objective of Study 

The goal of `FLUXNET-CH4 Upscaling` is to implement FLUXCOM-like ML approaches (e.g. [Jung et al. 2020](10.5194/bg-17-1343-2020)) to train a machine learning model using eddy covariance data that can predict wetland methane (CH4) fluxes globally. The predictions should be readily comparable to the Global Carbon Project (GCP) bottom-up process model ensembles that inform the Global Methane Budget ([Saunois et al. 2020](10.5194/essd-12-1561-2020)). Wetland fluxes specifically, rather than methane fluxes from all terrestrial ecosystems, are the predictive goal of this study because 1) most eddy covariance data available are in wetlands, with limited coverage across the multitude of upland ecosystems, 2) methane fluxes are highest and most variable in wetlands, and 3) comparable bottom-up process models predict wetland fluxes, then scale predictions to a global grid-cell using a prescribed (diagnostic runs) or model-derived (prognostic runs) wetland extent. In the last GCP Global Methane Budget ([Saunois et al. 2020](10.5194/essd-12-1561-2020)), diagnostic runs used the WAD2M product ([Zhang et al. in review](10.5194/essd-2020-262)). WAD2M includes coastal wetlands, however, we exclude coastal wetlands from the upscaling because they are salt-influenced and we do not have consistent salinity coverage, thus their inclusion is likely to bias flux estimates low even in non-coastal wetlands. The final model will be forced with available globally gridded data. Final product specifications are:

  - Monthly time-step
  - Historic reconstruction: ca. 2000 - 2018
  - 0.25-degree grid cell resolution (as is WAD2M)
  - Propagates training data uncertainties using Monte Carlo simulations
  - Considers sensitivity to global forcing data product choices

$~$    
  
#### 2. Input Data

**FLUXNET-CH4 Data**

The eddy covariance data used in this study are publicly available as part of the FLUXNET-CH4 community product V1.0 at [fluxnet.org](https://fluxnet.org/data/fluxnet-ch4-community-product/). The FLUXNET-CH4 synthesis activity is introduced in [Knox et al. 2019](10.1175/BAMS-D-18-0268.1) along with a detailed description of the eddy covariance post-processing steps including methane flux (FCH4) uncertainties. The first full (V1.0) dataset release (FLUXNET-CH4, hereafter) is described for 81 sites and is used in a wetland seasonality analysis in [Delwiche et al. in review](10.5194/essd-2020-307). **Data for this CH4 Upscaling Project were downloaded from [fluxnet.org](www.fluxnet.org) on Feb 22, 2021. [Download Manifest](https://docs.google.com/spreadsheets/d/1--_-XyBqsyiMIdc6JXqhilOOYFLeaY-UTMFhhI4Sd5M/edit#gid=0)**

Links:

  - [FLUXNET-CH4 Site (81) Metadata](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1384338468)
  - Permission was received via email on Feb 22, 2021, to use *Tier 2 Data Policy* sites in this study (SE-St1 and RU-Vrk; PI Thomas Friborg)
  - FLUXNET-CH4 data were used both for methane fluxes (FCH4; target variable) and tower-measured bio-meteorological variables (e.g., LE, GPP, TA; predictors)
      + Link to [FLUXNET.org variable descriptions](https://fluxnet.org/data/fluxnet-ch4-community-product/data-variables/)
  
$~$  
  
**Gridded Data (Predictors)**

A summary of predictors (`Predictor Summary`) is available [here](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=0), as well as an appendix table with all individual predictors (`Appendix: All Predictors`). Candidate predictors were drawn from a mix of source classes (EC tower measurements, global models, computations from observed data, or remote sensing (e.g. NASA MODIS)), different information content groups (Spatial-only or Spatio-temporal), and at different temporal frequencies (Static, Yearly, Monthly, Weekly, or Half-hourly). Using this information, we assigned each candidate predictor to a class (Generic, Climate, Biometeorological, Land Cover, Soil and Relief, or Greenness) to evaluate predictive power of particular classes, given the likelihood for redundancy in useful predictors across the full predictor set. For MODIS predictors, we computed the mean seasonal cycle (`_msc`), and yearly mean, min, max, and amplitude parameters, as in [Tramontana et al. (2016)](10.5194/bg-13-4291-2016) and [Jung et al. (2020)](10.5194/bg-17-1343-2020). For derived TerraClimate soil moisture and actual evapotranspiration (aet) we computed the interannual range and annual seasonality. More information on preprocessing of predictors is provided in the manuscript.

$~$

#### 3. FLUXNET-CH4 Pre-Processing

##### Local machine steps:

  + Create a copy of FLUXNET-CH4 data and name it `/fluxnet-ch4-data-original`
  + Unzip all site flux files in `/fluxnet-ch4-data` 
  + Reorganize into half-hourly (`/hh`) and `/daily` folders for easy access
  
##### Look at one site of **daily means** data.

```{r}
# setwd(loc)
# files <- list.files("fluxnet-ch4-data/daily/")
# one_site <- read.csv(paste(loc, "fluxnet-ch4-data/daily/", files[1], sep = ""))
# head(one_site)
```

No, but there is a quality flag `_QC`. `1`= data gap shorter than 2 months, `3` = gap exceeds 2 months. 

Issue: **There is also no uncertainty (`_UNC`) estimate on the downloaded daily mean data.**

##### Clean up workspace:

```{r}
# rm(one_site)
```

##### Look at one site of **half-hourly** data.

```{r}
setwd(loc)
files <- list.files("fluxnet-ch4-data/hh/")
one_site <- read.csv(paste(loc, "fluxnet-ch4-data/hh/", files[1], sep = ""))
head(one_site)
```

  - Half-hourly data has `FCH4` uncertainty columns `FCH4_F_RANDUNC` and `FCH4_F_ANNOPTLM_UNC`. Can be averaged over day or week.
  - Missing values are filled with `-9999`

##### Get FLUXNET-CH4 site names:

```{r}
site.names <- paste( substr(files, 5, 6), substr(files, 8, 10), sep = "")
```

##### Get all **half-hourly** flux data: (this will take a few minutes)

```{r echo = F, message = F, warning = F}
hh <- lapply(paste(loc, "fluxnet-ch4-data/hh/", files, sep = ""), read.csv)
names(hh) <- site.names
# str(hh)
```

##### Create a pristine replicate:

```{r}
hh2 <- hh
```

##### Look at all column names (including TIMESTAMP columns, to see date and time format):

```{r}
names(hh[[1]])
head(hh[[1]]$TIMESTAMP_END)
```

##### Write and apply function to expand `TIMESTAMP_END` into`Year`, `Month`, `Week`, `Day`, and `DOY` variables to facilitate merging with other data:

```{r}
expand_date <- function(hh_data) {
  hh_data <- hh_data %>% 
  mutate(Year = as.numeric(substr(TIMESTAMP_END, 1, 4)),
         Month = as.numeric(substr(TIMESTAMP_END, 5, 6)),
         Day = as.numeric(substr(TIMESTAMP_END, 7, 8)),
         Date = make_date(Year, Month, Day),
         DOY = yday(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week)) %>% 
    group_by(Year, DOY) %>% 
    mutate(HH = 1:n()) %>% 
  dplyr::select(Year, Month, Week, Day, HH, DOY, everything(), - Date, -TIMESTAMP_START, -TIMESTAMP_END)
}

hh <- lapply(hh, expand_date)
hh[[2]] # check it worked
```

##### Check if gap-filled FCH4 `CH4_F_ANNOPTLM` has already been pre-filled with observations `FCH4`, where available.

```{r}
hh[[1]] 
```

FCH4 in row 7 in `hh[[1]]` (ID BRNpw) = 10.71, which **matches** `CH4_F_ANNOPTLM` = 1.071e+01. 
**`CH4_F_ANNOPTLM` has been pre-filled with observations.**

##### Write and apply function to create `imputed`, a flag variable where:

  - `1` == `CH4_F_ANNOPTLM` was imputed
  - `2` == `CH4_F_ANNOPTLM` was observed

```{r}
create_imputed <- function(hh_data) {
  hh_data <- hh_data %>% 
    mutate(imputed = ifelse(FCH4 == -9999, 1, 0))
}

hh <- lapply(hh, create_imputed)
# hh[[1]] check it worked
```

##### Create ID column:

```{r}
for (i in 1:length(hh)){
  hh[[i]] <- hh[[i]] %>%
    mutate(ID = site.names[i]) %>%
    dplyr::select(ID, everything())
}
head(hh[[1]])
```

##### Convert missing values (`-9999`) to `NA`:

```{r}
for (i in 1:length(hh)){
  hh[[i]][hh[[i]] == -9999] <- NA
}
head(hh[[1]])
```






##### Get `u` and `v` wind components:

Identify which sites lack WD:

```{r}
find_wd <- function(hh_data) {
  sum(names(hh_data) == "WD") == 0
}

unlist(lapply(hh, find_wd))
sum(unname(unlist(lapply(hh, find_wd))))
```

OK, `CASCB` and `RUVrk` and missing `WD`.

Create dummy `WD` variables:

```{r}
hh$CASCB$WD <- NA
hh$RUVrk$WD <- NA

unlist(lapply(hh, find_wd))
sum(unname(unlist(lapply(hh, find_wd))))
```

Now compute wind direction components:

```{r}
compute_uv <- function(hh_data) {
  hh_data <- hh_data %>% 
    mutate(U = -WS_F * sin(2 * pi * WD/360),
           V = -WS_F * cos(2 * pi * WD/360))
}

hh <- lapply(hh, compute_uv)

head(hh[[1]]) # check it worked
```





##### Compute daily means of everything, including vector average WD and speed, excluding precip. (sums)

Create function to compute daily means:

```{r echo = F}
compute_daily1 <- function(hh_data) {
  hh_data <- hh_data %>% 
    group_by(ID, Year, DOY) %>% 
    summarize_all(list(~mean(., na.rm = T))) %>% 
    mutate(WD_mean = (atan2(U, V) * 360/2/pi) + 180,
           WS_mean = ((U^2 + V^2)^0.5)) %>% 
    dplyr::select(ID, Year, Month, Week, Day, DOY, everything(), -HH)
}
```

Check run time for one site:

```{r warning = F}
system.time( 
 compute_daily1(hh[[1]])
)
```

Apply to all sites using lapply:

```{r warning = F}
daily <- lapply(hh, compute_daily1)
```

Get sum of precip.

```{r warning = F, message = F}
sum_precip <- function(hh_data) {
  hh_data <- hh_data %>% 
    group_by(ID, Year, DOY) %>% 
    summarize(P_F_sum = sum(P_F))
}

daily_precip <- lapply(hh, sum_precip)
```

Rejoin precip. to main data frame:

```{r}
for (i in 1:length(daily)) {
daily[[i]] <- daily[[i]] %>% 
  left_join(daily_precip[[i]], by = c("ID", "Year", "DOY")) 
}
names(daily[[1]])
```

##### Output each "complete" daily site as a separate .csv file:

```{r}
for (i in 1:length(daily)) {
  daily[[i]] %>% write.csv(paste(loc, "fluxnet-ch4-data/daily_upch4/", site.names[i], "_daily_upch4.csv", sep = ""), 
                           row.names=FALSE)
}
```

#### 3. FLUXNET-CH4 Pre-Processing (cont.) 
##### (re)Load Daily Data

```{r message = F, warning = F, echo = F}
files <- list.files(paste(loc, "fluxnet-ch4-data/daily_upch4/", sep = ""))
daily <- lapply(paste(loc,"fluxnet-ch4-data/daily_upch4/",files, sep = ""), read.csv)
```

##### Bind rows

```{r}
daily_flat <- daily %>% bind_rows() %>% as_tibble()
head(daily_flat)
```

##### Compute FCH4 uncertainty, and subset gap-filled (and best) predictors

Notes on variable selection:

  + Total FCH4 uncertainty `FCH4_F_UNC` is random `FCH4_F_RANDUNC` and gap-filling `FCH4_F_ANNOPTLM_UNC` uncertainty summed in quadrature. I also drop the `FCH4_F_ANNOPTLM_QC` Quality control flag because I implement a stricter filtering criteria later of at least one observed flux per day.
  + The output of the daytime method `_DT` is used for gross primary production (GPP) and ecosystem respiration (RECO). Although both methods are subject to bias due to light-inhibition of leaf respiration, [Keenan et al. 2019](https://doi.org/10.1038/s41559-019-0809-2) show that the biases for the DT method only impact night-time respiration, but did not impact apparent photosynthesis or daytime respiration.
  + LE and NEE were gap-filled according to [Knox et al. 2019](10.1175/BAMS-D-18-0268.1) - same overall method to FCH4.
  + Other micro-meteorology (e.g., PPFD_IN) was gap-filled using ERA interim data (see [Delwiche et al.](10.5194/essd-2020-307)).
  + Only PPFD_IN (not PPFD_OUT) was selected as it can be approximated from SW_IN.
  + The shallowest available soil temperature is taken (`TS_1`)

```{r}
daily_subset <- daily_flat %>% 
  mutate(FCH4_F_UNC = sqrt(FCH4_F_ANNOPTLM_UNC^2 + FCH4_F_RANDUNC^2)) %>% 
  dplyr::select(ID, Year, Month, Week, Day, DOY,    # descriptive data
         FCH4 = FCH4_F_ANNOPTLM, FCH4_F_UNC, imputed,   # methane fluxes
         NEE = NEE_F_ANNOPTLM,  GPP = GPP_DT, RECO = RECO_DT,  # ecosystem C fluxes
         PPFD_IN = PPFD_IN_F, SW_IN = SW_IN_F, LW_IN = LW_IN_F, NETRAD = NETRAD_F,   # radiation
         LE = LE_F_ANNOPTLM, H = H_F,  # ecosystem energy fluxes
         TA = TA_F, PA = PA_F, RH = RH_F, VPD = VPD_F, P = P_F_sum, USTAR = USTAR, WS = WS_mean, # meteorology 
         TS = TS_1, SWC = SWC_F, WTD = WTD_F) # soil properties 
head(daily_subset)
```

##### Append site metadata

Pulled from [FLUXNET-CH4 Site Metadata](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1384338468)

```{r}
metadata <- read.csv(paste(loc, "fluxnet-ch4-data/metadata/fluxnet-ch4-site-metadata.csv", sep = "")) 
daily_subset_meta <- metadata %>% 
  mutate(ID = paste(substr(ID, 1, 2),substr(ID, 4, 6), sep="")) %>% 
  right_join(daily_subset, by = ("ID")) 
```
##### Save subset, flattened, and metadata-appended daily FLUXNET-CH4 data

```{r}
write.csv(daily_subset_meta, 
          paste(loc, "fluxnet-ch4-data/daily_flat/daily_subset_meta.csv", sep = ""),
          row.names = F)
```

#### 4. Gridded Data Pre-Processing

#### *Create Potential Radiation (RPot)*

**Description**
Get the mean seasonal cycle from Zutao Ouyang's 2001-2018 MATLAB output. Original citation for `Rpot` is [Peltola et al. 2019](10.5194/essd-11-1263-2019) where it was found to be a useful predictor of high latitude wetland FCH4.

Define a function to take a number of years and return an index of months:

```{r}
create_msc_index <- function(years) {
  msc_index <- list()
  for (i in 1:12){
      msc_index[[i]] <- seq(i, ((years-1)*12+i), by = 12)
  }
  msc_index
}
# create_msc_index(15) # test it out
```

Load in Rpot and create msc index for 19 years.

```{r}
Rpot <- brick(paste(loc, "grids/computed/Rpot.nc", sep = ""))
msc_index <- create_msc_index(19)
```

Get `_msc`:

```{r}
Rpot_msc <- list()
for (i in 1:12) {
  Rpot_msc[[i]] <- calc(Rpot[[msc_index[[i]]]], fun = mean)
}
```

Create raster stack:

```{r}
Rpot_msc <- stack(Rpot_msc)
```

Output `Rpot_msc.nc`:

```{r}
# writeRaster(Rpot_msc, paste(loc, "grids/computed/Rpot_msc.nc", sep = ""), format="CDF", overwrite = T)
```

#### *MODIS*

**Description**
Zutao Ouyang (Stanford University) extracted MODIS pixels at the FLUXNET-CH4 sites in April 2020, for 9 products:
 
  + Daytime Land Surface Temperature (`LSTD` from **MOD11A2**) 
      - *(not used as it is a correlate of nighttime temp., and nighttime is more applicable to soil conditions)*
  + Nighttime Land Surface Temperature (`LSTN` from **MOD11A2**)
  + Normalized Difference Vegetation Index (`NDVI` from **MOD09A1**)
  + Enhanced Vegetation Index (`EVI` from **MOD09A1**)
  + Leaf Area Index (`LAI` from **MCD15A2H**)
      - This is modeled, not directly measured.
  + Long Short Water Index (`LSWI`  from **MOD09A1**)
  + Simple Ratio Water Index (`SRWI` from **MOD09A1**)
  + Normalized Difference Water Index (`NDWI` from **MOD09A1**)
  + Normalized Difference Snow Index (`NDSI`  from **MOD11A2**)
  
**NOTE** Tables docmenting MODIS processing step effects on data are here  under [MODIS Processing](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1971246167) and QC figures are output to `upch4_local/modis/modis_qc`.


##### Get file local file names/paths for 8-day extracted MODIS data:

```{r}
setwd(paste(loc, "/modis/modis-extracted", sep = ""))
files <- list.files()
```

##### Set MODIS product/file names (for gather values):

```{r}
modis.names <- c("LSTD", "EVI", "LAI", "LSWI", "NDSI", "NDVI", "NDWI", "LSTN", "SRWI")
```

##### Read files:

```{r echo = F, warning = F, message = F}
modis <- lapply(paste(loc, "/modis/modis-extracted/", files, sep = ""), read.csv)
names(modis) <- modis.names
# str(modis)
```

##### Look at head for first file:

```{r}
head(modis[[1]])
```
Files are not tidy (short and wide). There are **86** extracted sites (extra sites than in FLUXNET-CH4 V1.0) but there should be **81**. 

##### Correct issues in Site IDs (columan names) to match `fluxnet-ch4-site-metadata.csv`:

```{r}
for(i in 1:length(modis)){
  modis[[i]] <- modis[[i]] %>% 
    dplyr::select(-`CA-DBB`, -`RU-SAM`, -`SE-Sto`, -`US-Bgl`, -`US-Brw`) %>% 
    rename(`JP-SwL` = `JP-Swl`, `PH-RiF` = `PH-RIf`, `US-ICs` = `US-Ics`, `US-MAC` = `MAERC`)
}
dim(modis[[1]])
```

##### Use gather to make long and narrow, remove hyphen from ID:

```{r}
for (i in 1:length(files)) {
  modis[[i]] <- modis[[i]] %>% 
    gather(key = ID, value = "modis.name", 2:82) %>% 
    mutate(ID = paste(substr(ID, 1, 2), substr(ID, 4, 6), sep = "")) %>% 
    as_tibble()
}
names(modis) <- modis.names
```

##### Check data are same length:

```{r}
str(modis)
```

1) `NDSI` is longer, 2) `LAI` is slightly shorter. Based on the dates, `NDSI` is daily.

Will not be able to bind columns with different lengths

##### Remove `LAI` and `NDSI` then rejoin:

```{r}
modis.1.names <- c("LSTD", "EVI", "LSWI", "NDVI", "NDWI", "LSTN", "SRWI")
modis.1 <- modis[modis.1.names]
```

##### Select only first two columns (Date and ID) then data columns:

```{r}
modis.1 <- modis.1 %>% 
  bind_cols() %>% 
  dplyr::select(1, 2, 3,6,9,12,15,18,21) %>% # subset only data columns
  as_tibble() 
names(modis.1) <- c("Date", "ID", modis.1.names)
```

##### Rejoin `LAI` using `Date`:

```{r}
modis.2 <- modis.1 %>% left_join(modis$LAI)
names(modis.2)[10] <- "LAI"
```

##### Rejoin `NDSI`:

```{r}
modis <- modis.2 %>% right_join(modis$NDSI)
names(modis)[11] <- "NDSI"
```

##### Convert `Date` into `Year` and `DOY`, and remove `LSTD`:

```{r}
modis <- modis %>% 
  mutate(Date = as.Date(Date, format = "%m/%d/%Y")) %>% 
  mutate(DOY = yday(Date),
         Date = as_date(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week),
         Week = as.factor(Week),
         Year = as.integer(substr(Date, 1,4 )),
         Month = as.numeric(substr(Date, 6, 7))) %>% 
  dplyr::select(ID, Date, Year, Month, Week, DOY, NDSI, NDVI, EVI, LAI, NDWI, SRWI, LSWI, LSTN) 
```

##### Count `NAs` before despiking:

```{r}
modis_nas <- modis %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)))) 
modis_nas
```
##### Despike outlier values for each product:

```{r}
modis_dsp <- modis %>% 
  mutate(NDSI_dsp = despike(NDSI, reference = 'trim', min = 0, max = 100, replace = "NA"),
         NDVI_dsp = despike(NDVI, reference = 'trim', min = -1, max = 1, replace = "NA"),
         EVI_dsp = despike(EVI, reference = 'trim', min = -1, max = 1, replace = "NA"),
         LAI_dsp = despike(LAI, reference = 'trim', min = 0, max = 5, replace = "NA"),
         NDWI_dsp = despike(NDWI, reference = 'trim', min = 0, max = 1, replace = "NA"),
         SRWI_dsp = despike(SRWI, reference = 'trim', min = -1, max = 3, replace = "NA"),
         LSWI_dsp = despike(LSWI, reference = 'trim', min = 0, max = 1, replace = "NA"),
         LSTN_dsp = despike(LSTN, reference = 'trim', min = -60, max = 50, replace = "NA")) %>% 
dplyr::select(ID, Date, Year, Month, Week, DOY, NDSI, NDVI, EVI, LAI, NDWI, SRWI, LSWI, LSTN,
              NDSI_dsp, NDVI_dsp, EVI_dsp, LAI_dsp, NDWI_dsp, SRWI_dsp, LSWI_dsp, LSTN_dsp) 
``` 

##### Count `NAs` after despiking:

```{r}
modis_dsp_nas <- modis_dsp %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)))) 
modis_dsp_nas
```
##### Calculate removed values during despiking and output to `upch4_local/modis/modis_qc/`

```{r}
modis_dsp_nas %>% 
  mutate(NDSI = NDSI - NDSI_dsp,
         NDVI = NDVI - NDVI_dsp,
         EVI = EVI - EVI_dsp,
         LAI = LAI - LAI_dsp,
         NDWI = NDWI - NDWI_dsp,
         SRWI = SRWI - SRWI_dsp,
         LSWI = LSWI - LSWI_dsp,
         LSTN = LSTN - LSTN_dsp) %>% 
  dplyr::select(-ID) %>% 
  summarize_all(list(~sum(.))) %>% 
  write.csv(paste(loc, "/modis/modis-qc/despiking_na_effects.csv", sep = ""))
```


##### Rename without `_dsp` suffix:

```{r}
modis_dsp <- modis_dsp %>% 
  dplyr::select(ID, Date, Year, Month, Week, DOY, 
                NDSI = NDSI_dsp, 
                NDVI = NDVI_dsp, 
                EVI = EVI_dsp,
                LAI = LAI_dsp,
                NDWI = NDWI_dsp,
                SRWI = SRWI_dsp,
                LSWI = LSWI_dsp,
                LSTN = LSTN_dsp)
```


##### Calculate mean  seasonal cycle (msc), then weekly means, then fill weekly gaps with msc:

```{r}
modis_gapfilled <- modis_dsp %>% 
  group_by(ID, Month) %>%  # this section computes  monthly values averaged over multiple years
  mutate(NDSI_msc = mean(NDSI, na.rm = TRUE),
         NDVI_msc = mean(NDVI, na.rm = TRUE),
         EVI_msc = mean(EVI, na.rm = TRUE), 
         LAI_msc = mean(LAI, na.rm = TRUE),
         NDWI_msc = mean(NDWI, na.rm = TRUE),
         SRWI_msc = mean(SRWI, na.rm = TRUE),
         LSWI_msc = mean(LSWI, na.rm = TRUE),
         LSTN_msc = mean(LSTN, na.rm =TRUE)) %>% 
  group_by(ID, Year, Month, Week) %>% # this section compute weekly means
  summarize(NDSI = mean(NDSI, na.rm = TRUE),
            NDVI = mean(NDVI, na.rm = TRUE),
            EVI = mean(EVI, na.rm = TRUE),
            LAI = mean(LAI, na.rm = TRUE),
            NDWI = mean(NDWI, na.rm = TRUE),
            SRWI = mean(SRWI, na.rm = TRUE),
            LSWI = mean(LSWI, na.rm = TRUE),
            LSTN = mean(LSTN, na.rm = TRUE),
            NDSI_msc = NDSI_msc[1],    # this section selects the monthly (msc) value corresponding to the weekly data
            NDVI_msc = NDVI_msc[1],
            EVI_msc = EVI_msc[1],
            LAI_msc = LAI_msc[1],
            NDWI_msc = NDWI_msc[1],
            SRWI_msc = SRWI_msc[1],
            LSWI_msc = LSWI_msc[1],
            LSTN_msc = LSTN_msc[1]) %>% 
  mutate(NDSI_F = ifelse(is.na(NDSI), NDSI_msc, NDSI), # this section fills any missing weekly values with the msc (monthly average)
         NDVI_F = ifelse(is.na(NDVI), NDVI_msc, NDVI),
         EVI_F = ifelse(is.na(EVI), EVI_msc, EVI),
         LAI_F = ifelse(is.na(LAI), LAI_msc, LAI),
         NDWI_F = ifelse(is.na(NDWI), NDWI_msc, NDWI),
         SRWI_F = ifelse(is.na(SRWI), SRWI_msc, SRWI),
         LSWI_F = ifelse(is.na(LSWI), LSWI_msc, LSWI),
         LSTN_F = ifelse(is.na(LSTN), LSTN_msc, LSTN)) 
```


##### If snow is on the ground, set water indices to the 5% quantile (frozen):

```{r}
modis_frozen <- modis_gapfilled %>% 
  group_by(ID,Year) %>% 
  mutate(NDWI_msc = ifelse(NDSI_msc > 0, quantile(NDWI_msc, 0.05, na.rm=TRUE), NDWI_msc),
         SRWI_msc = ifelse(NDSI_msc > 0, quantile(SRWI_msc, 0.05, na.rm=TRUE), SRWI_msc),
         LSWI_msc = ifelse(NDSI_msc > 0, quantile(LSWI_msc, 0.05, na.rm=TRUE), LSWI_msc),
         
         NDWI_F = ifelse(NDSI_F > 0, quantile(NDWI_F, 0.05, na.rm=TRUE), NDWI_F),
         SRWI_F = ifelse(NDSI_F > 0, quantile(SRWI_F, 0.05, na.rm=TRUE), SRWI_F),
         LSWI_F = ifelse(NDSI_F > 0, quantile(LSWI_F, 0.05, na.rm=TRUE), LSWI_F))
```

##### Compute mean, min, max, amplitude:

```{r message = F, warning = F}
modis_frozen <- modis_frozen %>% 
  group_by(ID, Year) %>% 
  mutate(NDSI_mean = mean(NDSI_F, na.rm=TRUE),
         NDVI_mean = mean(NDVI_F, na.rm=TRUE),
         EVI_mean = mean(EVI_F, na.rm=TRUE),
         LAI_mean = mean(LAI_F, na.rm=TRUE),
         NDWI_mean = mean(NDWI_F, na.rm=TRUE),
         SRWI_mean = mean(SRWI_F, na.rm=TRUE),
         LSWI_mean = mean(LSWI_F, na.rm=TRUE),
         LSTN_mean = mean(LSTN_F, na.rm=TRUE),
         
         NDSI_min = min(NDSI_F, na.rm=TRUE),
         NDVI_min = min(NDVI_F, na.rm=TRUE),
         EVI_min = min(EVI_F, na.rm=TRUE),
         LAI_min = min(LAI_F, na.rm=TRUE),
         NDWI_min = min(NDWI_F, na.rm=TRUE),
         SRWI_min = min(SRWI_F, na.rm=TRUE),
         LSWI_min = min(LSWI_F, na.rm=TRUE),
         LSTN_min = min(LSTN_F, na.rm=TRUE),
         
         NDSI_max = max(NDSI_F, na.rm=TRUE),
         NDVI_max = max(NDVI_F, na.rm=TRUE),
         EVI_max = max(EVI_F, na.rm=TRUE),
         LAI_max = max(LAI_F, na.rm=TRUE),
         NDWI_max = max(NDWI_F, na.rm=TRUE),
         SRWI_max = max(SRWI_F, na.rm=TRUE),
         LSWI_max = max(LSWI_F, na.rm=TRUE),
         LSTN_max = max(LSTN_F, na.rm=TRUE),
         
         NDSI_amp = NDSI_max-NDSI_min,
         NDVI_amp = NDVI_max-NDVI_min,
         EVI_amp = EVI_max-EVI_min,
         LAI_amp = LAI_max-LAI_min,
         NDWI_amp = NDWI_max-NDWI_min,
         SRWI_amp = SRWI_max-SRWI_min,
         LSWI_amp = LSWI_max-LSWI_min,
         LSTN_amp = LSTN_max-LSTN_min)
```

##### Get site IDs:

```{r}
site.names <- modis_frozen %>%
  ungroup() %>% 
  mutate(ID = factor(ID)) %>% 
  dplyr::select(ID) %>% 
  pull() %>% unique()
```

##### Get variable names:

```{r}
names <- names(modis)[7:14]
names_F <- paste(names, "_F", sep = "")
names_msc <- paste(names, "_msc", sep = "")
```

##### Create modis qcqa figures:

```{r message = F, warning = F}
for (i in 1:length(names)){
  
  # visualize 1-45
  modis_frozen %>%
    filter(ID %in% site.names[1:45]) %>%
    dplyr::select(Month, value_F = names_F[i], value_msc = names_msc[i]) %>% 
    ggplot(aes(Month, value_F)) +
    geom_point(size = 1, alpha = 0.3) +
    geom_line(aes(Month, value_msc), col = 'purple', size = 2) +
    facet_wrap(~ID, scales = 'free', ncol = 9) +
    my_theme
  ggsave(paste(loc, "/modis/modis-qc/", names[i], "_1.pdf", sep = ""),
         width = 50, height = 30, units = c("cm"), dpi = 300)
  
  # visualize 46-86
  modis_frozen %>%
    filter(ID %in% site.names[46:86]) %>%
    dplyr::select(Month, value_F = names_F[i], value_msc = names_msc[i]) %>% 
    ggplot(aes(Month, value_F)) +
    geom_point(size = 1, alpha = 0.3) +
    geom_line(aes(Month, value_msc), col = 'purple', size = 2) +
    facet_wrap(~ID, scales = 'free', ncol = 9) +
    my_theme
  ggsave(paste(loc, "/modis/modis-qc/", names[i], "_2.pdf", sep = ""),
         width = 50, height = 30, units = c("cm"), dpi = 300)
  
}
```

##### Output processed MODIS data:

```{r}
write.csv(modis_frozen, paste(loc, "/modis/modis-processed/modis-processed.csv", sep = ""),
          row.names = FALSE)
```

#### *Grid Extraction* 

**Extract geospatial data at FLUXNET-CH4 sites and output a single geospatial .csv.**

##### Load site metadata (ID, Latitude, Longitude), applicable to all extractions:

```{r}
sites <- read.csv(paste(loc, "fluxnet-ch4-data/metadata/fluxnet-ch4-site-metadata.csv", sep = "")) %>% 
  mutate(ID = paste(substr(ID, 1, 2), substr(ID, 4, 6), sep = ""))
site.coords <- cbind(sites$Longitude, sites$Latitude) # (x, y)
site.num <- length(site.coords[,1])
```

##### Global Canopy Height

```{r}
raster.names <- list.files(paste(loc, "grids/global-canopy-height/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/global-canopy-height/", raster.names, sep = ""))
canopyht <- as_tibble(raster::extract(stack, site.coords)) 
canopyht <- cbind(sites, canopyht) %>% 
  rename(canopyht = Simard_Pinto_3DGlobalVeg_JGR)
head(canopyht)
```

###### Check for missing data

```{r}
canopyht %>% 
  mutate(missing = is.na(canopyht)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(canopyht, paste(loc, "grids/extracted/", "canopyht.csv", sep = ""), row.names =  F)
```

##### Computed (rpot)

```{r}
stack <- stack(paste(loc, "grids/computed/Rpot_msc.nc", sep = ""))
rpot <- as_tibble(raster::extract(stack, site.coords)) 
rpot <- cbind(sites, rpot) %>% 
  gather(key = "Month", value = "rpot", 15:26) %>% 
  mutate(Month = str_remove(Month, "X"))
head(rpot)
```

###### Check for missing data

```{r}
rpot %>% 
  mutate(missing = is.na(rpot)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```
No missing data

###### Write data

```{r}
write.csv(rpot, paste(loc, "grids/extracted/", "rpot.csv", sep = ""), row.names =  F)
```

##### Compound Topographic Index

```{r}
raster.names <- list.files(paste(loc, "grids/geomorpho90m/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/geomorpho90m/", raster.names, sep = ""))
cti <- as_tibble(raster::extract(stack, site.coords)) 
cti <- cbind(sites, cti) %>% 
  rename(cti = dtm_cti_merit.dem_m_250m_s0cm_2018_v1.0)
head(cti)
```

No missing data. 

After QC below, noticed USHRA has a high CTI value. Check on map:

###### Check locations of USHRA and USHRC by cropping raster:

```{r}
# site.coords[sites$ID %in% c("DEDgw", "USDPW"),]
USHRA_bb <- c( "xmin" =  -91.751735 - 0.1, "xmax" = -91.751735 + 0.1, 
               "ymin" = 34.585208 - 0.1 , "ymax" =  34.585208 + 0.1)
USHRC_bb <- c( "xmin" =  -91.751658 - 0.1, "xmax" = -91.751658 + 0.1, 
               "ymin" = 34.588830 - 0.1 , "ymax" =  34.588830 + 0.1)
USHRA_bb <- crop(stack[[1]], USHRA_bb)
USHRC_bb <- crop(stack[[1]], USHRC_bb)
```

```{r}
plot(USHRA_bb)
points(site.coords)
```

Caused by HRA being apparently closer to a river/flow channel. No difference obvious in Google Earth images. Probably an issue with the CTI product.
Given USHRA will become an outlier, I'll correct it to match the lower USHRC value.

###### Correct USHRA to USHRC value

```{r}
cti[cti$ID == "USHRA", 15] <- cti[cti$ID == "USHRC", 15]
```


###### Check for missing data

```{r}
cti %>% 
  mutate(missing = is.na(cti)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))

```

No missing data

###### Write data

```{r}
write.csv(cti, paste(loc, "grids/extracted/", "cti.csv", sep = ""), row.names =  F)
```

##### Earth Environment Texture, Land Cover and Topography

Texture (14 variables)
Land Cover (2 variables)
Topography (10 variables)
See: [Appendix: All Predictors](https://docs.google.com/spreadsheets/d/1yBCIt5nFHVIb9v_0ExX8-4Svr0wlcmjD9vdp67hGtnQ/edit#gid=800847828). Search for `EarthEnv` under column `Gridded Product`.

###### First do texture:

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/texture/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/texture/", raster.names, sep = ""))
earthenv_texture <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_texture) <- c("cont","corr","cv","diss","entr","even","homo","max","rang","shan","simp","std","unif","var")
```

###### Then land cover (commented code for first-run creates `HD` - human development -  as the sum of LC7 and LC9):

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/cover/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/cover/", raster.names, sep = ""))

# # create `HD` (human development)
# HD <- stack[[1]] + stack[[2]]
# writeRaster(HD, paste(loc, "/grids/earthenv/cover/HD.tiff", overwrite = T))

earthenv_cover <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_cover) <- c("HD7", "HD9", "HD")
```

###### Then Topography:

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/topography/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/topography/", raster.names, sep = ""))
earthenv_topography <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_topography) <- c("east","elev","flat","hollow","north","pcurv","rough","slope","tcurv","tpi")
```

###### Now combine:

```{r}
earthenv <- cbind(sites, earthenv_texture, earthenv_cover, earthenv_topography)
```

###### Look at data histograms:

```{r}
earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  ggplot(aes(value)) + 
  geom_histogram() + 
  facet_wrap(~var, scales = 'free')
```

`cont`, `diss`, and `var` have extreme outliers (xmax = 10e+09)

###### Replace extreme outliers with median

```{r}
earthenv <- earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  group_by(var) %>% 
  mutate(value = ifelse(value > 10^9, median(value, na.rm = TRUE), value)) %>% 
  ungroup() %>% 
  spread(key = "var", value = "value")
```

Visualize again above to check they are corrected.

###### Also check for missing data:

```{r}
earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

###### There are two sites (USDPW and USWPT) with missing values. Plot the sites:

```{r}
plot(stack[[1]])
points(site.coords[sites$ID %in% c("USDPW", "USWPT"), ])
```

###### Fill with nearby site values. 

USDPW from USLA1

USWPT from USOWC

```{r}
earthenv[earthenv$ID == "USDPW", c(is.na(earthenv[earthenv$ID == "USDPW", ])) ] <- earthenv[earthenv$ID == "USLA1", c(is.na(earthenv[earthenv$ID == "USDPW", ])) ] 
earthenv[earthenv$ID == "USWPT", c(is.na(earthenv[earthenv$ID == "USWPT", ])) ] <- earthenv[earthenv$ID == "USOWC", c(is.na(earthenv[earthenv$ID == "USWPT", ])) ] 
```

###### Write data

```{r}
write.csv(earthenv, paste(loc, "grids/extracted/", "earthenv.csv", sep = ""), row.names =  F)
```

##### N and S Deposition

From [Lamarque et al. 2013](10.5194/acp-13-7997-2013):

  - accmip_nhx_acchist_2000.nc
  - accmip_noy_acchist_2000.nc
  - accmip_sox_acchist_2000.nc

```{r message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/ns-deposition/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/ns-deposition/", raster.names, sep = ""))
stack[[1]]
```

###### Update extent (which runs 0, 360, -90, 90)

```{r}
x1 <- crop(stack, extent(180, 360, -90, 90))
x2 <- crop(stack, extent(0, 180, -90, 90))   
extent(x1) <- c(-180, 0, -90, 90)
m <- merge(x1, x2)
names(m) <- names(stack)
```

###### Check overlap of points

```{r}
m[[1]] %>% na_if(-9999) %>% plot() 
```
###### Extract ns data and sum n variables for total n (`nt_depo`)

```{r}
ns_depo <- as_tibble(raster::extract(m, site.coords)) 
ns_depo <- cbind(sites, ns_depo) %>% 
  rename(nx_depo = Dry.deposition.NH3.NH4,
         ny_dep = Dry.deposition.NOy,
         s_dep = Dry.deposition.SO2.SO4) %>% 
  mutate(nt_depo = nx_depo + ny_dep)
head(ns_depo)
```
###### Check for missing data

```{r}
ns_depo %>% 
  gather(key = "var", value = "value", 15:17) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(ns_depo, paste(loc, "grids/extracted/", "ns_depo.csv", sep = ""), row.names =  F)
```

##### SoilGrids

6 products:

  - BLDFIE_M_sl2_250m_II.tif (bulk density)
  - OCDENS_M_sl2_250m_II.tif (organic carbon density)
  - PHIHOX_M_sl2_250m_II.tif (soil pH)
  - CLYPPT_M_sl2_250m_II.tif (proportion of clay particles)
  - AWCh3_M_sl2_250m_II.tif (soil water holding capacity)
  - HISTPR_250m_II.tif (histogram abundance)

```{r}
raster.names <- list.files(paste(loc, "grids/soilgrids/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/soilgrids/", raster.names, sep = ""))
# plot(stack[[3]])
soilgrids <- as_tibble(raster::extract(stack, site.coords)) 
soilgrids <- cbind(sites, soilgrids) %>% 
  rename(awc = AWCh3_M_sl2_250m_ll,
         bd = BLDFIE_M_sl2_250m_ll,
         clay = CLYPPT_M_sl2_250m_ll,
         hist = HISTPR_250m_ll,
         ocd = OCDENS_M_sl2_250m_ll,
         ph = PHIHOX_M_sl2_250m_ll)
head(soilgrids)
```

###### Check for missing soilgrids data:

```{r}
soilgrids %>% 
  gather(key = "var", value = "value", 15:20) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```
###### Check locations of DEDgw and USDPW by cropping raster:

```{r}
# site.coords[sites$ID %in% c("DEDgw", "USDPW"),]
DEDgw_bb <- c( "xmin" =  13.05427 - 0.1, "xmax" = 13.05427 + 0.1, 
               "ymin" = 53.15141 - 0.1 , "ymax" =  53.15141 + 0.1)
USDPW_bb <- c( "xmin" =  -81.43611 - 0.1, "xmax" = -81.43611 + 0.1,
               "ymin" = 28.05206 - 0.1 , "ymax" =  28.05206 + 0.1)
DEDgw_loc <- crop(stack[[1]], DEDgw_bb)
USDPW_loc <- crop(stack[[1]], USDPW_bb)
```

###### First check DEDgw

```{r}
plot(DEDgw_loc)
points(site.coords)
```

###### Then check USDPW

```{r}
plot(USDPW_loc)
points(site.coords)
```
By chance these two sites fall on NA pixels, by being close to water bodies.

###### Shift latitude by 0.005-degrees N

```{r}
sites.temp <- sites %>% 
  filter(ID %in% c("DEDgw","USDPW")) %>% 
  mutate(Latitude = Latitude + 0.005)
site.coords.temp <- sites.temp %>% 
  dplyr::select(Longitude, Latitude) %>% 
  unname() %>% as.matrix()
```

###### Extract only these two lat-adjusted sites

```{r}
soilgrids.temp <- as_tibble(raster::extract(stack, site.coords.temp)) 
soilgrids.temp <- cbind(sites.temp, soilgrids.temp) %>% 
  rename(awc = AWCh3_M_sl2_250m_ll,
         bd = BLDFIE_M_sl2_250m_ll,
         clay = CLYPPT_M_sl2_250m_ll,
         hist = HISTPR_250m_ll,
         ocd = OCDENS_M_sl2_250m_ll,
         ph = PHIHOX_M_sl2_250m_ll)
head(soilgrids.temp)
```

###### Rejoin soilgrids

```{r}
soilgrids <- soilgrids %>% 
  filter(!is.na(bd)) %>% 
  bind_rows(soilgrids.temp) %>%
  arrange(ID)
```

###### Write data

```{r}
write.csv(soilgrids, paste(loc, "grids/extracted/", "soilgrids.csv", sep = ""), row.names =  F)
```

##### TerraClimate

2 products:
  
  - Soil Moisture (soilwater and soilwaterR (range), soilwaterS (annual seasonality))
  - Actual Evapotranspiration (aet and aetR, aetS)
  
###### Read Actual Evapotranspiration grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/terraclimate/aet/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/terraclimate/aet/", raster.names, sep = ""))
plot(stack[[3]])
```
###### Extract Actual Evapotranspiration grids

```{r}
# extract data at sites
terraclimate.aet <- as_tibble(raster::extract(stack, site.coords))

# create new name vector
x <- c(as.character(c(1:216)))

# rename to retain order
terraclimate.aet <- terraclimate.aet %>% as_tibble() %>% 
  rename_all(~x)

# look at combined data
terraclimate.aet <- cbind(sites,terraclimate.aet)
head(terraclimate.aet)
```

###### Finalize Actual Evapotranspiration

```{r}
terraclimate.aet <- terraclimate.aet %>%
  gather(key = Month, value = aet, 15:length(terraclimate.aet)) %>% 
  mutate(Month = as.numeric(Month), 
         aet = as.numeric(aet)) %>% 
  arrange(ID, Month) %>% 
  mutate(Year = floor((Month - 0.1) / 12 + 2001)) %>% 
  group_by(ID, Year) %>% 
  mutate(Month = 1:n()) %>% 
  mutate(aetS = (aet - min(aet, na.rm = TRUE)) / (max(aet, na.rm = TRUE) - min(aet, na.rm = TRUE)),
         aetR = max(aet)-min(aet))
head(terraclimate.aet)
```

###### Read Soil Water grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/terraclimate/soil/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/terraclimate/soil/", raster.names, sep = ""))
plot(stack[[3]])
```

###### Extract Soil Water grids

```{r}
# extract data at sites
terraclimate.soilwater <- as_tibble(raster::extract(stack, site.coords))

# create new name vector
x <- c(as.character(c(1:216)))

# rename to retain order
terraclimate.soilwater <- terraclimate.soilwater %>% as_tibble() %>% 
  rename_all(~x)

# look at combined data
terraclimate.soilwater <- cbind(sites,terraclimate.soilwater)
head(terraclimate.soilwater)
```

###### Finalize soilwater data

```{r}
terraclimate.soilwater <- terraclimate.soilwater %>%
  gather(key = Month, value = soilwater, 15:length(terraclimate.soilwater)) %>% 
  mutate(Month = as.numeric(Month),
         soilwater = as.numeric(soilwater)) %>% 
  arrange(ID, Month) %>% 
  mutate(Year = floor((Month-0.1)/12+2001)) %>% 
  group_by(ID, Year) %>% 
  mutate(Month = 1:n()) %>% 
  mutate(soilwaterS = (soilwater-min(soilwater,na.rm=TRUE))/(max(soilwater,na.rm=TRUE)-min(soilwater,na.rm=TRUE)),
         soilwaterR = max(soilwater)-min(soilwater)) %>% 
  mutate(soilwaterS = ifelse(soilwaterR == 0 & is.na(soilwaterS), 0.5, soilwaterS))
head(terraclimate.soilwater)
```

###### Combined aet and soilwater

```{r}
# combine both aet and soilwater
terraclimate <- terraclimate.aet %>% 
  left_join(terraclimate.soilwater) %>% 
  dplyr::select(1:14, 17, 15, aet, aetS, aetR, soilwater, soilwaterS, soilwaterR)
head(terraclimate)
```

###### Check for missing data

```{r}
terraclimate %>% 
  gather(key = "var", value = "value", 15:20) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(terraclimate, paste(loc, "grids/extracted/", "terraclimate.csv", sep = ""), row.names =  F)
```

##### Fractional Vegetation Cover (VCF)

###### Read VCF grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/VCF5KYR/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/VCF5KYR/", raster.names, sep = ""))
plot(stack[[3]])
```

```{r}
# extract data at sites
vcf <- as_tibble(raster::extract(stack, site.coords))
```

###### Tiff names have year, however some years are missing. Create new name vector from years and data types:

```{r}
years_missing <- c(seq(1982,2016,1)) %>% as_tibble() %>% rename(Year = value) %>% 
  cbind(c(1:35)) %>% as_tibble() %>% rename(Row = `c(1:35)`) %>% 
  filter(Row %in% c(13, 19)) %>% dplyr::select(Year) %>% 
  pull() 

names_vcf <- c(seq(1982,2016,1)) %>% as_tibble() %>% rename(Year = value) %>% 
  cbind(c(1:35)) %>% as_tibble() %>% rename(Row = `c(1:35)`) %>% 
  filter(!Row %in% c(13, 19)) %>% dplyr::select(Year) %>% 
  pull() %>% rep(each = 3) %>% as.character()

suffix <- c("_tree","_shrub","_bare") %>% rep(33)

names_vcf <- paste(names_vcf, suffix, sep="")
```

###### Rename to retain order of data, then add missing years as means of adjacent years, and extent time series by two years to 2018 (assuming no change):

```{r}
# rename to retain order
vcf <- vcf %>% as_tibble() %>% 
  rename_all(~names_vcf)

# add in missing years
vcf <- vcf %>% mutate(row = 1:n()) %>% 
  group_by(row) %>% 
        mutate(`1994_bare` = mean(c(`1993_bare`,`1995_bare`)), 
               `1994_tree` = mean(c(`1993_tree`,`1995_tree`)), 
               `1994_shrub` = mean(c(`1993_shrub`,`1995_shrub`)), 
           
               `2000_bare` = mean(c(`1999_bare`,`2001_bare`)), 
               `2000_tree` = mean(c(`1999_tree`,`2001_tree`)), 
               `2000_shrub` = mean(c(`1999_shrub`,`2001_shrub`)),
               
               `2017_bare` = `2016_bare`,
               `2017_tree` = `2016_tree`,
               `2017_shrub` = `2016_shrub`,

               `2018_bare` = `2016_bare`,
               `2018_tree` = `2016_tree`,
               `2018_shrub` = `2016_shrub`) %>% ungroup() %>% 
  dplyr::select(-row)
```

###### look at combined data

```{r}
vcf <- cbind(sites,vcf)
head(vcf)
```
###### Finalize dataset: gather data split year and predictor

```{r}
vcf <- vcf %>%
  gather(key = Year, value = value, 15:length(vcf)) %>% 
  mutate(Predictor = substr(Year,6,10),
         Year = substr(Year,1,4)) %>% 
  spread(key = Predictor, value = value)
head(vcf)
```
###### Plot data

```{r}
vcf %>% 
  gather(key = "vcf_var", value = value, 16:18) %>% 
  ggplot(aes(value)) + 
  geom_histogram() +
  facet_wrap(~vcf_var) +
  my_theme
```
Data looks reasonable, all between 0 and 100.

###### Check that trends look reasonable:

```{r}
vcf %>% 
  gather(key = "vcf_var", value = value, 16:18) %>% 
  ggplot(aes(Year, value, color = vcf_var)) + 
  geom_point() +
  facet_wrap(~ID) +
  my_theme
```

US-AO3 looks weird (zeros)


###### Replace USA03 with USA10 data, due to strange behavior:

```{r}
vcf[vcf$ID == "USA03", 16:18] <- vcf[vcf$ID == "USA10", 16:18]
```

###### Write data

```{r}
write.csv(vcf, paste(loc, "grids/extracted/", "vcf.csv", sep = ""), row.names =  F)
```

##### Wetland Extent (WAD2M)

###### Get rasters

Use the coarser 0.5-degree map because 0.25-degree edge roughness causes more missing data

```{r}
setwd(loc)
raster.names <-list.files(paste(loc, "grids/WAD2M/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/WAD2M/", sep = "", raster.names[2]))
plot(stack[[7]])
points(site.coords)
```
###### Extract data at sites and look at it

```{r}
wf <- as_tibble(raster::extract(stack, site.coords))
wf <- cbind(sites,wf)
head(wf)
```

###### Finalize WF Seasonality:

  - Create date, month, and year columns from varnames
  - Replace any missing data with zero 
      + some sites are marine/rice paddy, or coastal  (false, need correction where freshwater wetland, see below)
      + frozen high latitude locations in winter are assigned NA (real, no correction)
  - Calculate seasonality

```{r}
wf <- wf %>%
  gather(key = Month, value = WF, 15:length(wf)) %>% 
  mutate(Month2 = Month,
         Date = substr(Month2,2,11),
         Year = substr(Month2,2,5),
         Month = as.factor(as.integer(substr(Month2,7,8)))) %>% 
  dplyr::select(-Month2) %>% 
  mutate(Date = as.Date(Date, format = "%Y.%m.%d"),
         DOY = yday(Date)) %>% 
  group_by(ID, Year) %>% 
  mutate(WF = replace_na(WF,0),
         WFS = (WF-min(WF,na.rm=TRUE))/(max(WF,na.rm=TRUE)-min(WF,na.rm=TRUE))) %>% 
  group_by(ID, Year) %>% 
  mutate(WFR = max(WF)-min(WF)) %>% 
  ungroup() %>% 
  mutate(WFS = ifelse(WFR == 0 & is.na(WFS), 0.5, WFS)) %>% 
  dplyr::select(1:14, Date, Year, Month, DOY, WF, WFS, WFR)
```

###### Look at data patterns

```{r}
wf %>% 
  gather(key = "wf_var", value = "value", 19) %>% 
  ggplot(aes(Date, value, color = wf_var)) +
  geom_point() +
  facet_wrap(~ID, scale = 'free')
```
There are missing data for: "DEHte", "HKMPM", "JPSwL", "KRCRK", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USHRA", "USHRC", "USNC4", "USNGB"
  
Of these, "HKMPM", "JPSwL", "KRCRK", "USHRA", "USHRC", are either lakes, rice, or mangroves (i.e. cannot easily fill missing data)

But these can be corrected: "DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNC4", "USNGB"

###### Get bounding boxes for each correctable site

```{r}
coords_subset <- site.coords[sites$ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNC4", "USNGB"),] 
coords_subset_names <- sites$ID[sites$Latitude %in% coords_subset]

bounding_box <- function(x) {
  c("xmin" =  x[1] - 3, "xmax" = x[1] + 3, 
               "ymin" = x[2] - 3 , "ymax" =  x[2] + 3)
}

bounding_boxes <- list()
for(i in 1:nrow(coords_subset)){
 bounding_boxes[[i]] <- bounding_box(coords_subset[i,])
}

bounding_boxes_maps <- list()
for(i in 1:length(bounding_boxes)){
  bounding_boxes_maps[[i]] <- crop(stack[[8]], bounding_boxes[[i]])
}
```

###### Match up longitudes with site names:

```{r}
sites %>% 
  filter(Longitude %in% coords_subset[,1])
```

###### Look at each site:

```{r}
for(i in 1:length(bounding_boxes)){
  plot(bounding_boxes_maps[[i]])
  points(coords_subset)
}
```

Looks like moving all 1:6 and 8 south by 1 degree, and moving 7 west by 1 degree will help with extractions:

###### Adjust lat/longs and reextract just these 8 sites

```{r}
site.coords.temp <- sites %>% 
  filter(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB", "USNC4")) %>% 
  mutate(Latitude = ifelse(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNGB"), Latitude - 0.35, Latitude),
         Longitude = ifelse(ID == "USNC4", Longitude - 0.25, Longitude)) %>% 
  dplyr::select(Longitude, Latitude) %>% 
  as.matrix() %>% unname()
```

###### Look just at the locations of these adjusted sites:

```{r}
for(i in 1:length(bounding_boxes)){
  plot(bounding_boxes_maps[[i]])
  points(site.coords.temp)
}
```

###### Re-extract at adjusted sites:

```{r}
sites.temp <- sites %>% 
  filter(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB", "USNC4"))
         
wf.temp <- as_tibble(raster::extract(stack, site.coords.temp))
wf.temp <- cbind(sites.temp, wf.temp)
head(wf.temp)
```

###### Finalize WF Seasonality for temporary sites

```{r}
wf.temp <- wf.temp %>%
  gather(key = Month, value = WF, 15:length(wf.temp)) %>% 
  mutate(Month2 = Month,
         Date = substr(Month2,2,11),
         Year = substr(Month2,2,5),
         Month = as.factor(as.integer(substr(Month2,7,8)))) %>% 
  dplyr::select(-Month2) %>% 
  mutate(Date = as.Date(Date, format = "%Y.%m.%d"),
         DOY = yday(Date)) %>% 
  group_by(ID, Year) %>% 
  mutate(WF = replace_na(WF,0),
         WFS = (WF-min(WF,na.rm=TRUE))/(max(WF,na.rm=TRUE)-min(WF,na.rm=TRUE))) %>% 
  group_by(ID, Year) %>% 
  mutate(WFR = max(WF)-min(WF)) %>% 
  ungroup() %>% 
  mutate(WFS = ifelse(WFR == 0 & is.na(WFS), 0.5, WFS)) %>% 
  dplyr::select(1:14, Date, Year, Month, DOY, WF, WFS, WFR)
```

###### Rejoin data

```{r}
wf.rm <- wf %>% 
  filter(!ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB", "USNC4"))

wf <- wf.rm %>% 
  bind_rows(wf.temp) %>% 
  arrange(ID, Date)
```

###### Look at data patterns

```{r}
wf %>% 
  gather(key = "wf_var", value = "value", 19) %>% 
  ggplot(aes(Date, value, color = wf_var)) +
  geom_point() +
  facet_wrap(~ID, scale = 'free')
```

Data are now complete (rice, mangrove, and lake sites still ~0 but cannot be corrected, USNC4 was a wetland and has been corrected)

###### Write data

```{r}
write.csv(wf, paste(loc, "grids/extracted/", "wf.csv", sep = ""), row.names =  F)
```

##### WorldClim 2.0

```{r warning = F}
setwd(loc)
raster.names <-list.files(paste(loc, "grids/worldclim/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/worldclim/", sep = "", raster.names))
plot(stack[[7]])
points(site.coords)
```

###### Extract worldclim data

```{r}
worldclim <- as_tibble(raster::extract(stack, site.coords)) 
worldclim <- cbind(sites, worldclim) 
head(worldclim)
```

###### Check for missing data

```{r}
worldclim %>% 
  filter(is.na(wc2.0_bio_30s_02))
```
HKMPM data are missing. 

###### Look at HKMPM location:

```{r}
coords_subset <- site.coords[sites$ID %in% c("HKMPM"),] 
coords_subset_names <- sites$ID[sites$Latitude %in% coords_subset]

bounding_boxes <- bounding_box(coords_subset)

bounding_boxes_maps <- crop(stack[[1]], bounding_boxes)

plot(bounding_boxes_maps)
points(114.02924,  22.49817)
```
Can move onto nearby coastline by shifting west

###### Shift HKMPM west:

```{r}
site.coords.temp <- sites %>% 
  filter(ID %in% c("HKMPM")) %>% 
  mutate(Longitude = Longitude + 0.05) %>% 
  dplyr::select(Longitude, Latitude) %>% 
  as.matrix() %>% unname()

sites.temp <- sites %>% 
  filter(ID %in% c("HKMPM")) 
         
worldclim.temp <- as_tibble(raster::extract(stack, site.coords.temp))
worldclim.temp <- cbind(sites.temp, worldclim.temp)
head(worldclim.temp)
```
###### Rejoin

```{r}
worldclim <- worldclim %>% 
  filter(!ID == "HKMPM") %>% 
  bind_rows(worldclim.temp) %>% 
  arrange(ID)
```

###### Create new name vector to match 19 products
```{r}
newnames <- c("wc_mat", "wc_dtr", "wc_iso", "wc_ts",
       "wc_mtqm", "wc_mtcm", "wc_tr", "wc_mtwtq",
       "wc_mtdq", "wc_mtwq", "wc_mtcq", "wc_map",
       "wc_pwtm", "wc_pdm", "wc_ps", "wc_pwtq",
       "wc_pdq", "wc_pwq", "wc_pcq")

oldnames <- names(worldclim)[15:33]
worldclim <- worldclim %>% 
 rename_at(vars(oldnames), ~ newnames)
```
###### Write worlclim data

```{r}
write.csv(worldclim, paste(loc, "grids/extracted/worldclim.csv", sep = ""), row.names = F)
```


#### *Grid Data Merge*

###### Read in all .csv files

```{r, warning = F}
setwd(loc)
csv.names <- list.files(paste(loc, "grids/extracted/", sep  = ""), pattern = "csv$", full.names = FALSE)
geospatial <- lapply(paste(loc, "grids/extracted/", csv.names, sep = ""), read.csv) 
names(geospatial) <- csv.names
```

###### Join static data

```{r, message = F, warning = F}

metadata_names <- names(geospatial$canopyht.csv)[2:14]

remove_metadata <- function(x) {
  x %>% 
    dplyr::select(-metadata_names)
}

canopyht <- remove_metadata(geospatial$canopyht.csv)
cti <- remove_metadata(geospatial$cti.csv)
worldclim <- remove_metadata(geospatial$worldclim.csv)
soilgrids <- remove_metadata(geospatial$soilgrids.csv)
earthenv <- remove_metadata(geospatial$earthenv.csv)

geospatial_data_static <- canopyht %>% 
  left_join(cti) %>% 
  left_join(worldclim) %>% 
  left_join(soilgrids) %>% 
  left_join(earthenv)

```
###### Join temporal data

```{r}
geospatial_data_temporal <- as_tibble(geospatial$wf.csv) %>% dplyr::select(-DOY, -Date) %>% # start with wetland fraction then add others
  left_join(geospatial$terraclimate.csv) %>% 
  left_join(geospatial$ns_depo.csv) %>% 
  left_join(geospatial$vcf.csv) %>% 
  left_join(geospatial$rpot) 
```
###### Merge static and temporal data

```{r}
geospatial_data <- geospatial_data_temporal %>% 
  full_join(geospatial_data_static)
```

###### Look at `geospatial_data` NAs

```{r}
nas <- geospatial_data %>% 
  group_by(ID, Latitude, Longitude) %>% 
  summarize_all(~sum(is.na(.))) %>% 
  dplyr::select(-Year, -Month) %>% 
  ungroup() %>% 
  gather(key = variable, value = NAs, 15:85) %>% 
  filter(NAs > 0)
nas
```

There is some missing data for 2000 for the Terraclimate temporal products.  Can impute at later step using caret.

###### Write out NA file

```{r}
write.csv(nas, paste(loc, "/grids/extracted/extracted_nas.csv", sep = ""), row.names = FALSE)
```

###### Write out `geospatial_data` file

```{r}
write.csv(geospatial_data, paste(loc, "/grids/extracted-final/geospatial.csv", sep = ""), row.names = FALSE)
```

#### *Merge geospatial and modis data*

```{r}
geospatial_data <- read.csv(paste(loc, "grids/extracted-final/geospatial.csv", sep = "")) %>% as_tibble()

modis_data <- read.csv(paste(loc, "modis/modis-processed/modis-processed.csv", sep = "")) %>% 
  dplyr::select(-NDSI, -NDVI, -EVI, -LAI, -NDWI, -SRWI, -LSWI, -LSTN) %>% 
  rename(NDSI = NDSI_F, NDVI = NDVI_F, EVI = EVI_F, LAI = LAI_F, NDWI = NDWI_F, SRWI = SRWI_F, LSWI = LSWI_F, LSTN = LSTN_F) %>% 
  as_tibble()

modis_geo <- modis_data %>% left_join(geospatial_data) %>% 
  dplyr::select(ID, Site.Name, Data.Policy, Country, Latitude, Longitude, IGBP, Site.Classification, Upland.Class, DOI,
                DOI.Reference, Site.Personnel, Base.File.Host, Upscaling, Year, Month, Week, everything()) 
```

###### Write out modis_geo data file

```{r}
write.csv(modis_geo, paste(loc, "training-data/modis_geospatial.csv", sep = ""), row.names = FALSE)
```

#### *QAQC Plots for all Predictors*

##### Read in `modis_geospatial.csv`:

```{r}
modis_geo <- read.csv(paste(loc, "training-data/modis_geospatial.csv", sep = ""))
```

##### Computed Predictors (rpot)

```{r}
modis_geo %>% 
  ggplot(aes(Month, rpot)) +
  geom_line() +
  facet_wrap(~ID) +
  labs(y = expression("Potential Radiation at the top of the Atmosphere (W m"^{-2}*")")) +
  scale_x_continuous(breaks = c(6, 12), labels = c(6, 12)) + 
  my_theme
ggsave(paste(loc, "grids/extracted-qc/rpot.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

First QAQC evaluation issues:

  + Missing data for CADBB; JPSwl; PHRlf; RUSAM; SESto; USBgl; USBrw; USIcs 
  + 3 of these are site mis-matches between modis and geospatial data (CADBB, RUSAM, SESto, USBgl, USBrw do not exist in `fluxnet-ch4-site-metadata.csv`)
  + The others are possibly mis-matches in spelling (JPSwL, PHRlF, USICs is the correct spelling, in `fluxnet-ch4-site-metadata.csv`)

Second QAQC evaluation:
  
  + No issues. Corrected modis input data site names.

##### WorldClim Predictors

###### Get Indices

```{r}
names(modis_geo)
```

Need 85-103.

###### Plot all worldclim

```{r}
oldnames <- names(modis_geo)[85:103]
newnames <- letters[1:19]

names_key <- setNames(oldnames, newnames)

modis_geo %>% 
  rename_at(vars(oldnames), ~ newnames) %>% 
  gather(key = "wc_var", value = "value", 85:103) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, value, color = group)) +
  geom_point() +
  facet_wrap(~wc_var, scales = "free", labeller = as_labeller(names_key)) +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

ggsave(paste(loc, "grids/extracted-qc/worldclim.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Ranges look reasonable. No outliers.

##### EarthEnv Predictors

Need 110-136

###### Plot earthenv

```{r}
oldnames <- names(modis_geo)[110:136]
newnames <- c(letters, "ab")

names_key <- setNames(oldnames, newnames)

modis_geo %>% 
  rename_at(vars(oldnames), ~ newnames) %>% 
  gather(key = "earthenv", value = "value", 110:136) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, value, color = group)) +
    geom_point() +
    facet_wrap(~earthenv, scales = 'free', labeller = as_labeller(names_key)) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/earthenv.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

##### Terraclimate Predictors

Need 69-74.

###### Plot the actual values

```{r}
# aet
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, aet, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID, scales = 'free') +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration (mm/month)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aet.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil water
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, soilwater, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture (mm/month)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwater.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

###### Plot the ranges

```{r}
# aet
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Year, aetR, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration Annual Range (mm)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aetR.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil water
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Year, soilwaterR, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture Annual Range (mm)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwaterR.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

###### Plot the seasonality

```{r}
# aetS
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, aetS, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration Annual Seasonality (unitless)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aetS.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil waterS
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, soilwaterS, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture Annual Seasonality (unitless)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwaterS.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

QC issues:

  + `aet` looks fine
  + `soilwater` seasonality is very variable within some sites, and obviously wrong in locations where wetlands are not the dominant component of the pixel
  (e.g., US-TW1 through US-Twt, which are delta marshes embedded in a dryland Mediterranean climate)

##### VCF Predictors 

Need 79-81

```{r}
modis_geo %>% 
  gather(key = "vcf_var", value = "value", 79:81) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, value, color = group)) +
  geom_point() +
  facet_wrap(~vcf_var, scales = "free") +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/vcf.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

###### Does each cover type (shrub, tree, bare) sum to 100?
```{r}
modis_geo %>% 
  mutate(vcf_sum = bare + shrub + tree) %>% 
    ggplot(aes(ID, vcf_sum)) +
    geom_point() +
    my_theme
ggsave(paste(loc, "grids/extracted-qc/vcf_var_sum.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

First QC passed: everything looks good, and cover types at each site sum to 100.


##### CTI Predictors 

```{r}
modis_geo %>% 
  dplyr::select(ID, cti) %>% 
  group_by(ID) %>% 
  summarize(ID = ID[1], cti = cti[1]) %>% 
  ggplot(aes(ID, cti, label = ID)) +
  geom_point() +
  geom_text(hjust = 0, nudge_x = 0.1) +
      my_theme +
  labs(y = "Compound Topographic Index (GeoMorpho90m") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/cti.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

First QAQC issues:

  - USHRA seems anomalously high. Go and check why in raster prep. 
      + After inspection, corrected USHRA to USHRC value, given proximity and no visible diff. in sites. (i.e. CTI product issue)
  - Other patterns seem to make sense, `CHDav` is in the alps and should have a low/negative CTI

##### Wetland Fraction (WF) Predictors 

66 (WF)
67 (WFS)
68 (WFR)

###### Plot the actual values

```{r}
# WF
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, WF, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID, scales = 'free') +
  my_theme +
  theme(axis.text.x = element_blank()) +
  labs(y = "Wetland Fraction (WAD2M)")
ggsave(paste(loc, "grids/extracted-qc/wad2m-wf.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

WF QC: 

  - HKMPM, JPSwL, KRCRK, USHRA, USHRC and USNC4 all show 0 wetland fraction year-round
      + Correct HKMPM and JPSwL to 1 year-round, because they are a mangrove and lake respectively (always flooded)
      + Map and shift the others coords to get a better value

###### Plot the ranges

```{r}
# WFR
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, WFR, color = group)) +
    geom_point(size = 0.5, angle = 90) +
  my_theme +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(y = "Wetland Fraction Range (WAD2M)")
ggsave(paste(loc, "grids/extracted-qc/wad2m-wfr.pdf", sep = ""),
       width = 40, height = 30, units = c("cm"), dpi = 300)
```

###### Plot the seasonality

```{r}
# WFS
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, WFS, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID, scales = 'free') +
  my_theme +
  theme(axis.text.x = element_blank()) +
  labs(y = "Wetland Fraction Seasonality (WAD2M)")
ggsave(paste(loc, "grids/extracted-qc/wad2m-wfs.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

##### canopyht Predictors

```{r}
modis_geo %>% 
  dplyr::select(ID, canopyht) %>% 
  group_by(ID) %>% 
  summarize(ID = ID[1], canopyht = canopyht[1]) %>% 
  ggplot(aes(ID, canopyht, label = ID)) +
  geom_point() +
  geom_text(hjust = 0, nudge_x = 0.1) +
      my_theme +
  labs(y = "Canpoy Height (m)") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/canopyht.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Looks OK!

##### NS Deposition Predictors

Get 75-78

```{r}
modis_geo %>% 
  gather(key = "ns_depo_var", value = "value", 75:78) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  group_by(ID, ns_depo_var) %>% 
  summarize(ID = ID[1], ns_depo_var = ns_depo_var[1], value = value[1], group = group[1]) %>% 
  ggplot(aes(ID, value, color = group, label = ID)) +
  geom_point() +
  geom_text(hjust = 0, nudge_x = 0.5, size = 2) +
  facet_wrap(~ns_depo_var, scales = "free") +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/ns_depo.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Looks fine. 

##### SoilGrids250m Predictors

Get 104-109.

```{r}
modis_geo %>% 
  gather(key = "soilgrids_var", value = "value", 104:109) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  group_by(ID, soilgrids_var) %>% 
  summarize(ID = ID[1], soilgrids_var = soilgrids_var[1], value = value[1], group = group[1]) %>% 
  ggplot(aes(ID, value, color = group, label = ID)) +
  geom_point() +
  geom_text(hjust = 0, nudge_x = 0.5, size = 2) +
  facet_wrap(~soilgrids_var, scales = "free") +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/soilgrids.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Looks fine!

#### 5. Finalize Training Data

##### Merge Flux and Geospatial Data

```{r}
fluxes <- read.csv(paste(loc, "fluxnet-ch4-data/daily_flat/daily_subset_meta.csv", sep = ""))
modis_geo <- read.csv(paste(loc, "training-data/modis_geospatial.csv", sep = ""))
names(fluxes)
names(modis_geo)
```

```{r}
modis_geo_subset <- modis_geo %>% 
  dplyr::select(ID, Year, Month, Week, 18:136)

flux_modis_geo_daily <- fluxes %>% 
  left_join(modis_geo_subset)
```

##### Get wetland flux sites only (45 sites):

```{r}
wetland_ids <- levels(as.factor(fluxes$ID))

flux_modis_geo_daily <- flux_modis_geo_daily %>% 
  filter(ID %in% wetland_ids)
```

##### Write merged dataset `flux_modis_geo_daily`:

```{r}
write.csv(flux_modis_geo_daily, paste(loc, "/training-data/flux_modis_geo_daily.csv", sep = ""), row.names = F)
names(flux_modis_geo_daily)
```

##### Output Flux QC Plots

```{r}
min(flux_modis_geo_daily$Year)
max(flux_modis_geo_daily$Year)
years <- c(2006:2018)

# scales fixed
for(i in 1:length(years)) {
  flux_modis_geo_daily %>% 
  filter(Year == years[i]) %>% 
    mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
  ggplot(aes(DOY, FCH4, color = factor(obs_threshold))) + 
    geom_point() +
    facet_wrap(~ID) +
    # scale_y_log10() +
    labs(y = expression("FCH4 (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste(loc, "fluxnet-ch4-data/flux-qc/", years[i], "_fixed.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
}

# scales free
for(i in 1:length(years)) {
  flux_modis_geo_daily %>% 
  filter(Year == years[i]) %>% 
    mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
  ggplot(aes(DOY, FCH4, color = factor(obs_threshold))) + 
    geom_point() +
    facet_wrap(~ID, scales = 'free') +
    # scale_y_log10() +
    labs(y = expression("FCH4 (nmol m"^{-2}*" s"^{-1}*")")) +
  my_theme
ggsave(paste(loc, "fluxnet-ch4-data/flux-qc/", years[i], "_free.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
}
```

Fluxes QC: 
  - Remove `USSne` Data for 2016 (wetland not yet restored) 

#### 5. (cont) Reload All Data (`flux_modis_geo_daily`)

```{r}
all_data <- read.csv(paste(loc, "training-data/flux_modis_geo_daily.csv", sep = "")) 
names(all_data)
```

##### Adjust Site Data: remove USSne year 2016

```{r}
all_data <- all_data %>%  
  mutate(remove = ifelse(ID == "USSne" & Year == 2016, 1, 0)) %>% 
  filter(remove == 0) %>% dplyr::select(-remove)
```

##### Look at data, and extent of missing data

```{r}
str(all_data)
```

##### Summarize missingness in predictors:

```{r}
all_data %>% 
   dplyr::select(-`Site.Name`, -`Data.Policy`, -Country, -IGBP, -`Site.Classification`, -DOI,  -`DOI.Reference`, -`Site.Personnel`, 
                -`Upland.Class`, -`Base.File.Host`, -Upscaling, -FCH4, -FCH4_F_UNC, -imputed) %>% 
  group_by(ID) %>% 
  summarize_all(list(~sum(is.na(.)) / n() )) %>% 
  write.csv(paste(loc, "training-data/missingness/predictor_missingness.csv", sep = ""), row.names = F)

```

##### Remove extraneous variables and non-class-double data and impute the first 40 predictors

Using predict function will not work with mixed-classes.
Most of the missing data are in the first EC tower and MODIS variables.


```{r}
all_data <- all_data %>% 
  dplyr::select(-PPFD_IN, -NETRAD, -H, -RH, -USTAR, -WS, -TS, -SWC, -WTD, -LAI, -LAI_msc, -LAI_mean, -LAI_min, -LAI_max, -LAI_amp)

all_data_impute <- all_data %>% 
  dplyr::select(-ID, -`Site.Name`, -`Data.Policy`, -Country, -IGBP, -`Site.Classification`, -DOI,  -`DOI.Reference`, -`Site.Personnel`, 
                -`Upland.Class`, -`Base.File.Host`, -Upscaling, -FCH4, -FCH4_F_UNC, -imputed) 

impute_names <- all_data_impute %>% names()


pp <- preProcess(all_data_impute[, 1:40], method = "bagImpute")
all_data_impute_pp <- predict(pp, all_data_impute[, 1:40])
rm(pp)
all_data[impute_names[1:40]] <- all_data_impute_pp
```

##### Summarize missingness after imputation in predictors:

```{r}
all_data %>% 
   dplyr::select(-`Site.Name`, -`Data.Policy`, -Country, -IGBP, -`Site.Classification`, -DOI,  -`DOI.Reference`, -`Site.Personnel`,
                -`Upland.Class`, -`Base.File.Host`, -Upscaling, -FCH4, -FCH4_F_UNC, -imputed) %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)) / n() )) %>% 
  write.csv(paste(loc, "training-data/missingness/predictor_missingness_after_impute.csv", sep = ""), row.names = F)
```

##### There are still some missing data, check where:

```{r}
all_data %>% 
  filter(is.na(wc_mtdq))
```
##### Fix by set Month 2, Week 8 equal to Month 2 Week 7:

```{r}
all_data[all_data$Year == 2016 & all_data$Month == 2 & all_data$Week == 8 , 57:145] <- all_data[all_data$Year == 2016 & all_data$Month == 2 & all_data$Week == 7 , 57:145]
```

##### Summarize missingness after manual imputation in predictors:

```{r}
all_data %>% 
   dplyr::select(-`Site.Name`, -`Data.Policy`, -Country, -IGBP, -`Site.Classification`, -DOI,  -`DOI.Reference`, -`Site.Personnel`,
                -`Upland.Class`, -`Base.File.Host`, -Upscaling, -FCH4, -FCH4_F_UNC, -imputed) %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)) / n() )) %>% 
  write.csv(paste(loc, "training-data/missingness/predictor_missingness_after_impute_after_manual.csv", sep = ""), row.names = F)
```

##### Add lagged variables

```{r}
all_data <- all_data %>% 
  group_by(ID) %>% 
  mutate(rpot_LAG30 = lag(rpot, 30, default = median(rpot[1:30], na.rm=TRUE)), ## generic monthly seasonality (30, 60, 90)
         rpot_LAG60 = lag(rpot, 60, default = median(rpot[1:60], na.rm=TRUE)), 
         rpot_LAG90 = lag(rpot, 90, default = median(rpot[1:90], na.rm=TRUE)), 
         rpot_LEAD30 = lead(rpot, 30, default = median(rpot[(length(rpot)-30):length(rpot)], na.rm=TRUE)), 
         rpot_LEAD60 = lead(rpot, 60, default = median(rpot[(length(rpot)-60):length(rpot)], na.rm=TRUE)), 
         rpot_LEAD90 = lead(rpot, 90, default = median(rpot[(length(rpot)-90):length(rpot)], na.rm=TRUE)), 
         
         WFS_LAG30 = lag(WFS, 30, default = median(WFS[1:30], na.rm=TRUE)), ## RS monthly seasonality (30, 60, 90)
         WFS_LAG60 = lag(WFS, 60, default = median(WFS[1:60], na.rm=TRUE)), 
         WFS_LAG90 = lag(WFS, 90, default = median(WFS[1:90], na.rm=TRUE)), 
         WFS_LEAD30 = lead(WFS, 30, default = median(WFS[(length(WFS)-30):length(WFS)], na.rm=TRUE)), 
         WFS_LEAD60 = lead(WFS, 60, default = median(WFS[(length(WFS)-60):length(WFS)], na.rm=TRUE)), 
         WFS_LEAD90 = lead(WFS, 90, default = median(WFS[(length(WFS)-90):length(WFS)], na.rm=TRUE)),
         
         LSTN_LAG7 = lag(LSTN, 7, default = median(LSTN[1:7], na.rm=TRUE)),  ## RS 7-day seasonality (7, 14, 21)
         LSTN_LAG14 = lag(LSTN, 14, default = median(LSTN[1:14], na.rm=TRUE)), 
         LSTN_LAG21 = lag(LSTN, 21, default = median(LSTN[1:21], na.rm=TRUE)), 
         LSTN_LEAD7 = lead(LSTN, 7, default = median(LSTN[(length(LSTN)-7):length(LSTN)], na.rm=TRUE)), 
         LSTN_LEAD14 = lead(LSTN, 14, default = median(LSTN[(length(LSTN)-14):length(LSTN)], na.rm=TRUE)), 
         LSTN_LEAD21 = lead(LSTN, 21, default = median(LSTN[(length(LSTN)-21):length(LSTN)], na.rm=TRUE)),
         
         NDSI_LAG7 = lag(NDSI, 7, default = median(NDSI[1:7], na.rm=TRUE)), 
         NDSI_LAG14 = lag(NDSI, 14, default = median(NDSI[1:14], na.rm=TRUE)), 
         NDSI_LAG21 = lag(NDSI, 21, default = median(NDSI[1:21], na.rm=TRUE)), 
         NDSI_LEAD7 = lead(NDSI, 7, default = median(NDSI[(length(NDSI)-7):length(NDSI)], na.rm=TRUE)), 
         NDSI_LEAD14 = lead(NDSI, 14, default = median(NDSI[(length(NDSI)-14):length(NDSI)], na.rm=TRUE)), 
         NDSI_LEAD21 = lead(NDSI, 21, default = median(NDSI[(length(NDSI)-21):length(NDSI)], na.rm=TRUE)),
         
         NDVI_LAG7 = lag(NDVI, 7, default = median(NDVI[1:7], na.rm=TRUE)), 
         NDVI_LAG14 = lag(NDVI, 14, default = median(NDVI[1:14], na.rm=TRUE)), 
         NDVI_LAG21 = lag(NDVI, 21, default = median(NDVI[1:21], na.rm=TRUE)), 
         NDVI_LEAD7 = lead(NDVI, 7, default = median(NDVI[(length(NDVI)-7):length(NDVI)], na.rm=TRUE)), 
         NDVI_LEAD14 = lead(NDVI, 14, default = median(NDVI[(length(NDVI)-14):length(NDVI)], na.rm=TRUE)), 
         NDVI_LEAD21 = lead(NDVI, 21, default = median(NDVI[(length(NDVI)-21):length(NDVI)], na.rm=TRUE)),
         
         EVI_LAG7 = lag(EVI, 7, default = median(EVI[1:7], na.rm=TRUE)), 
         EVI_LAG14 = lag(EVI, 14, default = median(EVI[1:14], na.rm=TRUE)), 
         EVI_LAG21 = lag(EVI, 21, default = median(EVI[1:21], na.rm=TRUE)), 
         EVI_LEAD7 = lead(EVI, 7, default = median(EVI[(length(EVI)-7):length(EVI)], na.rm=TRUE)), 
         EVI_LEAD14 = lead(EVI, 14, default = median(EVI[(length(EVI)-14):length(EVI)], na.rm=TRUE)), 
         EVI_LEAD21 = lead(EVI, 21, default = median(EVI[(length(EVI)-21):length(EVI)], na.rm=TRUE)),
         
         NDWI_LAG7 = lag(NDWI, 7, default = median(NDWI[1:7], na.rm=TRUE)), 
         NDWI_LAG14 = lag(NDWI, 14, default = median(NDWI[1:14], na.rm=TRUE)), 
         NDWI_LAG21 = lag(NDWI, 21, default = median(NDWI[1:21], na.rm=TRUE)), 
         NDWI_LEAD7 = lead(NDWI, 7, default = median(NDWI[(length(NDWI)-7):length(NDWI)], na.rm=TRUE)), 
         NDWI_LEAD14 = lead(NDWI, 14, default = median(NDWI[(length(NDWI)-14):length(NDWI)], na.rm=TRUE)), 
         NDWI_LEAD21 = lead(NDWI, 21, default = median(NDWI[(length(NDWI)-21):length(NDWI)], na.rm=TRUE)),
         
         SRWI_LAG7 = lag(SRWI, 7, default = median(SRWI[1:7], na.rm=TRUE)), 
         SRWI_LAG14 = lag(SRWI, 14, default = median(SRWI[1:14], na.rm=TRUE)), 
         SRWI_LAG21 = lag(SRWI, 21, default = median(SRWI[1:21], na.rm=TRUE)), 
         SRWI_LEAD7 = lead(SRWI, 7, default = median(SRWI[(length(SRWI)-7):length(SRWI)], na.rm=TRUE)), 
         SRWI_LEAD14 = lead(SRWI, 14, default = median(SRWI[(length(SRWI)-14):length(SRWI)], na.rm=TRUE)), 
         SRWI_LEAD21 = lead(SRWI, 21, default = median(SRWI[(length(SRWI)-21):length(SRWI)], na.rm=TRUE)),
         
         LSWI_LAG7 = lag(LSWI, 7, default = median(LSWI[1:7], na.rm=TRUE)), 
         LSWI_LAG14 = lag(LSWI, 14, default = median(LSWI[1:14], na.rm=TRUE)), 
         LSWI_LAG21 = lag(LSWI, 21, default = median(LSWI[1:21], na.rm=TRUE)), 
         LSWI_LEAD7 = lead(LSWI, 7, default = median(LSWI[(length(LSWI)-7):length(LSWI)], na.rm=TRUE)), 
         LSWI_LEAD14 = lead(LSWI, 14, default = median(LSWI[(length(LSWI)-14):length(LSWI)], na.rm=TRUE)), 
         LSWI_LEAD21 = lead(LSWI, 21, default = median(LSWI[(length(LSWI)-21):length(LSWI)], na.rm=TRUE)),
         
         
         soilwater_LAG30 = lag(soilwater, 30, default = median(soilwater[1:30], na.rm=TRUE)),  ## MET monthly seasonality (30, 60, 90)
         soilwater_LAG60 = lag(soilwater, 60, default = median(soilwater[1:60], na.rm=TRUE)), 
         soilwater_LAG90 = lag(soilwater, 90, default = median(soilwater[1:90], na.rm=TRUE)), 
         soilwater_LEAD30 = lead(soilwater, 30, default = median(soilwater[(length(soilwater)-30):length(soilwater)], na.rm=TRUE)), 
         soilwater_LEAD60 = lead(soilwater, 60, default = median(soilwater[(length(soilwater)-60):length(soilwater)], na.rm=TRUE)), 
         soilwater_LEAD90 = lead(soilwater, 90, default = median(soilwater[(length(soilwater)-90):length(soilwater)], na.rm=TRUE)),
         
         soilwaterS_LAG30 = lag(soilwaterS, 30, default = median(soilwaterS[1:30], na.rm=TRUE)),  
         soilwaterS_LAG60 = lag(soilwaterS, 60, default = median(soilwaterS[1:60], na.rm=TRUE)), 
         soilwaterS_LAG90 = lag(soilwaterS, 90, default = median(soilwaterS[1:90], na.rm=TRUE)), 
         soilwaterS_LEAD30 = lead(soilwaterS, 30, default = median(soilwaterS[(length(soilwaterS)-30):length(soilwaterS)], na.rm=TRUE)), 
         soilwaterS_LEAD60 = lead(soilwaterS, 60, default = median(soilwaterS[(length(soilwaterS)-60):length(soilwaterS)], na.rm=TRUE)), 
         soilwaterS_LEAD90 = lead(soilwaterS, 90, default = median(soilwaterS[(length(soilwaterS)-90):length(soilwaterS)], na.rm=TRUE)),
         
         aet_LAG30 = lag(aet, 30, default = median(aet[1:30], na.rm=TRUE)), 
         aet_LAG60 = lag(aet, 60, default = median(aet[1:60], na.rm=TRUE)), 
         aet_LAG90 = lag(aet, 90, default = median(aet[1:90], na.rm=TRUE)), 
         aet_LEAD30 = lead(aet, 30, default = median(aet[(length(aet)-30):length(aet)], na.rm=TRUE)), 
         aet_LEAD60 = lead(aet, 60, default = median(aet[(length(aet)-60):length(aet)], na.rm=TRUE)), 
         aet_LEAD90 = lead(aet, 90, default = median(aet[(length(aet)-90):length(aet)]), na.rm=TRUE),
         
         aetS_LAG30 = lag(aetS, 30, default = median(aetS[1:30], na.rm=TRUE)),  
         aetS_LAG60 = lag(aetS, 60, default = median(aetS[1:60], na.rm=TRUE)), 
         aetS_LAG90 = lag(aetS, 90, default = median(aetS[1:90], na.rm=TRUE)), 
         aetS_LEAD30 = lead(aetS, 30, default = median(aetS[(length(aetS)-30):length(aetS)], na.rm=TRUE)), 
         aetS_LEAD60 = lead(aetS, 60, default = median(aetS[(length(aetS)-60):length(aetS)], na.rm=TRUE)), 
         aetS_LEAD90 = lead(aetS, 90, default = median(aetS[(length(aetS)-90):length(aetS)], na.rm=TRUE)),
         
         
         TA_LAG1 = lag(TA, 1, default = median(TA[1:1], na.rm=TRUE)),  ## MET daily seasonality (1, 3, 7)
         TA_LAG3 = lag(TA, 3, default = median(TA[1:3], na.rm=TRUE)), 
         TA_LAG7 = lag(TA, 7, default = median(TA[1:7], na.rm=TRUE)), 
         TA_LEAD1 = lead(TA, 1, default = median(TA[(length(TA)-1):length(TA)], na.rm=TRUE)), 
         TA_LEAD3 = lead(TA, 3, default = median(TA[(length(TA)-3):length(TA)], na.rm=TRUE)), 
         TA_LEAD7 = lead(TA, 7, default = median(TA[(length(TA)-7):length(TA)], na.rm=TRUE)), 
         
         SW_IN_LAG1 = lag(SW_IN, 1, default = median(SW_IN[1:1], na.rm=TRUE)),  
         SW_IN_LAG3 = lag(SW_IN, 3, default = median(SW_IN[1:3], na.rm=TRUE)),  
         SW_IN_LAG7 = lag(SW_IN, 7, default = median(SW_IN[1:7], na.rm=TRUE)), 
         SW_IN_LEAD1 = lead(SW_IN, 1, default = median(SW_IN[(length(SW_IN)-1):length(SW_IN)], na.rm=TRUE)), 
         SW_IN_LEAD1 = lead(SW_IN, 3, default = median(SW_IN[(length(SW_IN)-3):length(SW_IN)], na.rm=TRUE)), 
         SW_IN_LEAD7 = lead(SW_IN, 7, default = median(SW_IN[(length(SW_IN)-7):length(SW_IN)], na.rm=TRUE)), 
         
         LW_IN_LAG1 = lag(LW_IN, 1, default = median(LW_IN[1:1], na.rm=TRUE)), 
         LW_IN_LAG3 = lag(LW_IN, 3, default = median(LW_IN[1:3], na.rm=TRUE)), 
         LW_IN_LAG7 = lag(LW_IN, 7, default = median(LW_IN[1:7], na.rm=TRUE)), 
         LW_IN_LEAD1 = lead(LW_IN, 1, default = median(LW_IN[(length(LW_IN)-1):length(LW_IN)], na.rm=TRUE)), 
         LW_IN_LEAD3 = lead(LW_IN, 3, default = median(LW_IN[(length(LW_IN)-3):length(LW_IN)], na.rm=TRUE)), 
         LW_IN_LEAD7 = lead(LW_IN, 7, default = median(LW_IN[(length(LW_IN)-7):length(LW_IN)], na.rm=TRUE)), 
         
         PA_LAG1 = lag(PA, 1, default = median(PA[1:1], na.rm=TRUE)),  
         PA_LAG3 = lag(PA, 3, default = median(PA[1:3], na.rm=TRUE)), 
         PA_LAG7 = lag(PA, 7, default = median(PA[1:7], na.rm=TRUE)), 
         PA_LEAD1 = lead(PA, 1, default = median(PA[(length(PA)-1):length(PA)], na.rm=TRUE)),
         PA_LEAD3 = lead(PA, 3, default = median(PA[(length(PA)-3):length(PA)], na.rm=TRUE)),
         PA_LEAD7 = lead(PA, 7, default = median(PA[(length(PA)-7):length(PA)], na.rm=TRUE)), 
         
         VPD_LAG1 = lag(VPD, 1, default = median(VPD[1:1], na.rm=TRUE)), 
         VPD_LAG3 = lag(VPD, 3, default = median(VPD[1:3], na.rm=TRUE)), 
         VPD_LAG7 = lag(VPD, 7, default = median(VPD[1:7], na.rm=TRUE)), 
         VPD_LEAD1 = lead(VPD, 1, default = median(VPD[(length(VPD)-1):length(VPD)], na.rm=TRUE)),
         VPD_LEAD3 = lead(VPD, 3, default = median(VPD[(length(VPD)-3):length(VPD)], na.rm=TRUE)),
         VPD_LEAD7 = lead(VPD, 7, default = median(VPD[(length(VPD)-7):length(VPD)], na.rm=TRUE)), 
         
         P_LAG1 = lag(P, 1, default = median(P[1:1], na.rm=TRUE)),  
         P_LAG3 = lag(P, 3, default = median(P[1:3], na.rm=TRUE)),  
         P_LAG7 = lag(P, 7, default = median(P[1:7], na.rm=TRUE)), 
         P_LEAD1 = lead(P, 1, default = median(P[(length(P)-1):length(P)], na.rm=TRUE)), 
         P_LEAD3 = lead(P, 3, default = median(P[(length(P)-3):length(P)], na.rm=TRUE)),
         P_LEAD7 = lead(P, 7, default = median(P[(length(P)-7):length(P)], na.rm=TRUE)), 
         
         
         NEE_LAG1 = lag(NEE, 1, default = median(NEE[1:1], na.rm=TRUE)),  ## FLUX daily seasonality (1, 3, 7)
         NEE_LAG3 = lag(NEE, 3, default = median(NEE[1:3], na.rm=TRUE)),
         NEE_LAG7 = lag(NEE, 7, default = median(NEE[1:7], na.rm=TRUE)), 
         NEE_LEAD1 = lead(NEE, 1, default = median(NEE[(length(NEE)-1):length(NEE)], na.rm=TRUE)), 
         NEE_LEAD3 = lead(NEE, 3, default = median(NEE[(length(NEE)-3):length(NEE)], na.rm=TRUE)), 
         NEE_LEAD7 = lead(NEE, 7, default = median(NEE[(length(NEE)-7):length(NEE)], na.rm=TRUE)), 
         
         GPP_LAG1 = lag(GPP, 1, default = median(GPP[1:1], na.rm=TRUE)),  
         GPP_LAG3 = lag(GPP, 3, default = median(GPP[1:3], na.rm=TRUE)),
         GPP_LAG7 = lag(GPP, 7, default = median(GPP[1:7], na.rm=TRUE)), 
         GPP_LEAD1 = lead(GPP, 1, default = median(GPP[(length(GPP)-1):length(GPP)], na.rm=TRUE)), 
         GPP_LEAD3 = lead(GPP, 3, default = median(GPP[(length(GPP)-3):length(GPP)], na.rm=TRUE)), 
         GPP_LEAD7 = lead(GPP, 7, default = median(GPP[(length(GPP)-7):length(GPP)], na.rm=TRUE)), 
         
         RECO_LAG1 = lag(RECO, 1, default = median(RECO[1:1], na.rm=TRUE)),  
         RECO_LAG3 = lag(RECO, 3, default = median(RECO[1:3], na.rm=TRUE)),  
         RECO_LAG7 = lag(RECO, 7, default = median(RECO[1:7], na.rm=TRUE)), 
         RECO_LEAD1 = lead(RECO, 1, default = median(RECO[(length(RECO)-1):length(RECO)], na.rm=TRUE)), 
         RECO_LEAD3 = lead(RECO, 3, default = median(RECO[(length(RECO)-3):length(RECO)], na.rm=TRUE)),
         RECO_LEAD7 = lead(RECO, 7, default = median(RECO[(length(RECO)-7):length(RECO)], na.rm=TRUE)), 
         
         LE_LAG1 = lag(LE, 1, default = median(LE[1:1], na.rm=TRUE)), 
         LE_LAG3 = lag(LE, 3, default = median(LE[1:3], na.rm=TRUE)),
         LE_LAG7 = lag(LE, 7, default = median(LE[1:7], na.rm=TRUE)), 
         LE_LEAD1 = lead(LE, 1, default = median(LE[(length(LE)-1):length(LE)], na.rm=TRUE)), 
         LE_LEAD3 = lead(LE, 3, default = median(LE[(length(LE)-3):length(LE)], na.rm=TRUE)), 
         LE_LEAD7 = lead(LE, 7, default = median(LE[(length(LE)-7):length(LE)], na.rm=TRUE))) %>% 
  ungroup()
```

##### Look at first 32 predictors (after imputation)

```{r}
names(all_data_impute)[8:17]
```

```{r}
# NEE
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, NEE, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Net Ecosystem Exchange ("*mu*"mol m"^{-2}*" s"^{-1}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/NEE.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# GPP
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, GPP, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Gross Primary Production ("*mu*"mol m"^{-2}*" s"^{-1}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/GPP.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# RECO
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, RECO, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Ecosystem Respiration ("*mu*"mol m"^{-2}*" s"^{-1}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/RECO.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# SW_IN
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, SW_IN, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Incoming Shortwave Radiation (W m"^{-2}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/SW_IN.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# LW_IN
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, LW_IN, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Incoming Longwave Radiation (W m"^{-2}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/LW_IN.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# LE
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, LE, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Latent Heat Flux (W m"^{-2}*")")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/LE.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# TA
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, TA, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Air Temperature ("*degree*"C)")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/TA.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# PA
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, PA, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Air Pressure (kPa)")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/PA.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# VPD
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, VPD, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Vapor Pressure Deficit (hPa)")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/VPD.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

# P
all_data %>% 
  mutate(obs_threshold = ifelse(imputed < 1, "yes", "no")) %>% 
    ggplot(aes(Week, P, color = obs_threshold, alpha = obs_threshold)) +
    geom_point() +
    facet_wrap(~ID, scales = 'free') + 
    labs(y = expression("Daily Precipitation (mm)")) +
    my_theme +
    scale_color_manual(values = c("grey", "black")) + scale_alpha_manual(values = c(0.3, 1))
ggsave(paste(loc, "/training-data/imputation-qc/P.png", sep = ""),
               width = 40, height = 30, units = c("cm"), dpi = 300)

```

##### Output: Write complete `all_data`

```{r}
write.csv(all_data, paste(loc, "/training-data/all_data.csv", sep = ""), row.names = F)
```

##### Output: Table of Final FLUXNET-CH4 Inputs: 

  + **Manually edit to create new file `list_of_predictors_metadata.csv` with class and information content variables

```{r}
names(all_data)[23:ncol(all_data)] %>% write.csv(paste(loc, "/training-data/list_of_predictors.csv", sep = ""))
```

#### 6. Forward Feature Selection

##### Read in data and remove upland class

```{r}
all_data <- read.csv(paste(loc, "/training-data/all_data.csv", sep = "")) %>% 
  dplyr::select(-`Upland.Class`)
```

##### Subset only days with at least one observation

```{r}
all_data <- all_data %>% 
  filter(imputed < 1) %>% 
  dplyr::select(-imputed)
dim(all_data)
```

##### Remove other stray NAs using complete cases

```{r}
all_data <- all_data %>% 
  filter(complete.cases(.))
dim(all_data)
```

##### Setup LOSOCV (cluster sites based on location)

```{r}
site_loc <- all_data %>% 
  group_by(ID) %>% 
  summarize(Latitude = Latitude[1],
            Longitude = Longitude[1]) 

x <- site_loc$Longitude
y <- site_loc$Latitude
xy <- SpatialPointsDataFrame(matrix(c(x, y), ncol = 2), 
                             data.frame(ID = site_loc$ID),
                             proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84"))

# calculate Euclidian distance 
site_dist <- xy %>% distm()

# cluster
site_clusters <- hclust(as.dist(site_dist), method = 'complete')

# define 300 km threshold
d <- 300000
xy$Cluster <- cutree(site_clusters, h=d) 
xy <- xy %>% as_tibble() %>% 
  dplyr::select(ID, Cluster)

# rejoin folds
all_data <- all_data %>% 
  left_join(xy, by = c("ID")) %>%
  dplyr::select(Cluster = Cluster, everything())
```

##### Summarize clusters now that data have been filtered

```{r}
# summarize folds and sites
nclusters <- max(all_data$Cluster)
nsites <- length(levels(factor(all_data$ID)))

# check data count per site and cluster
cluster_bysite <- all_data %>% 
  mutate(index = 1, 
         total = length(index),
         Class = `Site.Classification`) %>% 
  group_by(ID, Cluster, Latitude) %>% 
  summarize(Longitude = Longitude[1],
            Class = Class[1],
            count = sum(index),
            percent = count/total[1]*100) 

# cluster names
cluster_bycluster <- cluster_bysite %>% ungroup() %>% 
  mutate(total = sum(count)) %>% 
  group_by(Cluster) %>% 
  summarize(Class = Class[1],
            Latitude = mean(Latitude),
            Longitude = mean(Longitude),
            cluster_name = ID[1],
            count = sum(count),
            percent = count/total[1]*100) %>% 
  mutate(Band = ifelse(Latitude < 30, "Tropics", NA),
         Band = ifelse(Latitude < 60 & Latitude > 30, "Temperate", Band),
         Band = ifelse(Latitude > 60, "Boreal", Band)) 

# output cluster stats
write.csv(cluster_bysite, paste(loc, "/training-data/clusters/cluster_bysite.csv", sep = ""), row.names = F)
write.csv(cluster_bycluster, paste(loc, "/training-data/clusters/cluster_bycluster.csv", sep = ""), row.names = F)
```

##### Output training data (with clusters)

```{r}
write.csv(all_data, paste(loc, "training-data/final.csv", sep  = ""), row.names = F)
```


##### Rank predictors using FFS (ALL)

#### 6. (cont.) Read in final data and predictor metadata

```{r}
all_data <- read.csv(paste(loc, "training-data/final.csv", sep = ""))
pred.metadata <- read.csv(paste(loc, "/training-data/list_of_predictors_metadata.csv", sep = ""))
```

###### Select features

```{r}
feat <- pred.metadata$Predictor
feat_l <- length(feat)
data_l <- length(all_data$Cluster)
```

###### Create pairwise feat combinations for FFS

```{r}
feat_pairs_full <- combn(feat, 2, simplify = FALSE)
length(feat_pairs_full)
```

###### Remove pairs of static predictors (unlikely to be informative)

```{r}
static.predictors <- pred.metadata %>% 
  filter(Content == "Spatial") %>% 
  dplyr::select(Predictor, Content)

feat_pairs_n <- length(feat_pairs_full)

feat_pairs_remove <- bind_cols(feat_pairs_full) %>% 
  gather(key = "pair", value = "Predictor") %>% 
  mutate(pair = str_remove(pair, "...")) %>% 
  left_join(static.predictors) %>% 
  filter(!is.na(Content)) %>% 
  group_by(pair) %>% 
  summarize(n_static = n()) %>% 
  filter(n_static == 2) %>% 
  dplyr::select(pair) %>% pull() %>% as.numeric()

feat_pairs <- feat_pairs_full[-feat_pairs_remove]

length(feat_pairs) 
```

###### Save `feat_pairs` to view

```{r}
bind_cols(feat_pairs) %>% 
  gather(key = Pair, value = Predictor) %>% 
  mutate(Pair = str_remove(Pair,'...')) %>% 
  write.csv(paste(loc, "training-data/feat/feat_pairs.csv", sep = ""), row.names = FALSE)
```

#### 6. (cont.) FFS of first predictor pair

##### Get fold number and fold names

```{r}
set.seed(23)

# data inner folds 
folds <- all_data %>% mutate(Cluster = as.factor(Cluster)) %>% dplyr::select(Cluster) %>% pull() %>% levels() %>% length()
fold_names <- all_data %>% group_by(Cluster) %>% summarize(Name = ID[1]) 
```

##### Setup ML folds

```{r}
train_feat <- list()
train_label <- list()
validate_data <- list()

for (i in 1:folds) {
  train_label[[i]] <- all_data %>% filter(!Cluster == i) %>% dplyr::select(FCH4) %>% pull()
  train_feat[[i]] <- all_data %>% filter(!Cluster == i) %>% dplyr::select(feat)
  validate_data[[i]] <- all_data %>% filter(Cluster == i) %>% dplyr::select(Cluster, FCH4, feat)
}

folds_index <- list() # need list within list (each inner list has all fold site except LOSO fold)
for (i in 1:folds) {
  x <- list()
  folds_index[[i]] <- x
}

for (i in 1:folds) {
  for (j in 1:folds) {
    folds_index[[i]][[j]] <- all_data %>% 
      filter(!Cluster == i) %>% 
      mutate(index = 1:n()) %>% 
      filter(!Cluster == j) %>% 
      dplyr::select(index) %>% 
      pull()
  }
  folds_index[[i]] <- folds_index[[i]][-i]
}
length(folds_index) == length(train_label) & length(folds_index) == length(train_feat)  & length(folds_index) == length(validate_data)
```

##### Run FFS First Pair in Parallel

```{r}
set_index <- c(2001:4000)

numCores <- detectCores()

rf_pred <- mclapply(feat_pairs[set_index], fit_all_pairs, mc.cores = numCores)

for(i in 1:2000){
  rf_pred[[i]] <- rf_pred[[i]] %>% 
    mutate(feat_pair = set_index[i])
}

for(i in 1:2000){
  write.csv(rf_pred[[i]], paste(loc, "ffs/first-pair/rf-pred/rf_pred_", set_index[i], ".csv", sep = ""), row.names = F)
}
```

##### Compute FFS First Pair metrics  

May need to read in `rf_pred` csv files again.

```{r}
rf.metrics <- list()

files <- list.files(paste(loc, "ffs/first-pair/rf-pred/", sep = ""))
rf_pred <- lapply(read.csv, files)

compute_metrics <- function(rf_pred) {
    rf_pred %>% 
    dplyr::select(FCH4, FCH4P) %>% 
    summarize(samples = n(),
              PMARE = 100/n() * sum(abs(FCH4 - FCH4P) / abs(FCH4)),
              R2 = cor(FCH4P, FCH4)^2,
              NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
              MAE = sum(abs(FCH4 - FCH4P))/n(),
              nMAE = MAE/sd(FCH4),
              MeanO = mean(FCH4),
              MedO = median(FCH4),
              sdO = sd(FCH4),
              MeanP = mean(FCH4P),
              MedP = median(FCH4P),
              sdP = sd(FCH4P),
              nSD = sdP/sdO,
              Bias = mean(FCH4P - FCH4),
              cBias = abs(Bias)/sum(abs(Bias))*100,
              predictors = list(feat_pairs[[i]]) )
}

rf.metrics <- lapply(rf_pred, compute_metrics)

rf.metrics.all <- bind_rows(rf.metrics) %>% 
  arrange(MAE) %>% 
  rowwise() %>% 
  mutate(pred1 = predictors[1],
         pred2 = predictors[2]) %>% 
  dplyr::select(-predictors)

write.csv(rf.metrics.all, paste(loc, "/ffs/first-pair/rf-metrics/rf.metrics.all.csv", sep = ""), row.names = F)

```

##### Look at metrics distribution

```{r}
rf.metrics.all %>% 
  ggplot(aes(R2)) +
  geom_histogram() + my_theme
ggsave(paste(loc, "ffs/first-pair/rf-metrics/R2_distribution.png", sep = ""),
       width = 8, height = 15, units = c("cm"), dpi = 300) 

rf.metrics.all %>% 
  ggplot(aes(MAE)) +
  geom_histogram() + my_theme
ggsave(paste(loc, "ffs/first-pair/rf-metrics/MAE_distribution.png", sep = ""),
       width = 8, height = 15, units = c("cm"), dpi = 300) 
```

##### get the min MAE and amx R2 (are they the same?)

```{r}
max(rf.metrics.all$R2)
min(rf.metrics.all$MAE)
rf.metrics.all
```

Yes both are for `canopyht` and `TA`.

##### look at top 10 Predictor Pairs

```{r}
rf.metrics.all %>% 
  arrange(MAE) %>% 
  head(10) %>% 
  dplyr::select(pred1, pred2)
```
##### Stepwise FFS (for first 19 steps, to avoid local minima)

```{r}
feat <- pred.metadata$Predictor
feat_original <- feat
data_l <- length(all_data$Cluster)


feat_best <- c("TA", "canopyht")
feat_best_index <- c(which(feat == "TA"), 
                     which(feat == "canopyht"))

feat <- feat[- c(feat_best_index) ]
feat_l <- length(feat)
```

##### Setup ML folds

```{r}
train_feat <- list()
train_label <- list()
validate_data <- list()

for (i in 1:folds) {
  train_label[[i]] <- all_data %>% filter(!Cluster == i) %>% dplyr::select(FCH4) %>% pull()
  train_feat[[i]] <- all_data %>% filter(!Cluster == i) %>% dplyr::select(feat_original)
  validate_data[[i]] <- all_data %>% filter(Cluster == i) %>% dplyr::select(Cluster, FCH4, feat_original)
}

folds_index <- list() # need list within list (each inner list has all fold site except LOSO fold)
for (i in 1:folds) {
  x <- list()
  folds_index[[i]] <- x
}

for (i in 1:folds) {
  for (j in 1:folds) {
    folds_index[[i]][[j]] <- all_data %>% 
      filter(!Cluster == i) %>% 
      mutate(index = 1:n()) %>% 
      filter(!Cluster == j) %>% 
      dplyr::select(index) %>% 
      pull()
  }
  folds_index[[i]] <- folds_index[[i]][-i]
}
length(folds_index) == length(train_label) & length(folds_index) == length(train_feat)  & length(folds_index) == length(validate_data)
```

##### Update compute metrics function (remove predictor list variable of length 2)

```{r}
compute_metrics <- function(rf_pred) {
    rf_pred %>% 
    dplyr::select(FCH4, FCH4P) %>% 
    summarize(samples = n(),
              PMARE = 100/n() * sum(abs(FCH4 - FCH4P) / abs(FCH4)),
              R2 = cor(FCH4P, FCH4)^2,
              NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
              MAE = sum(abs(FCH4 - FCH4P))/n(),
              nMAE = MAE/sd(FCH4),
              MeanO = mean(FCH4),
              MedO = median(FCH4),
              sdO = sd(FCH4),
              MeanP = mean(FCH4P),
              MedP = median(FCH4P),
              sdP = sd(FCH4P),
              nSD = sdP/sdO,
              Bias = mean(FCH4P - FCH4),
              cBias = abs(Bias)/sum(abs(Bias))*100) 
}
```

##### Train multiple RFs (with CV) for FFS to select the next 10 best predictors

```{r}

feat_initial <- 2
feat_best_row <- list()
rf.metrics.all <- list()


for(j in 1:10){
  
# create index for a list of features
feat_l <- length(feat)
set_index <- c(1:feat_l)
feat_list <- as.list(feat)

numCores <- detectCores()

# call fit a stepwise function to train RFs using one additional predictor
rf_pred <- mclapply(feat_list[set_index], fit_stepwise, mc.cores = numCores)


for(i in 1:feat_l){
  rf_pred[[i]] <- rf_pred[[i]] %>% 
    mutate(feat_single = feat[i])
}

for(i in 1:feat_l){
  write.csv(rf_pred[[i]], paste(loc, "ffs/stepwise/rf-pred/rf_pred_", j, "thstep_", set_index[i], "_", feat[i],  ".csv", sep = ""), row.names = F)
}

# call compute_metrics function
rf.metrics <- lapply(rf_pred, compute_metrics)

# bind rows and append feature column, then arrange by MAE
rf.metrics.all[[j]] <- bind_rows(rf.metrics) %>% 
  cbind(feat) %>% 
  arrange(MAE) 

# save the row of the best performing new predictor
feat_best_row[[j]] <- rf.metrics.all %>% 
    filter(MAE == min(MAE))
  
# get the specific predictor name
feat_best_add <- feat_best_row[[j]] %>% 
    dplyr::select(feat) %>% 
    pull() %>% as.character()
  
# add to initial best_feat
feat_best <- c(feat_best, feat_best_add)
  
# add one to feat_initial size
feat_initial <- feat_initial + 1
  
# remove feat_best from feat
feat_best_add_index <- which(feat == feat_best_add)
feat <- feat[- feat_best_add_index ]
  
print(paste('added', feat_best_add))

}

```


```{r}
feat_best_row_all <- bind_rows(feat_best_row)

write.csv(feat_best,
          "/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/FFS/FWET_ALL/200828_ffs_additional_predictors_k1_k10.csv",
          row.names = FALSE)

write.csv(feat_best_row_all, 
          "/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/FFS/FWET_ALL/200828_ffs_additional_metrics_k1_k10.csv",
          row.names = FALSE)


write.csv(rf_metrics_all,
          "/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/FFS/FWET_ALL/200828_ffs_k10_rf_metrics_all.csv",
          row.names = FALSE)

```



# outer loop lists
feat_best_row <- list()

# initialize feature 
feat_initial <- 6

for (k in 8:10) {

  rf_metrics <- list()
  
  for(j in 1:length(feat)) {
    # for(j in 1:length(feat_pairs)) {
    # set up RF lists
    tgrid <- list()
    myControl <- list()
    rf_model <- list()
    rf_varimp <- list()
    
    for (i in 1:folds) {
      
      num.trees.rf <- ifelse(feat_initial < 10, 100, 150)
      
      ## Create tune-grid
      tgrid <- expand.grid(
        mtry = c(2),
        splitrule = "variance", 
        min.node.size = c(50)
      )
      
      ## Create trainControl object
      myControl <- trainControl(
        method = "none",
        classProbs = FALSE,
        allowParallel = TRUE
      )
      
      ## train rf on folds
      rf_model[[i]] <- train(
        x = train_feat[[i]][,c(feat_best,feat[j])], 
        y = train_label[[i]],
        num.trees = num.trees.rf, 
        method = 'ranger',
        trControl = myControl,
        tuneGrid = tgrid,
        metric = "MAE"
      )
      print(i)
    }
    
    # get all hold out predictions
    rf.pred <- list()
    for (i in 1:folds) {
      rf.pred[[i]] <- validate_data[[i]] %>% 
        filter(Cluster == i) %>%   
        mutate(FCH4P = predict(rf_model[[i]], .),
               index = 1:n())
    }
    rf.pred.all <- bind_rows(rf.pred)
    
    rf_metrics[[j]] <- ungroup(rf.pred.all) %>% 
      summarize(samples = n(),
                R2 = cor(FCH4P, FCH4)^2,
                NSE = 1 - sum((FCH4 - FCH4P)^2) / sum((FCH4 - mean(FCH4))^2),
                MAE = sum(abs(FCH4 - FCH4P))/n(),
                nMAE = MAE/sd(FCH4),
                MeanO = mean(FCH4),
                MedO = median(FCH4),
                sdO = sd(FCH4),
                MeanP = mean(FCH4P),
                MedP = median(FCH4P),
                sdP = sd(FCH4P),
                nSD = sdP/sdO,
                Bias = mean(FCH4P - FCH4),
                cBias = abs(Bias)/sum(abs(Bias))*100,
                predictor = feat[j])
    
    print(paste('predictor ', j))
    print(paste(k, 'th iteration'))
    
  }
  
  #bind all metrics for each
  rf_metrics_all <- bind_rows(rf_metrics) 
  
  # save the row of the best performing new predictor
  feat_best_row[[k]] <- rf_metrics_all %>% 
    filter(MAE == min(MAE))
  
  # get the specific predictor name
  feat_best_add <- feat_best_row[[k]] %>% 
    dplyr::select(predictor) %>% 
    pull() %>% as.character()
  
  # add to initial best_feat
  feat_best <- c(feat_best,feat_best_add)
  
  # add one to feat_initial size
  feat_initial <- feat_initial + 1
  
  # remove feat_best from feat
  feat_best_add_index <- which(feat == feat_best_add)
  feat <- feat[- feat_best_add_index ]
  
  print(paste('added ', feat_best_add))
}

feat_best_row_all <- bind_rows(feat_best_row)

write.csv(feat_best,
          "/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/FFS/FWET_ALL/200828_ffs_additional_predictors_k1_k10.csv",
          row.names = FALSE)

write.csv(feat_best_row_all, 
          "/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/FFS/FWET_ALL/200828_ffs_additional_metrics_k1_k10.csv",
          row.names = FALSE)


write.csv(rf_metrics_all,
          "/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/FFS/FWET_ALL/200828_ffs_k10_rf_metrics_all.csv",
          row.names = FALSE)



