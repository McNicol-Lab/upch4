---
title: "FLUXNET-CH4 Upscaling"
author: "Gavin McNicol"
date: "2/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages and ggplot theme:

```{r message = F}
library(tidyverse) 
library(lubridate)
library(raster)
library(oce) # for time series transformations
source("code/ggplot_theme.R")
```

**NOTE: This work flow uses many large files so most data is stored locally and requires hard filepaths**
The local file path used for all large files is: `/Volumes/LACIE SHARE/Stanford CH4/upch4_local/`.

Set the local head directory:

```{r}
loc <- "/Volumes/LACIE SHARE/Stanford CH4/upch4_local/"
```

#### Workflow

  1. Objective of Study
  2. Input Data
      + FLUXNET-CH4 Data
      + Gridded Data
  3. FLUXNET-CH4 Pre-Processing
      + FLUXNET-CH4 Data Preparation
          - Averaging
          - Variable Selection
  4. Gridded Data Pre-Processing
      + Grid Preparation
          - Potential Radiation (computed)
          - MODIS
      + Grid Extraction
          - Canopy Height
          - Computed (Rpot)
          - Compound Topographic Index
          - Earth Environment
          - N and S Deposition
          - SoilGrids
          - TerraClimate
          - Fractional Vegetation Cover (VCF)
          - Wetland Extent (WAD2M)
          - WorldClim 2.0
      + Merge Gridded Data
      + Gridded Data Quality Control
  5. Finalize Training Data
      + Subset Wetlands, Dates
      + Cluster Sites for Cross Validation
      + Finalize Data
          - Remove Extraneous
          - Impute Missing Data
          - Add Lagged Data
      + FLUXNET-CH4 Data Quality Control
      + Table of Final FLUXNET-CH4 Inputs
  6. Forward Feature Selection (FFS)
      + Filter Weekly Data
      + Feature Subset Experiments
      + FFS
          - First Pair
          - Additional Stepwise Features
          - FFS Evolution Plots
      + Summarize FFS Performance
  7. Cross Validation
      + ML Model Training
          - RF and Final Predictors or Subsets
          - XGB and Final Predictors or Subsets
          - ANN and Final Predictors or Subsets
          - RNN and Final Predictors or Subsets
      + Output Ensembles and Predictions
      + Validation 
          - Global Performance
          - Site-means
          - Monthly Seasonal Cycles
          - Monthly Anomalies
  8. Variable Importances
      + Variable Importance Rankings
      + Variable Responses
      + Partial Dependency Plots
      + ShapR
  9. Upscaled Model with Monte Carlo (MC)
      + Forcing Data
          - Mapping
          - Member Product Choices
          - Evaluate Product Divergence
      + Data Preparation
          - Extract Gridded Data for MC
          - Pre-Process FLUXNET-CH4 for MC
      + MC Simulations
      + MC ML Model Training
      + MC ML Model Validation
  10. Upscaling
      + Prepare Member Forcing Data
      + Run on Computing Cluster
          - Output Grids and Sums
      + Product Evaluation
          - Unweighted Wetland Fluxes (nmol)
          - Weighted Sums and Uncertainties (Tg)
          - Independent Validation
  11. Data Representativeness
      + Prepare Gridded Data
      + Global Dissimilarity
      + Tower Constituency
      + Extrapolation Errors
          - MC ML Model Training - Dissimilarity Only
          
[**Link to Workflow Figure**](https://drive.google.com/drive/folders/1jiFyqzoxxMpdtRLCwxCtzKpfILRNIW5K)
          
$~$            
      
#### 1. Objective of Study 

The goal of `FLUXNET-CH4 Upscaling` is to implement FLUXCOM-like ML approaches (e.g. [Jung et al. 2020](10.5194/bg-17-1343-2020)) to train a machine learning model using eddy covariance data that can predict wetland methane (CH4) fluxes globally. The predictions should be readily comparable to the Global Carbon Project (GCP) bottom-up process model ensembles that inform the Global Methane Budget ([Saunois et al. 2020](10.5194/essd-12-1561-2020)). Wetland fluxes specifically, rather than methane fluxes from all terrestrial ecosystems, are the predictive goal of this study because 1) most eddy covariance data available are in wetlands, with limited coverage across the multitude of upland ecosystems, 2) methane fluxes are highest and most variable in wetlands, and 3) comparable bottom-up process models predict wetland fluxes, then scale predictions to a global grid-cell using a prescribed (diagnostic runs) or model-derived (prognostic runs) wetland extent. In the last GCP Global Methane Budget ([Saunois et al. 2020](10.5194/essd-12-1561-2020)), diagnostic runs used the WAD2M product ([Zhang et al. in review](10.5194/essd-2020-262)). WAD2M includes coastal wetlands, however, we exclude coastal wetlands from the upscaling because they are salt-influenced and we do not have consistent salinity coverage, thus their inclusion is likely to bias flux estimates low even in non-coastal wetlands. The final model will be forced with available globally gridded data. Final product specifications are:

  - Monthly time-step
  - Historic reconstruction: ca. 2000 - 2018
  - 0.25-degree grid cell resolution (as is WAD2M)
  - Propagates training data uncertainties using Monte Carlo simulations
  - Considers sensitivity to global forcing data product choices

$~$    
  
#### 2. Input Data

**FLUXNET-CH4 Data**

The eddy covariance data used in this study are publicly available as part of the FLUXNET-CH4 community product V1.0 at [fluxnet.org](https://fluxnet.org/data/fluxnet-ch4-community-product/). The FLUXNET-CH4 synthesis activity is introduced in [Knox et al. 2019](10.1175/BAMS-D-18-0268.1) along with a detailed description of the eddy covariance post-processing steps including methane flux (FCH4) uncertainties. The first full (V1.0) dataset release (FLUXNET-CH4, hereafter) is described for 81 sites and is used in a wetland seasonality analysis in [Delwiche et al. in review](10.5194/essd-2020-307). **Data for this CH4 Upscaling Project were downloaded from [fluxnet.org](www.fluxnet.org) on Feb 22, 2021. [Download Manifest](https://docs.google.com/spreadsheets/d/1--_-XyBqsyiMIdc6JXqhilOOYFLeaY-UTMFhhI4Sd5M/edit#gid=0)**

Links:

  - [FLUXNET-CH4 Site (81) Metadata](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1384338468)
  - Permission was received via email on Feb 22, 2021, to use *Tier 2 Data Policy* sites in this study (SE-St1 and RU-Vrk; PI Thomas Friborg)
  - FLUXNET-CH4 data were used both for methane fluxes (FCH4; target variable) and tower-measured bio-meteorological variables (e.g., LE, GPP, TA; predictors)
      + Link to [FLUXNET.org variable descriptions](https://fluxnet.org/data/fluxnet-ch4-community-product/data-variables/)
  
$~$  
  
**Gridded Data (Predictors)**

A summary of predictors (`Predictor Summary`) is available [here](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=0), as well as an appendix table with all individual predictors (`Appendix: All Predictors`). Candidate predictors were drawn from a mix of source classes (EC tower measurements, global models, computations from observed data, or remote sensing (e.g. NASA MODIS)), different information content groups (Spatial-only or Spatio-temporal), and at different temporal frequencies (Static, Yearly, Monthly, Weekly, or Half-hourly). Using this information, we assigned each candidate predictor to a class (Generic, Climate, Biometeorological, Land Cover, Soil and Relief, or Greenness) to evaluate predictive power of particular classes, given the likelihood for redundancy in useful predictors across the full predictor set. For MODIS predictors, we computed the mean seasonal cycle (`_msc`), and yearly mean, min, max, and amplitude parameters, as in [Tramontana et al. (2016)](10.5194/bg-13-4291-2016) and [Jung et al. (2020)](10.5194/bg-17-1343-2020). For derived TerraClimate soil moisture and actual evapotranspiration (aet) we computed the interannual range and annual seasonality. More information on preprocessing of predictors is provided in the manuscript.

$~$

#### 3. FLUXNET-CH4 Pre-Processing

##### Local machine steps:

  + Create a copy of FLUXNET-CH4 data and name it `/fluxnet-ch4-data-original`
  + Unzip all site flux files in `/fluxnet-ch4-data` 
  + Reorganize into half-hourly (`/hh`) and `/daily` folders for easy access
  
##### Look at one site of **daily means** data.

```{r}
# setwd(loc)
# files <- list.files("fluxnet-ch4-data/daily/")
# one_site <- read_csv(paste(loc, "fluxnet-ch4-data/daily/", files[1], sep = ""))
# head(one_site)
```

No, but there is a quality flag `_QC`. `1`= data gap shorter than 2 months, `3` = gap exceeds 2 months. 

Issue: **There is also no uncertainty (`_UNC`) estimate on the downloaded daily mean data.**

##### Clean up workspace:

```{r}
# rm(one_site)
```

##### Look at one site of **half-hourly** data.

```{r}
setwd(loc)
files <- list.files("fluxnet-ch4-data/hh/")
one_site <- read_csv(paste(loc, "fluxnet-ch4-data/hh/", files[1], sep = ""))
head(one_site)
```

  - Half-hourly data has `FCH4` uncertainty columns `FCH4_F_RANDUNC` and `FCH4_F_ANNOPTLM_UNC`. Can be averaged over day or week.
  - Missing values are filled with `-9999`

##### Get FLUXNET-CH4 site names:

```{r}
site.names <- paste( substr(files, 5, 6), substr(files, 8, 10), sep = "")
```

##### Get all **half-hourly** flux data: (this will take a few minutes)

```{r echo = F, message = F, warning = F}
hh <- lapply(paste(loc, "fluxnet-ch4-data/hh/", files, sep = ""), read_csv)
names(hh) <- site.names
# str(hh)
```

##### Create a pristine replicate:

```{r}
hh2 <- hh
```

##### Look at all column names (including TIMESTAMP columns, to see date and time format):

```{r}
names(hh[[1]])
head(hh[[1]]$TIMESTAMP_END)
```

##### Write and apply function to expand `TIMESTAMP_END` into`Year`, `Month`, `Week`, `Day`, and `DOY` variables to facilitate merging with other data:

```{r}
expand_date <- function(hh_data) {
  hh_data <- hh_data %>% 
  mutate(Year = as.numeric(substr(TIMESTAMP_END, 1, 4)),
         Month = as.numeric(substr(TIMESTAMP_END, 5, 6)),
         Day = as.numeric(substr(TIMESTAMP_END, 7, 8)),
         Date = make_date(Year, Month, Day),
         DOY = yday(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week)) %>% 
    group_by(Year, DOY) %>% 
    mutate(HH = 1:n()) %>% 
  dplyr::select(Year, Month, Week, Day, HH, DOY, everything(), - Date, -TIMESTAMP_START, -TIMESTAMP_END)
}

hh <- lapply(hh, expand_date)
hh[[2]] # check it worked
```

##### Check if gap-filled FCH4 `CH4_F_ANNOPTLM` has already been pre-filled with observations `FCH4`, where available.

```{r}
hh[[1]] 
```

FCH4 in row 7 in `hh[[1]]` (ID BRNpw) = 10.71, which **matches** `CH4_F_ANNOPTLM` = 1.071e+01. 
**`CH4_F_ANNOPTLM` has been pre-filled with observations.**

##### Write and apply function to create `imputed`, a flag variable where:

  - `1` == `CH4_F_ANNOPTLM` was imputed
  - `2` == `CH4_F_ANNOPTLM` was observed

```{r}
create_imputed <- function(hh_data) {
  hh_data <- hh_data %>% 
    mutate(imputed = ifelse(FCH4 == -9999, 1, 0))
}

hh <- lapply(hh, create_imputed)
# hh[[1]] check it worked
```

##### Create ID column:

```{r}
for (i in 1:length(hh)){
  hh[[i]] <- hh[[i]] %>%
    mutate(ID = site.names[i]) %>%
    dplyr::select(ID, everything())
}
head(hh[[1]])
```

##### Convert missing values (`-9999`) to `NA`:

```{r}
for (i in 1:length(hh)){
  hh[[i]][hh[[i]] == -9999] <- NA
}
head(hh[[1]])
```






##### Get `u` and `v` wind components:

Identify which sites lack WD:

```{r}
find_wd <- function(hh_data) {
  sum(names(hh_data) == "WD") == 0
}

unlist(lapply(hh, find_wd))
sum(unname(unlist(lapply(hh, find_wd))))
```

OK, `CASCB` and `RUVrk` and missing `WD`.

Create dummy `WD` variables:

```{r}
hh$CASCB$WD <- NA
hh$RUVrk$WD <- NA

unlist(lapply(hh, find_wd))
sum(unname(unlist(lapply(hh, find_wd))))
```

Now compute wind direction components:

```{r}
compute_uv <- function(hh_data) {
  hh_data <- hh_data %>% 
    mutate(U = -WS_F * sin(2 * pi * WD/360),
           V = -WS_F * cos(2 * pi * WD/360))
}

hh <- lapply(hh, compute_uv)

head(hh[[1]]) # check it worked
```





##### Compute daily means of everything, including vector average WD and speed, excluding precip. (sums)

Create function to compute daily means:

```{r echo = F}
compute_daily1 <- function(hh_data) {
  hh_data <- hh_data %>% 
    group_by(ID, Year, DOY) %>% 
    summarize_all(list(~mean(., na.rm = T))) %>% 
    mutate(WD_mean = (atan2(U, V) * 360/2/pi) + 180,
           WS_mean = ((U^2 + V^2)^0.5)) %>% 
    dplyr::select(ID, Year, Month, Week, Day, DOY, everything(), -HH)
}
```

Check run time for one site:

```{r warning = F}
system.time( 
 compute_daily1(hh[[1]])
)
```

Apply to all sites using lapply:

```{r warning = F}
daily <- lapply(hh, compute_daily1)
```

Get sum of precip.

```{r warning = F, message = F}
sum_precip <- function(hh_data) {
  hh_data <- hh_data %>% 
    group_by(ID, Year, DOY) %>% 
    summarize(P_F_sum = sum(P_F))
}

daily_precip <- lapply(hh, sum_precip)
```

Rejoin precip. to main data frame:

```{r}
for (i in 1:length(daily)) {
daily[[i]] <- daily[[i]] %>% 
  left_join(daily_precip[[i]], by = c("ID", "Year", "DOY")) 
}
names(daily[[1]])
```

##### Output each "complete" daily site as a separate .csv file:

```{r}
for (i in 1:length(daily)) {
  daily[[i]] %>% write.csv(paste(loc, "fluxnet-ch4-data/daily_upch4/", site.names[i], "_daily_upch4.csv", sep = ""), 
                           row.names=FALSE)
}
```

#### 3. FLUXNET-CH4 Pre-Processing (cont.) 
##### (re)Load Daily Data

```{r message = F, warning = F, echo = F}
files <- list.files(paste(loc, "fluxnet-ch4-data/daily_upch4/", sep = ""))
daily <- lapply(paste(loc,"fluxnet-ch4-data/daily_upch4/",files, sep = ""), read_csv)
```

##### Bind rows

```{r}
daily_flat <- daily %>% bind_rows() %>% as_tibble()
head(daily_flat)
```

##### Compute FCH4 uncertainty, and subset gap-filled (and best) predictors

Notes on variable selection:

  + Total FCH4 uncertainty `FCH4_F_UNC` is random `FCH4_F_RANDUNC` and gap-filling `FCH4_F_ANNOPTLM_UNC` uncertainty summed in quadrature. I also drop the `FCH4_F_ANNOPTLM_QC` Quality control flag because I implement a stricter filtering criteria later of at least one observed flux per day.
  + The output of the daytime method `_DT` is used for gross primary production (GPP) and ecosystem respiration (RECO). Although both methods are subject to bias due to light-inhibition of leaf respiration, [Keenan et al. 2019](https://doi.org/10.1038/s41559-019-0809-2) show that the biases for the DT method only impact night-time respiration, but did not impact apparent photosynthesis or daytime respiration.
  + LE and NEE were gap-filled according to [Knox et al. 2019](10.1175/BAMS-D-18-0268.1) - same overall method to FCH4.
  + Other micro-meteorology (e.g., PPFD_IN) was gap-filled using ERA interim data (see [Delwiche et al.](10.5194/essd-2020-307)).
  + Only PPFD_IN (not PPFD_OUT) was selected as it can be approximated from SW_IN.
  + The shallowest available soil temperature is taken (`TS_1`)

```{r}
daily_subset <- daily_flat %>% 
  mutate(FCH4_F_UNC = sqrt(FCH4_F_ANNOPTLM_UNC^2 + FCH4_F_RANDUNC^2)) %>% 
  dplyr::select(ID, Year, Month, Week, Day, DOY,    # descriptive data
         FCH4 = FCH4_F_ANNOPTLM, FCH4_F_UNC, imputed,   # methane fluxes
         NEE = NEE_F_ANNOPTLM,  GPP = GPP_DT, RECO = RECO_DT,  # ecosystem C fluxes
         PPFD_IN = PPFD_IN_F, SW_IN = SW_IN_F, LW_IN = LW_IN_F, NETRAD = NETRAD_F,   # radiation
         LE = LE_F_ANNOPTLM, H = H_F,  # ecosystem energy fluxes
         TA = TA_F, PA = PA_F, RH = RH_F, VPD = VPD_F, P = P_F_sum, USTAR = USTAR, WS = WS_mean, # meteorology 
         TS = TS_1, SWC = SWC_F, WTD = WTD_F) # soil properties 
head(daily_subset)
```

##### Append site metadata

Pulled from [FLUXNET-CH4 Site Metadata](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1384338468)

```{r}
metadata <- read_csv(paste(loc, "fluxnet-ch4-data/metadata/fluxnet-ch4-site-metadata.csv", sep = "")) 
daily_subset_meta <- metadata %>% 
  mutate(ID = paste(substr(ID,1,2),substr(ID,4,6), sep="")) %>% 
  right_join(daily_subset, by = ("ID")) 
```
##### Save subset, flattened, and metadata-appended daily FLUXNET-CH4 data

```{r}
write.csv(daily_subset_meta, 
          paste(loc, "fluxnet-ch4-data/daily_flat/daily_subset_meta.csv", sep = ""),
          row.names = F)
```

#### 4. Gridded Data Pre-Processing

#### *Potential Radiation (RPot)*

**Description**
Get the mean seasonal cycle from Zutao Ouyang's 2001-2018 MATLAB output. Original citation for `Rpot` is [Peltola et al. 2019](10.5194/essd-11-1263-2019) where it was found to be a useful predictor of high latitude wetland FCH4.

Define a function to take a number of years and return an index of months:

```{r}
create_msc_index <- function(years) {
  msc_index <- list()
  for (i in 1:12){
      msc_index[[i]] <- seq(i, ((years-1)*12+i), by = 12)
  }
  msc_index
}
# create_msc_index(15) # test it out
```

Load in Rpot and create msc index for 19 years.

```{r}
Rpot <- brick(paste(loc, "grids/computed/Rpot.nc", sep = ""))
msc_index <- create_msc_index(19)
```

Get `_msc`:

```{r}
Rpot_msc <- list()
for (i in 1:12) {
  Rpot_msc[[i]] <- calc(Rpot[[msc_index[[i]]]], fun = mean)
}
```

Create raster stack:

```{r}
Rpot_msc <- stack(Rpot_msc)
```

Output `Rpot_msc.nc`:

```{r}
# writeRaster(Rpot_msc, paste(loc, "grids/computed/Rpot_msc.nc", sep = ""), format="CDF", overwrite = T)
```

#### *MODIS*

**Description**
Zutao Ouyang (Stanford University) extracted MODIS pixels at the FLUXNET-CH4 sites in April 2020, for 9 products:
 
  + Daytime Land Surface Temperature (`LSTD` from **MOD11A2**) 
      - *(not used as it is a correlate of nighttime temp., and nighttime is more applicable to soil conditions)*
  + Nighttime Land Surface Temperature (`LSTN` from **MOD11A2**)
  + Normalized Difference Vegetation Index (`NDVI` from **MOD09A1**)
  + Enhanced Vegetation Index (`EVI` from **MOD09A1**)
  + Leaf Area Index (`LAI` from **MCD15A2H**)
      - This is modeled, not directly measured.
  + Long Short Water Index (`LSWI`  from **MOD09A1**)
  + Simple Ratio Water Index (`SRWI` from **MOD09A1**)
  + Normalized Difference Water Index (`NDWI` from **MOD09A1**)
  + Normalized Difference Snow Index (`NDSI`  from **MOD11A2**)
  
**NOTE** Tables docmenting MODIS processing step effects on data are here  under [MODIS Processing](https://docs.google.com/spreadsheets/d/1DN0huLs-vVM3g_XcF1hBQrTpkKaGhzuWwaacfbe4iCo/edit#gid=1971246167) and QC figures are output to `upch4_local/modis/modis_qc`.


##### Get file local file names/paths for 8-day extracted MODIS data:

```{r}
setwd(paste(loc, "/modis/modis-extracted", sep = ""))
files <- list.files()
```

##### Set MODIS product/file names (for gather values):

```{r}
modis.names <- c("LSTD", "EVI", "LAI", "LSWI", "NDSI", "NDVI", "NDWI", "LSTN", "SRWI")
```

##### Read files:

```{r echo = F, warning = F, message = F}
modis <- lapply(paste(loc, "/modis/modis-extracted/", files, sep = ""), read_csv)
names(modis) <- modis.names
# str(modis)
```

##### Look at head for first file:

```{r}
head(modis[[1]])
```
Files are not tidy (short and wide). There are **86** extracted sites (extra sites than in FLUXNET-CH4 V1.0) but there should be **81**. 

##### Correct issues in Site IDs (columan names) to match `fluxnet-ch4-site-metadata.csv`:

```{r}
for(i in 1:length(modis)){
  modis[[i]] <- modis[[i]] %>% 
    dplyr::select(-`CA-DBB`, -`RU-SAM`, -`SE-Sto`, -`US-Bgl`, -`US-Brw`) %>% 
    rename(`JP-SwL` = `JP-Swl`, `PH-RiF` = `PH-RIf`, `US-ICs` = `US-Ics`, `US-MAC` = `MAERC`)
}
dim(modis[[1]])
```

##### Use gather to make long and narrow, remove hyphen from ID:

```{r}
for (i in 1:length(files)) {
  modis[[i]] <- modis[[i]] %>% 
    gather(key = ID, value = "modis.name", 2:82) %>% 
    mutate(ID = paste(substr(ID, 1, 2), substr(ID, 4, 6), sep = "")) %>% 
    as_tibble()
}
names(modis) <- modis.names
```

##### Check data are same length:

```{r}
str(modis)
```

1) `NDSI` is longer, 2) `LAI` is slightly shorter. Based on the dates, `NDSI` is daily.

Will not be able to bind columns with different lengths

##### Remove `LAI` and `NDSI` then rejoin:

```{r}
modis.1.names <- c("LSTD", "EVI", "LSWI", "NDVI", "NDWI", "LSTN", "SRWI")
modis.1 <- modis[modis.1.names]
```

##### Select only first two columns (Date and ID) then data columns:

```{r}
modis.1 <- modis.1 %>% 
  bind_cols() %>% 
  dplyr::select(1, 2, 3,6,9,12,15,18,21) %>% # subset only data columns
  as_tibble() 
names(modis.1) <- c("Date", "ID", modis.1.names)
```

##### Rejoin `LAI` using `Date`:

```{r}
modis.2 <- modis.1 %>% left_join(modis$LAI)
names(modis.2)[10] <- "LAI"
```

##### Rejoin `NDSI`:

```{r}
modis <- modis.2 %>% right_join(modis$NDSI)
names(modis)[11] <- "NDSI"
```

##### Convert `Date` into `Year` and `DOY`, and remove `LSTD`:

```{r}
modis <- modis %>% 
  mutate(Date = as.Date(Date, format = "%m/%d/%Y")) %>% 
  mutate(DOY = yday(Date),
         Date = as_date(Date),
         Week = ceiling(DOY/7),
         Week = ifelse(Week == 53, 52, Week),
         Week = as.factor(Week),
         Year = as.integer(substr(Date, 1,4 )),
         Month = as.numeric(substr(Date, 6, 7))) %>% 
  dplyr::select(ID, Date, Year, Month, Week, DOY, NDSI, NDVI, EVI, LAI, NDWI, SRWI, LSWI, LSTN) 
```

##### Count `NAs` before despiking:

```{r}
modis_nas <- modis %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)))) 
modis_nas
```
##### Despike outlier values for each product:

```{r}
modis_dsp <- modis %>% 
  mutate(NDSI_dsp = despike(NDSI, reference = 'trim', min = 0, max = 100, replace = "NA"),
         NDVI_dsp = despike(NDVI, reference = 'trim', min = -1, max = 1, replace = "NA"),
         EVI_dsp = despike(EVI, reference = 'trim', min = -1, max = 1, replace = "NA"),
         LAI_dsp = despike(LAI, reference = 'trim', min = 0, max = 5, replace = "NA"),
         NDWI_dsp = despike(NDWI, reference = 'trim', min = 0, max = 1, replace = "NA"),
         SRWI_dsp = despike(SRWI, reference = 'trim', min = -1, max = 3, replace = "NA"),
         LSWI_dsp = despike(LSWI, reference = 'trim', min = 0, max = 1, replace = "NA"),
         LSTN_dsp = despike(LSTN, reference = 'trim', min = -60, max = 50, replace = "NA")) %>% 
dplyr::select(ID, Date, Year, Month, Week, DOY, NDSI, NDVI, EVI, LAI, NDWI, SRWI, LSWI, LSTN,
              NDSI_dsp, NDVI_dsp, EVI_dsp, LAI_dsp, NDWI_dsp, SRWI_dsp, LSWI_dsp, LSTN_dsp) 
``` 

##### Count `NAs` after despiking:

```{r}
modis_dsp_nas <- modis_dsp %>%
  group_by(ID) %>%
  summarize_all(list(~sum(is.na(.)))) 
modis_dsp_nas
```
##### Calculate removed values during despiking and output to `upch4_local/modis/modis_qc/`

```{r}
modis_dsp_nas %>% 
  mutate(NDSI = NDSI - NDSI_dsp,
         NDVI = NDVI - NDVI_dsp,
         EVI = EVI - EVI_dsp,
         LAI = LAI - LAI_dsp,
         NDWI = NDWI - NDWI_dsp,
         SRWI = SRWI - SRWI_dsp,
         LSWI = LSWI - LSWI_dsp,
         LSTN = LSTN - LSTN_dsp) %>% 
  dplyr::select(-ID) %>% 
  summarize_all(list(~sum(.))) %>% 
  write.csv(paste(loc, "/modis/modis-qc/despiking_na_effects.csv", sep = ""))
```


##### Rename without `_dsp` suffix:

```{r}
modis_dsp <- modis_dsp %>% 
  dplyr::select(ID, Date, Year, Month, Week, DOY, 
                NDSI = NDSI_dsp, 
                NDVI = NDVI_dsp, 
                EVI = EVI_dsp,
                LAI = LAI_dsp,
                NDWI = NDWI_dsp,
                SRWI = SRWI_dsp,
                LSWI = LSWI_dsp,
                LSTN = LSTN_dsp)
```


##### Calculate mean  seasonal cycle (msc), then weekly means, then fill weekly gaps with msc:

```{r}
modis_gapfilled <- modis_dsp %>% 
  group_by(ID, Month) %>%  # this section computes  monthly values averaged over multiple years
  mutate(NDSI_msc = mean(NDSI, na.rm = TRUE),
         NDVI_msc = mean(NDVI, na.rm = TRUE),
         EVI_msc = mean(EVI, na.rm = TRUE), 
         LAI_msc = mean(LAI, na.rm = TRUE),
         NDWI_msc = mean(NDWI, na.rm = TRUE),
         SRWI_msc = mean(SRWI, na.rm = TRUE),
         LSWI_msc = mean(LSWI, na.rm = TRUE),
         LSTN_msc = mean(LSTN, na.rm =TRUE)) %>% 
  group_by(ID, Year, Month, Week) %>% # this section compute weekly means
  summarize(NDSI = mean(NDSI, na.rm = TRUE),
            NDVI = mean(NDVI, na.rm = TRUE),
            EVI = mean(EVI, na.rm = TRUE),
            LAI = mean(LAI, na.rm = TRUE),
            NDWI = mean(NDWI, na.rm = TRUE),
            SRWI = mean(SRWI, na.rm = TRUE),
            LSWI = mean(LSWI, na.rm = TRUE),
            LSTN = mean(LSTN, na.rm = TRUE),
            NDSI_msc = NDSI_msc[1],    # this section selects the monthly (msc) value corresponding to the weekly data
            NDVI_msc = NDVI_msc[1],
            EVI_msc = EVI_msc[1],
            LAI_msc = LAI_msc[1],
            NDWI_msc = NDWI_msc[1],
            SRWI_msc = SRWI_msc[1],
            LSWI_msc = LSWI_msc[1],
            LSTN_msc = LSTN_msc[1]) %>% 
  mutate(NDSI_F = ifelse(is.na(NDSI), NDSI_msc, NDSI), # this section fills any missing weekly values with the msc (monthly average)
         NDVI_F = ifelse(is.na(NDVI), NDVI_msc, NDVI),
         EVI_F = ifelse(is.na(EVI), EVI_msc, EVI),
         LAI_F = ifelse(is.na(LAI), LAI_msc, LAI),
         NDWI_F = ifelse(is.na(NDWI), NDWI_msc, NDWI),
         SRWI_F = ifelse(is.na(SRWI), SRWI_msc, SRWI),
         LSWI_F = ifelse(is.na(LSWI), LSWI_msc, LSWI),
         LSTN_F = ifelse(is.na(LSTN), LSTN_msc, LSTN)) 
```


##### If snow is on the ground, set water indices to the 5% quantile (frozen):

```{r}
modis_frozen <- modis_gapfilled %>% 
  group_by(ID,Year) %>% 
  mutate(NDWI_msc = ifelse(NDSI_msc > 0, quantile(NDWI_msc, 0.05, na.rm=TRUE), NDWI_msc),
         SRWI_msc = ifelse(NDSI_msc > 0, quantile(SRWI_msc, 0.05, na.rm=TRUE), SRWI_msc),
         LSWI_msc = ifelse(NDSI_msc > 0, quantile(LSWI_msc, 0.05, na.rm=TRUE), LSWI_msc),
         
         NDWI_F = ifelse(NDSI_F > 0, quantile(NDWI_F, 0.05, na.rm=TRUE), NDWI_F),
         SRWI_F = ifelse(NDSI_F > 0, quantile(SRWI_F, 0.05, na.rm=TRUE), SRWI_F),
         LSWI_F = ifelse(NDSI_F > 0, quantile(LSWI_F, 0.05, na.rm=TRUE), LSWI_F))
```

##### Compute mean, min, max, amplitude:

```{r message = F, warning = F}
modis_frozen <- modis_frozen %>% 
  group_by(ID, Year) %>% 
  mutate(NDSI_mean = mean(NDSI_F, na.rm=TRUE),
         NDVI_mean = mean(NDVI_F, na.rm=TRUE),
         EVI_mean = mean(EVI_F, na.rm=TRUE),
         LAI_mean = mean(LAI_F, na.rm=TRUE),
         NDWI_mean = mean(NDWI_F, na.rm=TRUE),
         SRWI_mean = mean(SRWI_F, na.rm=TRUE),
         LSWI_mean = mean(LSWI_F, na.rm=TRUE),
         LSTN_mean = mean(LSTN_F, na.rm=TRUE),
         
         NDSI_min = min(NDSI_F, na.rm=TRUE),
         NDVI_min = min(NDVI_F, na.rm=TRUE),
         EVI_min = min(EVI_F, na.rm=TRUE),
         LAI_min = min(LAI_F, na.rm=TRUE),
         NDWI_min = min(NDWI_F, na.rm=TRUE),
         SRWI_min = min(SRWI_F, na.rm=TRUE),
         LSWI_min = min(LSWI_F, na.rm=TRUE),
         LSTN_min = min(LSTN_F, na.rm=TRUE),
         
         NDSI_max = max(NDSI_F, na.rm=TRUE),
         NDVI_max = max(NDVI_F, na.rm=TRUE),
         EVI_max = max(EVI_F, na.rm=TRUE),
         LAI_max = max(LAI_F, na.rm=TRUE),
         NDWI_max = max(NDWI_F, na.rm=TRUE),
         SRWI_max = max(SRWI_F, na.rm=TRUE),
         LSWI_max = max(LSWI_F, na.rm=TRUE),
         LSTN_max = max(LSTN_F, na.rm=TRUE),
         
         NDSI_amp = NDSI_max-NDSI_min,
         NDVI_amp = NDVI_max-NDVI_min,
         EVI_amp = EVI_max-EVI_min,
         LAI_amp = LAI_max-LAI_min,
         NDWI_amp = NDWI_max-NDWI_min,
         SRWI_amp = SRWI_max-SRWI_min,
         LSWI_amp = LSWI_max-LSWI_min,
         LSTN_amp = LSTN_max-LSTN_min)
```

##### Get site IDs:

```{r}
site.names <- modis_frozen %>%
  ungroup() %>% 
  mutate(ID = factor(ID)) %>% 
  dplyr::select(ID) %>% 
  pull() %>% unique()
```

##### Get variable names:

```{r}
names <- names(modis)[7:14]
names_F <- paste(names, "_F", sep = "")
names_msc <- paste(names, "_msc", sep = "")
```

##### Create modis qcqa figures:

```{r message = F, warning = F}
for (i in 1:length(names)){
  
  # visualize 1-45
  modis_frozen %>%
    filter(ID %in% site.names[1:45]) %>%
    dplyr::select(Month, value_F = names_F[i], value_msc = names_msc[i]) %>% 
    ggplot(aes(Month, value_F)) +
    geom_point(size = 1, alpha = 0.3) +
    geom_line(aes(Month, value_msc), col = 'purple', size = 2) +
    facet_wrap(~ID, scales = 'free', ncol = 9) +
    my_theme
  ggsave(paste(loc, "/modis/modis-qc/", names[i], "_1.pdf", sep = ""),
         width = 50, height = 30, units = c("cm"), dpi = 300)
  
  # visualize 46-86
  modis_frozen %>%
    filter(ID %in% site.names[46:86]) %>%
    dplyr::select(Month, value_F = names_F[i], value_msc = names_msc[i]) %>% 
    ggplot(aes(Month, value_F)) +
    geom_point(size = 1, alpha = 0.3) +
    geom_line(aes(Month, value_msc), col = 'purple', size = 2) +
    facet_wrap(~ID, scales = 'free', ncol = 9) +
    my_theme
  ggsave(paste(loc, "/modis/modis-qc/", names[i], "_2.pdf", sep = ""),
         width = 50, height = 30, units = c("cm"), dpi = 300)
  
}
```

##### Output processed MODIS data:

```{r}
write.csv(modis_frozen, paste(loc, "/modis/modis-processed/modis-processed.csv", sep = ""),
          row.names = FALSE)
```

#### *Grid Extraction* 

**Extract geospatial data at FLUXNET-CH4 sites and output a single geospatial .csv.**

##### Load site metadata (ID, Latitude, Longitude), applicable to all extractions:

```{r}
sites <- read_csv(paste(loc, "fluxnet-ch4-data/metadata/fluxnet-ch4-site-metadata.csv", sep = "")) %>% 
  mutate(ID = paste(substr(ID, 1, 2), substr(ID, 4, 6), sep = ""))
site.coords <- cbind(sites$Longitude, sites$Latitude) # (x, y)
site.num <- length(site.coords[,1])
```

##### Global Canopy Height

```{r}
raster.names <- list.files(paste(loc, "grids/global-canopy-height/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/global-canopy-height/", raster.names, sep = ""))
canopyht <- as_tibble(raster::extract(stack, site.coords)) 
canopyht <- cbind(sites, canopyht) %>% 
  rename(canopyht = Simard_Pinto_3DGlobalVeg_JGR)
head(canopyht)
```

###### Check for missing data

```{r}
canopyht %>% 
  mutate(missing = is.na(canopyht)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(canopyht, paste(loc, "grids/extracted/", "canopyht.csv", sep = ""), row.names =  F)
```

##### Computed (rpot)

```{r}
stack <- stack(paste(loc, "grids/computed/Rpot_msc.nc", sep = ""))
rpot <- as_tibble(raster::extract(stack, site.coords)) 
rpot <- cbind(sites, rpot) %>% 
  gather(key = "Month", value = "rpot", 15:26) %>% 
  mutate(Month = str_remove(Month, "X"))
head(rpot)
```

###### Check for missing data

```{r}
rpot %>% 
  mutate(missing = is.na(rpot)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```
No missing data

###### Write data

```{r}
write.csv(rpot, paste(loc, "grids/extracted/", "rpot.csv", sep = ""), row.names =  F)
```

##### Compound Topographic Index

```{r}
raster.names <- list.files(paste(loc, "grids/geomorpho90m/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/geomorpho90m/", raster.names, sep = ""))
cti <- as_tibble(raster::extract(stack, site.coords)) 
cti <- cbind(sites, cti) %>% 
  rename(cti = dtm_cti_merit.dem_m_250m_s0cm_2018_v1.0)
head(cti)
```

No missing data

###### Check for missing data

```{r}
cti %>% 
  mutate(missing = is.na(cti)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))

```

No missing data

###### Write data

```{r}
write.csv(cti, paste(loc, "grids/extracted/", "cti.csv", sep = ""), row.names =  F)
```

##### Earth Environment Texture, Land Cover and Topography

Texture (14 variables)
Land Cover (2 variables)
Topography (10 variables)
See: [Appendix: All Predictors](https://docs.google.com/spreadsheets/d/1yBCIt5nFHVIb9v_0ExX8-4Svr0wlcmjD9vdp67hGtnQ/edit#gid=800847828). Search for `EarthEnv` under column `Gridded Product`.

###### First do texture:

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/texture/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/texture/", raster.names, sep = ""))
earthenv_texture <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_texture) <- c("cont","corr","cv","diss","entr","even","homo","max","rang","shan","simp","std","unif","var")
```

###### Then land cover (commented code for first-run creates `HD` - human development -  as the sum of LC7 and LC9):

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/cover/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/cover/", raster.names, sep = ""))

# # create `HD` (human development)
# HD <- stack[[1]] + stack[[2]]
# writeRaster(HD, paste(loc, "/grids/earthenv/cover/HD.tiff", overwrite = T))

earthenv_cover <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_cover) <- c("HD7", "HD9", "HD")
```

###### Then Topography:

```{r}
raster.names <- list.files(paste(loc, "grids/earthenv/topography/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/earthenv/topography/", raster.names, sep = ""))
earthenv_topography <- as_tibble(raster::extract(stack, site.coords)) 
names(earthenv_topography) <- c("east","elev","flat","hollow","north","pcurv","rough","slope","tcurv","tpi")
```

###### Now combine:

```{r}
earthenv <- cbind(sites, earthenv_texture, earthenv_cover, earthenv_topography)
```

###### Look at data histograms:

```{r}
earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  ggplot(aes(value)) + 
  geom_histogram() + 
  facet_wrap(~var, scales = 'free')
```

`cont`, `diss`, and `var` have extreme outliers (xmax = 10e+09)

###### Replace extreme outliers with median

```{r}
earthenv <- earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  group_by(var) %>% 
  mutate(value = ifelse(value > 10^9, median(value, na.rm = TRUE), value)) %>% 
  ungroup() %>% 
  spread(key = "var", value = "value")
```

Visualize again above to check they are corrected.

###### Also check for missing data:

```{r}
earthenv %>% 
  gather(key = "var", value = "value", 15:41) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

###### There are two sites (USDPW and USWPT) with missing values. Plot the sites:

```{r}
plot(stack[[1]])
points(site.coords[sites$ID %in% c("USDPW", "USWPT"), ])
```

###### Fill with nearby site values. 

USDPW from USLA1

USWPT from USOWC

```{r}
earthenv[earthenv$ID == "USDPW", c(is.na(earthenv[earthenv$ID == "USDPW", ])) ] <- earthenv[earthenv$ID == "USLA1", c(is.na(earthenv[earthenv$ID == "USDPW", ])) ] 
earthenv[earthenv$ID == "USWPT", c(is.na(earthenv[earthenv$ID == "USWPT", ])) ] <- earthenv[earthenv$ID == "USOWC", c(is.na(earthenv[earthenv$ID == "USWPT", ])) ] 
```

###### Write data

```{r}
write.csv(earthenv, paste(loc, "grids/extracted/", "earthenv.csv", sep = ""), row.names =  F)
```

##### N and S Deposition

From [Lamarque et al. 2013](10.5194/acp-13-7997-2013):

  - accmip_nhx_acchist_2000.nc
  - accmip_noy_acchist_2000.nc
  - accmip_sox_acchist_2000.nc

```{r message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/ns-deposition/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/ns-deposition/", raster.names, sep = ""))
stack[[1]]
```

###### Update extent (which runs 0, 360, -90, 90)

```{r}
x1 <- crop(stack, extent(180, 360, -90, 90))
x2 <- crop(stack, extent(0, 180, -90, 90))   
extent(x1) <- c(-180, 0, -90, 90)
m <- merge(x1, x2)
names(m) <- names(stack)
```

###### Check overlap of points

```{r}
m[[1]] %>% na_if(-9999) %>% plot() 
```
###### Extract ns data and sum n variables for total n (`nt_depo`)

```{r}
ns_depo <- as_tibble(raster::extract(m, site.coords)) 
ns_depo <- cbind(sites, ns_depo) %>% 
  rename(nx_depo = Dry.deposition.NH3.NH4,
         ny_dep = Dry.deposition.NOy,
         s_dep = Dry.deposition.SO2.SO4) %>% 
  mutate(nt_depo = nx_depo + ny_dep)
head(ns_depo)
```
###### Check for missing data

```{r}
ns_depo %>% 
  gather(key = "var", value = "value", 15:17) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(ns_depo, paste(loc, "grids/extracted/", "ns_depo.csv", sep = ""), row.names =  F)
```

##### SoilGrids

6 products:

  - BLDFIE_M_sl2_250m_II.tif (bulk density)
  - OCDENS_M_sl2_250m_II.tif (organic carbon density)
  - PHIHOX_M_sl2_250m_II.tif (soil pH)
  - CLYPPT_M_sl2_250m_II.tif (proportion of clay particles)
  - AWCh3_M_sl2_250m_II.tif (soil water holding capacity)
  - HISTPR_250m_II.tif (histogram abundance)

```{r}
raster.names <- list.files(paste(loc, "grids/soilgrids/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/soilgrids/", raster.names, sep = ""))
# plot(stack[[3]])
soilgrids <- as_tibble(raster::extract(stack, site.coords)) 
soilgrids <- cbind(sites, soilgrids) %>% 
  rename(awc = AWCh3_M_sl2_250m_ll,
         bd = BLDFIE_M_sl2_250m_ll,
         clay = CLYPPT_M_sl2_250m_ll,
         hist = HISTPR_250m_ll,
         ocd = OCDENS_M_sl2_250m_ll,
         ph = PHIHOX_M_sl2_250m_ll)
head(soilgrids)
```

###### Check for missing soilgrids data:

```{r}
soilgrids %>% 
  gather(key = "var", value = "value", 15:20) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```
###### Check locations of DEDgw and USDPW by cropping raster:

```{r}
# site.coords[sites$ID %in% c("DEDgw", "USDPW"),]
DEDgw_bb <- c( "xmin" =  13.05427 - 0.1, "xmax" = 13.05427 + 0.1, 
               "ymin" = 53.15141 - 0.1 , "ymax" =  53.15141 + 0.1)
USDPW_bb <- c( "xmin" =  -81.43611 - 0.1, "xmax" = -81.43611 + 0.1,
               "ymin" = 28.05206 - 0.1 , "ymax" =  28.05206 + 0.1)
DEDgw_loc <- crop(stack[[1]], DEDgw_bb)
USDPW_loc <- crop(stack[[1]], USDPW_bb)
```

###### First check DEDgw

```{r}
plot(DEDgw_loc)
points(site.coords)
```

###### Then check USDPW

```{r}
plot(USDPW_loc)
points(site.coords)
```
By chance these two sites fall on NA pixels, by being close to water bodies.

###### Shift latitude by 0.005-degrees N

```{r}
sites.temp <- sites %>% 
  filter(ID %in% c("DEDgw","USDPW")) %>% 
  mutate(Latitude = Latitude + 0.005)
site.coords.temp <- sites.temp %>% 
  dplyr::select(Longitude, Latitude) %>% 
  unname() %>% as.matrix()
```

###### Extract only these two lat-adjusted sites

```{r}
soilgrids.temp <- as_tibble(raster::extract(stack, site.coords.temp)) 
soilgrids.temp <- cbind(sites.temp, soilgrids.temp) %>% 
  rename(awc = AWCh3_M_sl2_250m_ll,
         bd = BLDFIE_M_sl2_250m_ll,
         clay = CLYPPT_M_sl2_250m_ll,
         hist = HISTPR_250m_ll,
         ocd = OCDENS_M_sl2_250m_ll,
         ph = PHIHOX_M_sl2_250m_ll)
head(soilgrids.temp)
```

###### Rejoin soilgrids

```{r}
soilgrids <- soilgrids %>% 
  filter(!is.na(bd)) %>% 
  bind_rows(soilgrids.temp) %>%
  arrange(ID)
```

###### Write data

```{r}
write.csv(soilgrids, paste(loc, "grids/extracted/", "soilgrids.csv", sep = ""), row.names =  F)
```

##### TerraClimate

2 products:
  
  - Soil Moisture (soilwater and soilwaterR (range), soilwaterS (annual seasonality))
  - Actual Evapotranspiration (aet and aetR, aetS)
  
###### Read Actual Evapotranspiration grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/terraclimate/aet/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/terraclimate/aet/", raster.names, sep = ""))
plot(stack[[3]])
```
###### Extract Actual Evapotranspiration grids

```{r}
# extract data at sites
terraclimate.aet <- as_tibble(raster::extract(stack, site.coords))

# create new name vector
x <- c(as.character(c(1:216)))

# rename to retain order
terraclimate.aet <- terraclimate.aet %>% as_tibble() %>% 
  rename_all(~x)

# look at combined data
terraclimate.aet <- cbind(sites,terraclimate.aet)
head(terraclimate.aet)
```

###### Finalize Actual Evapotranspiration

```{r}
terraclimate.aet <- terraclimate.aet %>%
  gather(key = Month, value = aet, 15:length(terraclimate.aet)) %>% 
  mutate(Month = as.numeric(Month), 
         aet = as.numeric(aet)) %>% 
  arrange(ID, Month) %>% 
  mutate(Year = floor((Month - 0.1) / 12 + 2001)) %>% 
  group_by(ID, Year) %>% 
  mutate(Month = 1:n()) %>% 
  mutate(aetS = (aet - min(aet, na.rm = TRUE)) / (max(aet, na.rm = TRUE) - min(aet, na.rm = TRUE)),
         aetR = max(aet)-min(aet))
head(terraclimate.aet)
```

###### Read Soil Water grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/terraclimate/soil/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/terraclimate/soil/", raster.names, sep = ""))
plot(stack[[3]])
```

###### Extract Soil Water grids

```{r}
# extract data at sites
terraclimate.soilwater <- as_tibble(raster::extract(stack, site.coords))

# create new name vector
x <- c(as.character(c(1:216)))

# rename to retain order
terraclimate.soilwater <- terraclimate.soilwater %>% as_tibble() %>% 
  rename_all(~x)

# look at combined data
terraclimate.soilwater <- cbind(sites,terraclimate.soilwater)
head(terraclimate.soilwater)
```

###### Finalize soilwater data

```{r}
terraclimate.soilwater <- terraclimate.soilwater %>%
  gather(key = Month, value = soilwater, 15:length(terraclimate.soilwater)) %>% 
  mutate(Month = as.numeric(Month),
         soilwater = as.numeric(soilwater)) %>% 
  arrange(ID, Month) %>% 
  mutate(Year = floor((Month-0.1)/12+2001)) %>% 
  group_by(ID, Year) %>% 
  mutate(Month = 1:n()) %>% 
  mutate(soilwaterS = (soilwater-min(soilwater,na.rm=TRUE))/(max(soilwater,na.rm=TRUE)-min(soilwater,na.rm=TRUE)),
         soilwaterR = max(soilwater)-min(soilwater)) %>% 
  mutate(soilwaterS = ifelse(soilwaterR == 0 & is.na(soilwaterS), 0.5, soilwaterS))
head(terraclimate.soilwater)
```

###### Combined aet and soilwater

```{r}
# combine both aet and soilwater
terraclimate <- terraclimate.aet %>% 
  left_join(terraclimate.soilwater) %>% 
  dplyr::select(1:14, 17, 15, aet, aetS, aetR, soilwater, soilwaterS, soilwaterR)
head(terraclimate)
```

###### Check for missing data

```{r}
terraclimate %>% 
  gather(key = "var", value = "value", 15:20) %>% 
  mutate(missing = is.na(value)) %>% 
  group_by(ID) %>% 
  summarize(total = n(),
            missing = sum(missing)) %>% 
  arrange(desc(missing))
```

No missing data

###### Write data

```{r}
write.csv(terraclimate, paste(loc, "grids/extracted/", "terraclimate.csv", sep = ""), row.names =  F)
```

##### Fractional Vegetation Cover (VCF)

###### Read VCF grids
  
```{r, message = F, warning = F}
raster.names <- list.files(paste(loc, "grids/VCF5KYR/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/VCF5KYR/", raster.names, sep = ""))
plot(stack[[3]])
```

```{r}
# extract data at sites
vcf <- as_tibble(raster::extract(stack, site.coords))
```

###### Tiff names have year, however some years are missing. Create new name vector from years and data types:

```{r}
years_missing <- c(seq(1982,2016,1)) %>% as_tibble() %>% rename(Year = value) %>% 
  cbind(c(1:35)) %>% as_tibble() %>% rename(Row = `c(1:35)`) %>% 
  filter(Row %in% c(13, 19)) %>% dplyr::select(Year) %>% 
  pull() 

names_vcf <- c(seq(1982,2016,1)) %>% as_tibble() %>% rename(Year = value) %>% 
  cbind(c(1:35)) %>% as_tibble() %>% rename(Row = `c(1:35)`) %>% 
  filter(!Row %in% c(13, 19)) %>% dplyr::select(Year) %>% 
  pull() %>% rep(each = 3) %>% as.character()

suffix <- c("_tree","_shrub","_bare") %>% rep(33)

names_vcf <- paste(names_vcf, suffix, sep="")
```

###### Rename to retain order of data, then add missing years as means of adjacent years, and extent time series by two years to 2018 (assuming no change):

```{r}
# rename to retain order
vcf <- vcf %>% as_tibble() %>% 
  rename_all(~names_vcf)

# add in missing years
vcf <- vcf %>% mutate(row = 1:n()) %>% 
  group_by(row) %>% 
        mutate(`1994_bare` = mean(c(`1993_bare`,`1995_bare`)), 
               `1994_tree` = mean(c(`1993_tree`,`1995_tree`)), 
               `1994_shrub` = mean(c(`1993_shrub`,`1995_shrub`)), 
           
               `2000_bare` = mean(c(`1999_bare`,`2001_bare`)), 
               `2000_tree` = mean(c(`1999_tree`,`2001_tree`)), 
               `2000_shrub` = mean(c(`1999_shrub`,`2001_shrub`)),
               
               `2017_bare` = `2016_bare`,
               `2017_tree` = `2016_tree`,
               `2017_shrub` = `2016_shrub`,

               `2018_bare` = `2016_bare`,
               `2018_tree` = `2016_tree`,
               `2018_shrub` = `2016_shrub`) %>% ungroup() %>% 
  dplyr::select(-row)
```

###### look at combined data

```{r}
vcf <- cbind(sites,vcf)
head(vcf)
```
###### Finalize dataset: gather data split year and predictor

```{r}
vcf <- vcf %>%
  gather(key = Year, value = value, 15:length(vcf)) %>% 
  mutate(Predictor = substr(Year,6,10),
         Year = substr(Year,1,4)) %>% 
  spread(key = Predictor, value = value)
head(vcf)
```
###### Plot data

```{r}
vcf %>% 
  gather(key = "vcf_var", value = value, 16:18) %>% 
  ggplot(aes(value)) + 
  geom_histogram() +
  facet_wrap(~vcf_var) +
  my_theme
```
Data looks reasonable, all between 0 and 100.

###### Check that trends look reasonable:

```{r}
vcf %>% 
  gather(key = "vcf_var", value = value, 16:18) %>% 
  ggplot(aes(Year, value, color = vcf_var)) + 
  geom_point() +
  facet_wrap(~ID) +
  my_theme
```

US-AO3 looks weird (zeros)


###### Replace USA03 with USA10 data, due to strange behavior:

```{r}
vcf[vcf$ID == "USA03", 16:18] <- vcf[vcf$ID == "USA10", 16:18]
```

###### Write data

```{r}
write.csv(vcf, paste(loc, "grids/extracted/", "vcf.csv", sep = ""), row.names =  F)
```

##### Wetland Extent (WAD2M)

###### Get rasters

Use the coarser 0.5-degree map because 0.25-degree edge roughness causes more missing data

```{r}
setwd(loc)
raster.names <-list.files(paste(loc, "grids/WAD2M/", sep = ""), pattern = "nc$", full.names = FALSE)
stack <- stack(paste(loc, "grids/WAD2M/", sep = "", raster.names[2]))
plot(stack[[7]])
points(site.coords)
```
###### Extract data at sites and look at it

```{r}
wf <- as_tibble(raster::extract(stack, site.coords))
wf <- cbind(sites,wf)
head(wf)
```

###### Finalize WF Seasonality:

  - Create date, month, and year columns from varnames
  - Replace any missing data with zero 
      + some sites are marine/rice paddy, or coastal  (false, need correction where freshwater wetland, see below)
      + frozen high latitude locations in winter are assigned NA (real, no correction)
  - Calculate seasonality

```{r}
wf <- wf %>%
  gather(key = Month, value = WF, 15:length(wf)) %>% 
  mutate(Month2 = Month,
         Date = substr(Month2,2,11),
         Year = substr(Month2,2,5),
         Month = as.factor(as.integer(substr(Month2,7,8)))) %>% 
  dplyr::select(-Month2) %>% 
  mutate(Date = as.Date(Date, format = "%Y.%m.%d"),
         DOY = yday(Date)) %>% 
  group_by(ID, Year) %>% 
  mutate(WF = replace_na(WF,0),
         WFS = (WF-min(WF,na.rm=TRUE))/(max(WF,na.rm=TRUE)-min(WF,na.rm=TRUE))) %>% 
  group_by(ID, Year) %>% 
  mutate(WFR = max(WF)-min(WF)) %>% 
  ungroup() %>% 
  mutate(WFS = ifelse(WFR == 0 & is.na(WFS), 0.5, WFS)) %>% 
  dplyr::select(1:14, Date, Year, Month, DOY, WF, WFS, WFR)
```

###### Look at data patterns

```{r}
wf %>% 
  gather(key = "wf_var", value = "value", 19:21) %>% 
  ggplot(aes(Date, value, color = wf_var)) +
  geom_point() +
  facet_wrap(~ID)
```
There are missing data for: "DEHte", "HKMPM", "JPSwL", "KRCRK", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USHRA", "USHRC", "USNC4", "USNGB"
  
Of these, "HKMPM", "JPSwL", "KRCRK", "USHRA", "USHRC", are either lakes, rice, or mangroves (i.e. cannot easily fill missing data)

But these can be corrected: "DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNC4", "USNGB"

###### Get bounding boxes for each correctable site

```{r}
coords_subset <- site.coords[sites$ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNC4", "USNGB"),] 
coords_subset_names <- sites$ID[sites$Latitude %in% coords_subset]

bounding_box <- function(x) {
  c("xmin" =  x[1] - 3, "xmax" = x[1] + 3, 
               "ymin" = x[2] - 3 , "ymax" =  x[2] + 3)
}

bounding_boxes <- list()
for(i in 1:nrow(coords_subset)){
 bounding_boxes[[i]] <- bounding_box(coords_subset[i,])
}

bounding_boxes_maps <- list()
for(i in 1:length(bounding_boxes)){
  bounding_boxes_maps[[i]] <- crop(stack[[8]], bounding_boxes[[i]])
}
```

###### Match up longitudes with site names:

```{r}
sites %>% 
  filter(Longitude %in% coords_subset[,1])
```

###### Look at each site:

```{r}
for(i in 1:length(bounding_boxes)){
  plot(bounding_boxes_maps[[i]])
  points(coords_subset)
}
```

Looks like moving all 1:6 and 8 south by 1 degree, and moving 7 west by 1 degree will help with extractions:

###### Adjust lat/longs and reextract just these 8 sites

```{r}
site.coords.temp <- sites %>% 
  filter(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB")) %>% 
  mutate(Latitude = ifelse(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT","USNGB"), Latitude - 1, Latitude),
         Longitude = ifelse(ID == "USNC4", Longitude - 1, Longitude)) %>% 
  dplyr::select(Longitude, Latitude) %>% 
  as.matrix() %>% unname()

sites.temp <- sites %>% 
  filter(ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB"))
         
wf.temp <- as_tibble(raster::extract(stack, site.coords.temp))
wf.temp <- cbind(sites.temp, wf.temp)
head(wf.temp)
```

###### Finalize WF Seasonality for temporary sites

```{r}
wf.temp <- wf.temp %>%
  gather(key = Month, value = WF, 15:length(wf.temp)) %>% 
  mutate(Month2 = Month,
         Date = substr(Month2,2,11),
         Year = substr(Month2,2,5),
         Month = as.factor(as.integer(substr(Month2,7,8)))) %>% 
  dplyr::select(-Month2) %>% 
  mutate(Date = as.Date(Date, format = "%Y.%m.%d"),
         DOY = yday(Date)) %>% 
  group_by(ID, Year) %>% 
  mutate(WF = replace_na(WF,0),
         WFS = (WF-min(WF,na.rm=TRUE))/(max(WF,na.rm=TRUE)-min(WF,na.rm=TRUE))) %>% 
  group_by(ID, Year) %>% 
  mutate(WFR = max(WF)-min(WF)) %>% 
  ungroup() %>% 
  mutate(WFS = ifelse(WFR == 0 & is.na(WFS), 0.5, WFS)) %>% 
  dplyr::select(1:14, Date, Year, Month, DOY, WF, WFS, WFR)
```

###### Rejoin data

```{r}
wf.rm <- wf %>% 
  filter(!ID %in% c("DEHte", "NZKop", "USA10", "USBeo", "USBes", "USCRT", "USNGB"))

wf <- wf.rm %>% 
  bind_rows(wf.temp) %>% 
  arrange(ID, Date)
```
###### Look at data patterns

```{r}
wf %>% 
  gather(key = "wf_var", value = "value", 19:21) %>% 
  ggplot(aes(Date, value, color = wf_var)) +
  geom_point() +
  facet_wrap(~ID)
```

Data are now complete (except rice paddies, lakes and mangroves which we cannot infer)

###### Write data

```{r}
write.csv(wf, paste(loc, "grids/extracted/", "wf.csv", sep = ""), row.names =  F)
```

##### WorldClim 2.0

```{r warning = F}
setwd(loc)
raster.names <-list.files(paste(loc, "grids/worldclim/", sep = ""), pattern = "tif$", full.names = FALSE)
stack <- stack(paste(loc, "grids/worldclim/", sep = "", raster.names))
plot(stack[[7]])
points(site.coords)
```

###### Extract worldclim data

```{r}
worldclim <- as_tibble(raster::extract(stack, site.coords)) 
worldclim <- cbind(sites, worldclim) 
head(worldclim)
```
###### Check for missing data

```{r}
worldclim %>% 
  filter(is.na(wc2.0_bio_30s_02))
```
HKMPM data are missing. 

###### Look at HKMPM location:

```{r}
coords_subset <- site.coords[sites$ID %in% c("HKMPM"),] 
coords_subset_names <- sites$ID[sites$Latitude %in% coords_subset]

bounding_boxes <- bounding_box(coords_subset)

bounding_boxes_maps <- crop(stack[[1]], bounding_boxes)

plot(bounding_boxes_maps)
points(114.02924,  22.49817)
```
Can move onto nearby coastline by shifting west

###### Shift HKMPM west:

```{r}
site.coords.temp <- sites %>% 
  filter(ID %in% c("HKMPM")) %>% 
  mutate(Longitude = Longitude + 0.05) %>% 
  dplyr::select(Longitude, Latitude) %>% 
  as.matrix() %>% unname()

sites.temp <- sites %>% 
  filter(ID %in% c("HKMPM")) 
         
worldclim.temp <- as_tibble(raster::extract(stack, site.coords.temp))
worldclim.temp <- cbind(sites.temp, worldclim.temp)
head(worldclim.temp)
```
###### Rejoin

```{r}
worldclim <- worldclim %>% 
  filter(!ID == "HKMPM") %>% 
  bind_rows(worldclim.temp) %>% 
  arrange(ID)
```

###### Create new name vector to match 19 products
```{r}
newnames <- c("wc_mat", "wc_dtr", "wc_iso", "wc_ts",
       "wc_mtqm", "wc_mtcm", "wc_tr", "wc_mtwtq",
       "wc_mtdq", "wc_mtwq", "wc_mtcq", "wc_map",
       "wc_pwtm", "wc_pdm", "wc_ps", "wc_pwtq",
       "wc_pdq", "wc_pwq", "wc_pcq")

oldnames <- names(worldclim)[15:33]
worldclim <- worldclim %>% 
 rename_at(vars(oldnames), ~ newnames)
```
###### Write worlclim data

```{r}
write.csv(worldclim, paste(loc, "grids/extracted/worldclim.csv", sep = ""), row.names = F)
```


#### *Grid Data Merge*

###### Read in all .csv files

```{r, warning = F}
setwd(loc)
csv.names <- list.files(paste(loc, "grids/extracted/", sep  = ""), pattern = "csv$", full.names = FALSE)
geospatial <- lapply(paste(loc, "grids/extracted/", csv.names, sep = ""), read.csv) 
names(geospatial) <- csv.names
```

###### Join static data

```{r, message = F, warning = F}

metadata_names <- names(geospatial$canopyht.csv)[2:14]

remove_metadata <- function(x) {
  x %>% 
    dplyr::select(-metadata_names)
}

canopyht <- remove_metadata(geospatial$canopyht.csv)
cti <- remove_metadata(geospatial$cti.csv)
worldclim <- remove_metadata(geospatial$worldclim.csv)
soilgrids <- remove_metadata(geospatial$soilgrids.csv)
earthenv <- remove_metadata(geospatial$earthenv.csv)

geospatial_data_static <- canopyht %>% 
  left_join(cti) %>% 
  left_join(worldclim) %>% 
  left_join(soilgrids) %>% 
  left_join(earthenv)

```
###### Join temporal data

```{r}
geospatial_data_temporal <- as_tibble(geospatial$wf.csv) %>% dplyr::select(-DOY, -Date) %>% # start with wetland fraction then add others
  left_join(geospatial$terraclimate.csv) %>% 
  left_join(geospatial$ns_depo.csv) %>% 
  left_join(geospatial$vcf.csv) %>% 
  left_join(geospatial$rpot) 
```
###### Merge static and temporal data

```{r}
geospatial_data <- geospatial_data_temporal %>% 
  full_join(geospatial_data_static)
```

###### Look at `geospatial_data` NAs

```{r}
nas <- geospatial_data %>% 
  group_by(ID, Latitude, Longitude) %>% 
  summarize_all(~sum(is.na(.))) %>% 
  dplyr::select(-Year, -Month) %>% 
  ungroup() %>% 
  gather(key = variable, value = NAs, 15:85) %>% 
  filter(NAs > 0)
nas
```

There is some missing data for 2000 for the Terraclimate temporal products.  Can impute at later step using caret.

###### Write out NA file

```{r}
write.csv(nas, paste(loc, "/grids/extracted/extracted_nas.csv", sep = ""), row.names = FALSE)
```

###### Write out `geospatial_data` file

```{r}
write.csv(geospatial_data, paste(loc, "/grids/extracted-final/geospatial.csv", sep = ""), row.names = FALSE)
```

#### *Merge geospatial and modis data*

```{r}
geospatial_data <- read.csv(paste(loc, "grids/extracted-final/geospatial.csv", sep = "")) %>% as_tibble()

modis_data <- read.csv(paste(loc, "modis/modis-processed/modis-processed.csv", sep = "")) %>% 
  dplyr::select(-NDSI, -NDVI, -EVI, -LAI, -NDWI, -SRWI, -LSWI, -LSTN) %>% 
  rename(NDSI = NDSI_F, NDVI = NDVI_F, EVI = EVI_F, LAI = LAI_F, NDWI = NDWI_F, SRWI = SRWI_F, LSWI = LSWI_F, LSTN = LSTN_F) %>% 
  as_tibble()

modis_geo <- modis_data %>% left_join(geospatial_data) %>% 
  dplyr::select(ID, Site.Name, Data.Policy, Country, Latitude, Longitude, IGBP, Site.Classification, Upland.Class, DOI,
                DOI.Reference, Site.Personnel, Base.File.Host, Upscaling, Year, Month, Week, everything()) 
```

###### Write out modis_geo data file

```{r}
write.csv(modis_geo, paste(loc, "training-data/modis_geospatial.csv", sep = ""), row.names = FALSE)
```

#### *QAQC Plots for all Predictors*

##### Read in `modis_geospatial.csv`:

```{r}
modis_geo <- read.csv(paste(loc, "training-data/modis_geospatial.csv", sep = ""))
```

##### Computed Predictors (rpot)

```{r}
modis_geo %>% 
  ggplot(aes(Month, rpot)) +
  geom_line() +
  facet_wrap(~ID) +
  labs(y = expression("Potential Radiation at the top of the Atmosphere (W m"^{-2}*")")) +
  scale_x_continuous(breaks = c(6, 12), labels = c(6, 12)) + 
  my_theme
ggsave(paste(loc, "grids/extracted-qc/rpot.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

First QAQC evaluation issues:

  + Missing data for CADBB; JPSwl; PHRlf; RUSAM; SESto; USBgl; USBrw; USIcs 
  + 3 of these are site mis-matches between modis and geospatial data (CADBB, RUSAM, SESto, USBgl, USBrw do not exist in `fluxnet-ch4-site-metadata.csv`)
  + The others are possibly mis-matches in spelling (JPSwL, PHRlF, USICs is the correct spelling, in `fluxnet-ch4-site-metadata.csv`)

Second QAQC evaluation:
  
  + No issues. Corrected modis input data site names.

##### WorldClim Predictors

###### Get Indices

```{r}
names(modis_geo)
```

Need 85-103.

###### Plot all worldclim

```{r}
oldnames <- names(modis_geo)[85:103]
newnames <- letters[1:19]

names_key <- setNames(oldnames, newnames)

modis_geo %>% 
  rename_at(vars(oldnames), ~ newnames) %>% 
  gather(key = "wc_var", value = "value", 85:103) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, value, color = group)) +
  geom_point() +
  facet_wrap(~wc_var, scales = "free", labeller = as_labeller(names_key)) +
      my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

ggsave(paste(loc, "grids/extracted-qc/worldclim.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

Ranges look reasonable. No outliers.

##### EarthEnv Predictors

Need 110-136

###### Plot earthenv

```{r}
oldnames <- names(modis_geo)[110:136]
newnames <- c(letters, "ab")

names_key <- setNames(oldnames, newnames)

modis_geo %>% 
  rename_at(vars(oldnames), ~ newnames) %>% 
  gather(key = "earthenv", value = "value", 110:136) %>% 
   mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(ID, value, color = group)) +
    geom_point() +
    facet_wrap(~earthenv, scales = 'free', labeller = as_labeller(names_key)) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave(paste(loc, "grids/extracted-qc/earthenv.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)
```

##### Terraclimate Predictors

Get 69-74.

###### Plot the actual values

```{r}
# aet
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, aet, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID, scales = 'free') +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration (mm/month)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aet.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil water
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, soilwater, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture (mm/month)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwater.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

###### Plot the ranges

```{r}
# aet
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Year, aetR, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration Annual Range (mm)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aetR.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil water
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Year, soilwaterR, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture Annual Range (mm)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwaterR.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

###### Plot the seasonality

```{r}
# aetS
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, aetS, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Actual Evapotranspiration Annual Seasonality (unitless)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-aetS.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

# soil waterS
modis_geo %>% 
     mutate(group = ifelse(Latitude > 60, "Boreal", NA),
          group = ifelse(Latitude < 60 & Latitude > 30, "Temperate", group),
          group = ifelse(Latitude < 30, "Tropical", group)) %>%
  ggplot(aes(Month, soilwaterS, color = group)) +
    geom_point(size = 0.5) +
    facet_wrap(~ID) +
  my_theme +
  theme(axis.text.x=element_blank()) +
  labs(y = "TerraClimate Soil Moisture Annual Seasonality (unitless)")
ggsave(paste(loc, "grids/extracted-qc/terraclimate-soilwaterS.pdf", sep = ""),
       width = 30, height = 30, units = c("cm"), dpi = 300)

```

QC issues:

  + AET looks fine
  + soilwater seasonality is very variable within some sites, and obviously wrong in locations where wetlands are not the dominant component of the pixel
  (e.g., US-TW1 through US-Twt, which are delta marshes embedded in a dryland Mediterranean climate)

## vcf Predictors
vcf <- read.csv("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Extractions/vcf.csv")

# check full year (1994 and 2000)
vcf %>% 
  gather(key = vcfpred, value = value, 6:length(vcf)) %>%
  filter(Year == 1994)

vcf %>% 
  gather(key = vcfpred, value = value, 6:length(vcf)) %>%
  ggplot(aes(vcfpred,value)) +
  geom_point() +
  facet_wrap(~ID, scales = 'free') +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Data QCQA/qaqc/vcf_postqcqa.pdf",
       width = 50, height = 30, units = c("cm"), dpi = 300)


## cti Predictors
cti <- read.csv("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Extractions/cti.csv")

cti %>% 
  ggplot(aes(ID,cti)) +
  geom_point() +
  geom_text(aes(label=ID), vjust = 1.5, hjust = 1) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Figures/qaqc/cti.pdf",
       width = 50, height = 30, units = c("cm"), dpi = 300)


## wf Predictors
wf <- read.csv("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Extractions/wf.csv")

wf %>% 
  filter(Year == 2010) %>% 
  ggplot(aes(Month,wf)) +
  geom_point() +
  facet_wrap(~ID) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Data QCQA/qaqc/WFS_postqcqa.pdf",
       width = 50, height = 30, units = c("cm"), dpi = 300)


## canopyht Predictors
canopyht <- read.csv("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Extractions/canopyht.csv")

canopyht %>% 
  ggplot(aes(ID, canopyht)) +
  geom_point() +
  geom_text(aes(label = ID), hjust = 1.5, vjust = 1) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Figures/qaqc/canopyht.pdf",
       width = 50, height = 30, units = c("cm"), dpi = 300)


## ns deposition Predictors
nsdepo <- read.csv("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Extractions/nsdepo.csv")

# look at range
nsdepo %>% arrange(desc(nt_depo))

nsdepo %>% 
  gather(key = pollutant, value = rate, 5:8) %>% 
  ggplot(aes(ID, log10(rate), color = pollutant)) +
  geom_point() +
  facet_wrap(~pollutant) +
  geom_text(aes(label = ID)) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Figures/qaqc/nsdepo.pdf",
       width = 50, height = 30, units = c("cm"), dpi = 300)

## sgrids Predictors
sgrids <- read.csv("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Extractions/sgrids.csv")

sgrids %>% 
  gather(key = variable, value = value, 5:10) %>% 
  ggplot(aes(ID, value, color = variable)) +
  geom_point() +
  facet_wrap(~variable, scales = 'free') +
  geom_text(aes(label = ID)) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
ggsave("/Volumes/LACIE SHARE/Stanford CH4/June 2020 Upscaling/Figures/qaqc/sgrids.pdf",
       width = 50, height = 30, units = c("cm"), dpi = 300)
